% (C) Brett Klamer - MIT - http://opensource.org/licenses/MIT
% Please contact me if you find any errors or make improvements
% Contact details at brettklamer.com

\documentclass[11pt,letterpaper,english,oneside]{article} % article class is a standard class
%==============================================================================
%Load Packages
%==============================================================================
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % easy page margins
\usepackage[utf8]{inputenc} % editor uses utf-8 encoding
\usepackage[T1]{fontenc} % T1 font pdf output
\usepackage{lmodern} % Latin modern roman font
\usepackage{bm, bbm} % bold and blackboard bold math symbols
\usepackage{amsmath, amsfonts, amssymb, amsthm} % math packages
\usepackage[final]{microtype} % better microtypography
\usepackage{graphicx} % for easier grahics handling
\usepackage[hidelinks, colorlinks=true, linkcolor = blue, urlcolor = blue]{hyperref} % to create hyperlinks
\usepackage{float} % tells floats to stay [H]ere!
\usepackage{mdframed} % it's better than framed. knitr uses framed so settings won't conflict
\usepackage{enumitem} % nice lists
\usepackage{fancyhdr} % nice headers
\usepackage{caption}  % to control figure and table captions

\newcommand{\var}{\mathrm{Var}}
\newcommand{\ep}{\varepsilon}
\newcommand{\E}{\mathbb{E}}
\DeclareMathOperator{\tr}{Tr}
\newcommand{\cN}{\mathcal{N}}
\captionsetup{width=0.9\textwidth, justification = raggedright}

%==============================================================================
% Enter name and homework title here
%==============================================================================
\author{Patrick Chao}
\title{Homework 3}
\date{Due Sunday, October 24 at 11:59pm}

%==============================================================================
% Put title and author in PDF properties
%==============================================================================
\makeatletter % change interpretation of @
\hypersetup{pdftitle={\@title},pdfauthor={\@author}}


%==============================================================================
% Header settings
%==============================================================================
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @

%==============================================================================
% List spacing
%==============================================================================
\setlist[itemize]{parsep=0em} % fix itemize spacing
\setlist[enumerate]{parsep=0em} % fix enumerate spacing

%==============================================================================
% Float spacing (changes spacing of tables, graphs, etc)
%==============================================================================
%\setlength{\textfloatsep}{3pt}
%\setlength{\intextsep}{3pt}

%==============================================================================
% Define Problem and Solution Environments
%==============================================================================
\theoremstyle{definition} % use definition's look
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\mdfsetup{ % box margin fix for mdframe and how it plays with parskip and others.
innerleftmargin=4pt,
innerrightmargin=4pt,
innertopmargin=-1pt,
innerbottommargin=4pt}
% \newenvironment{prob}{\begin{mdframed}\begin{problem}\hspace{0pt}}{\end{problem}\end{mdframed}}
\newenvironment{prob}{\clearpage \begin{problem}\hspace{0pt}}{\end{problem}}
\newenvironment{sol}{\begin{solution}\hspace{0pt}}{\end{solution}}

%==============================================================================
% set knitr options
%==============================================================================
% latex (change space before and after knitr kframe; based on framed package)
%\setlength{\OuterFrameSep}{0.3em}


\begin{document}

% R
<<setup, include=FALSE, echo=FALSE,cache=FALSE>>=

library(knitr)
# inline hook to process the output of \Sexpr{} statements to just 2 digits
inline_hook <- function(x) {
  if(is.numeric(x)) x <- round(x, 2)
  paste(as.character(x), collapse=", ")
}
knit_hooks$set(inline = inline_hook)

# cache all chunks
opts_chunk$set(cache = TRUE)

# create directory for figures
if(!dir.exists("figures")){
  dir.create("figures")
}
@

\SweaveOpts{concordance=TRUE}


\maketitle

\section{Instructions}

\paragraph{Setup.} Pull the latest version of this assignment from Github and set your working directory to \texttt{stat-961-fall-2021/homework/homework-3}. Consult the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/getting-started.pdf}{getting started guide} if you need to brush up on \texttt{R}, \texttt{LaTeX}, or \texttt{Git}.

\paragraph{Collaboration.} The collaboration policy is as stated on the Syllabus:

\begin{quote}
``Students are permitted to work together on homework assignments, but solutions must be written up and submitted individually. Students must disclose any sources of assistance they received; furthermore, they are prohibited from verbatim copying from any source and from consulting solutions to problems that may be available online and/or from past iterations of the course.''
\end{quote}

\noindent In accordance with this policy, \\

\noindent \textit{Please list anyone you discussed this homework with:} Jeffrey Zhang, Dongwoo Kim, Abhinav Chakraborty, Ryan Brill\\

\noindent \textit{Please list what external references you consulted (e.g. articles, books, or websites):} None

\paragraph{Writeup.} Use this document as a starting point for your writeup, adding your solutions between \verb|\begin{sol}| and \verb|\end{sol}|. See the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/preparing-reports.pdf}{preparing reports guide} for guidance on compilation, creation of figures and tables, and presentation quality. Show all the code you wrote to produce your numerical results, and include complete derivations typeset in LaTeX for the mathematical questions.

\paragraph{Programming.}

The \texttt{tidyverse} paradigm for data manipulation (\texttt{dplyr}) and plotting (\texttt{ggplot2}) are strongly encouraged, but points will not be deducted for using base \texttt{R}.
<<message=FALSE, cache = FALSE>>=
library(tidyverse)
@

\paragraph{Grading.} Each sub-part of each problem will be worth 3 points: 0 points for no solution or completely wrong solution; 1 point for some progress; 2 points for a mostly correct solution; 3 points for a complete and correct solution modulo small flaws. The presentation quality of the solution for each problem (as exemplified by the guidelines in Section 3 of the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/preparing-reports.pdf}{preparing reports guide}) will be evaluated out of an additional 3 points.

\paragraph{Submission.} Compile your writeup to PDF and submit to \href{https://www.gradescope.com/courses/284562}{Gradescope}.

\clearpage

\begin{prob} \label{prob:intercept-only}\textbf{Heteroskedasticity and correlated errors in the intercept-only model.} \\

\noindent Suppose that
\begin{equation}
y_i = \beta_0 + \epsilon_i, \quad \text{where } \bm \epsilon \sim N(0, \bm \Sigma)
\label{eq:arbitrary-correlation}
\end{equation}
for some positive definite $\bm \Sigma \in \mathbb R^{n \times n}$. The goal of this problem is to investigate the effects of heteroskedasticity and correlated errors on the validity and efficiency of least squares estimation and inference.

\begin{enumerate}

\item[(a)] (Validity of least squares inference) What is the usual least squares estimate $\widehat \beta^{\text{LS}}_0$ for $\beta_0$ (from Unit 2)? What is its variance under the model~\eqref{eq:arbitrary-correlation}? What is the usual variance estimate $\widehat{\text{Var}}[\widehat \beta^{\text{LS}}_0]$ (from Unit 2), and what is this estimator's expectation under~\eqref{eq:arbitrary-correlation}? The ratio
\begin{equation}
\tau_1 \equiv \frac{\mathbb E[\widehat{\text{Var}}[\widehat \beta^{\text{LS}}_0]]}{\text{Var}[\widehat \beta^{\text{LS}}_0]}
\label{eq:tau-1}
\end{equation}
is a measure of the validity of usual least squares inference under~\eqref{eq:arbitrary-correlation}. Write down an expression for $\tau_1$, and discuss the implications of $\tau_1$ for the Type-I error of the hypothesis test of $H_0: \beta_0 = 0$ and for the coverage of the confidence interval for $\beta_0$.

\item[(b)] (Efficiency of least squares estimator) Let's assume $\bm \Sigma$ is known. We could get valid inference based on $\widehat \beta^{\text{LS}}_0$ by using the variance formula from part (a). Alternatively, we could use the maximum likelihood estimate $\widehat \beta^{\text{ML}}_0$ for $\beta_0$. What is the variance of $\widehat \beta^{\text{ML}}_0$? The ratio
\begin{equation}
\tau_2 \equiv \frac{\text{Var}[\widehat \beta^{\text{LS}}_0]}{\text{Var}[\widehat \beta^{\text{ML}}_0]}
\label{eq:tau-2}
\end{equation}
is a measure of the efficiency of the usual least squares estimator under~\eqref{eq:arbitrary-correlation}, recalling that the maximum likelihood estimator is most efficient. Write down an expression for $\tau_2$, and discuss the implications of $\tau_2$ for the power of the hypothesis test of $H_0: \beta_0 = 0$ and for the width of the confidence interval for $\beta_0$.

\item[(c)] (Special case: Heteroskedasticity) Suppose $\bm \Sigma = \text{diag}(\sigma^2_1, \dots, \sigma^2_n)$ for some $\sigma^2_1, \dots, \sigma^2_n > 0$. Compute the ratios $\tau_1$ and $\tau_2$ defined in equations~\eqref{eq:tau-1} and~\eqref{eq:tau-2}, respectively. How do these ratios depend on $(\sigma^2_1, \dots, \sigma^2_n)$, and what are the implications for validity and efficiency?

\item[(d)] (Special case: Correlated errors) Suppose $(\epsilon_1, \dots, \epsilon_n)$ are \textit{equicorrelated}, i.e.
\begin{equation}
\Sigma_{j_1j_2} =
\begin{cases}
1, \quad &\text{if } j_1 = j_2; \\
\rho, \quad &\text{if } j_1 \neq j_2.
\end{cases}
\end{equation}
for some $\rho \geq 0$. Compute the ratios $\tau_1$ and $\tau_2$ defined in equations~\eqref{eq:tau-1} and~\eqref{eq:tau-2}, respectively. How do these ratios depend on $\rho$, and what are the implications for validity and efficiency?
\end{enumerate}

\end{prob}

\begin{sol}
\begin{enumerate}
    \item[(a)] The usual least squares estimate is $\hat \beta_0^{LS}=\bar y$. The variance under the model is
    \[\var [\bar y]=\var\left[\frac{1}{n}\sum_{i=1}^n y_i\right]=\frac{1}{n^2}\var[\mathbbm{1}^T\bm{Y}]=\frac{1}{n^2}\mathbbm{1}^T\bm{\Sigma}\mathbbm{1}.\]
    The usual variance estimate is
    \[\widehat{\var}[\hat \beta_0^{LS}]=\frac{1}{n}\frac{\|\hat \ep\|^2}{n-1}=\frac{\|{\bm Y}-\bar y\mathbbm{1}\|^2}{n(n-1)}.\]
    Taking the expectation,
    \begin{align*}
        \E[\widehat{\var}[\hat \beta_0^{LS}]] &= \frac{1}{n(n-1)}\E\left \|{\bm Y}-\frac{1}{n}\mathbbm{1} \mathbbm{1}^T{\bm Y}\right\|^2\\
        &=\frac{1}{n(n-1)}\E\left[{\bm Y}^T\left(I-\frac{1}{n}\mathbbm{1} \mathbbm{1}^T\right){\bm Y}\right]\\
        &=\frac{1}{n(n-1)}\left(\tr\left(\left(I-\frac{1}{n}\mathbbm{1} \mathbbm{1}^T\right){\bm \Sigma}\right)+\beta_0^2\mathbbm{1}^T\left(I-\frac{1}{n}\mathbbm{1} \mathbbm{1}^T\right) \mathbbm{1}\right)\\
        &=\frac{1}{n(n-1)}\left(\tr({\bm \Sigma})-\frac{1}{n}\mathbbm{1}^T\bm{\Sigma}\mathbbm{1}\right).
    \end{align*}
    The second inequality follows from the fact that $\left(I-\frac{1}{n}\mathbbm{1} \mathbbm{1}^T\right)^2=\left(I-\frac{1}{n}\mathbbm{1} \mathbbm{1}^T\right)$ and the third equality follows from the expectation of a quadratic form.

    We may rewrite $\tau_1$ as
    \[\tau_1=\frac{1}{n-1}\left(\frac{n\tr({\bm \Sigma})}{\mathbbm{1}^T\bm{\Sigma}\mathbbm{1}}-1\right).\]
    Intuitively, $\tau_1$ captures how accurate the estimated variance of $\hat \beta_0^{LS}$ actually is. First, note that $\tau_1\ge 0$ since these variances are both positive. We may furthermore notice that
    \[\tr({\bm \Sigma}) \le \mathbbm{1}^T\bm{\Sigma}\mathbbm{1}\]
    since the trace of a matrix is less than the sum of all entries. Therefore $0\le \tau_1\le 1$. Large values of $\tau_1$ are \emph{better}, the estimated variance is close to the true variance, and the Type-I error and coverage are close to the desired values. On the other hand, small values of $\tau_1$ are worse, resulting in poor coverage and inflated Type-I error.


    \item[(b)] Define $\tilde y={\bm \Sigma}^{-1/2}y$ and $\tilde x={\bm \Sigma}^{-1/2}x$. Then $\hat \beta_0^{ML}$ is
    \[\hat \beta_0^{ML}=(\tilde {\bm X}^T\tilde {\bm X})^{-1}\tilde {\bm X}^T \tilde {\bm Y}= (\mathbbm{1}^T\bm{\Sigma}^{-1}\mathbbm{1})^{-1} \mathbbm{1}^T\bm{\Sigma}^{-1/2}\bm{\Sigma}^{-1/2}{\bm Y}=\frac{\mathbbm{1}^T\bm{\Sigma}^{-1}{\bm Y}}{\mathbbm{1}^T\bm{\Sigma}^{-1}\mathbbm{1}}.\]
    The variance follows from the fact that $Y\sim \cN(\beta_0\mathbbm{1},{\bm \Sigma})$.
    \begin{align*}
        \var[\hat \beta_0^{ML}]=(\mathbbm{1}^T\bm{\Sigma}^{-1}\mathbbm{1})^{-2}\mathbbm{1}^T\bm{\Sigma}^{-1}\bm{\Sigma}\bm{\Sigma}^{-1}\mathbbm{1}=\frac{1}{\mathbbm{1}^T\bm{\Sigma}^{-1}\mathbbm{1}}.
    \end{align*}
    Plugging this into $\tau_2$, we have
    \[\tau_2=\frac{1}{n^2}(\mathbbm{1}^T\bm{\Sigma}\mathbbm{1})(\mathbbm{1}^T\bm{\Sigma}^{-1}\mathbbm{1}).\]
    We may show that $\tau_2\ge 1$. Since $\mathbbm{1}^T\mathbbm{1}=n$,
    \begin{align*}
        \tau_2&=\frac{\mathbbm{1}^T\bm{\Sigma}\mathbbm{1}}{\mathbbm{1}^T\mathbbm{1}}\frac{\mathbbm{1}^T\bm{\Sigma}^{-1}\mathbbm{1}}{\mathbbm{1}^T\mathbbm{1}}\\
        &\ge \min_{\bm x:\|\bm x\|_2=1}({\bm x}^T\bm{\Sigma}{\bm x})({\bm x}^T\bm{\Sigma}^{-1}{\bm x}).
    \end{align*}
    Since $\Sigma$ is a symmetric positive definition covariance matrix, we can consider the diagonalization ${\bm \Sigma}=\bm{ PDP}^{-1}$ with a diagonal matrix $\bm D=\mathrm{diag}(\lambda_1,\ldots,\lambda_n)$ and orthogonal matrix $\bm P$. Let ${\bm z}={\bm P}{\bm x}$.
    \begin{align*}
        \tau_2&\ge \min_{\bm x:\|\bm x\|_2=1}({\bm x}^T\bm{\Sigma}{\bm x})({\bm x}^T\bm{\Sigma}^{-1}{\bm x})\\
        &= \min_{\bm z:\|\bm z\|_2=1}({\bm z}^T\bm{ D}{\bm z})({\bm z}^T\bm{D}^{-1}{\bm z})\\
        &=\min_{\bm z:\|\bm z\|_2=1} \left(\sum_{i=1}^n\lambda_i z_i^2\right) \left(\sum_{i=1}^n\frac{1}{\lambda_i} z_i^2\right)\\
        &\ge  \left(\sum_{i=1}^n\frac{\sqrt{\lambda_i}}{\sqrt{\lambda_i}} z_i^2\right) ^2 =1.
    \end{align*}
    The inequality follows from Cauchy-Schwartz.

    Intuitively, since the maximum likelihood estimator is most efficient, $\tau_2\ge 1$ means that the least squares estimator will always have more variance than the MLE. However, if $\tau_2$ is close to $1$, then the least squares estimator does not perform much worse than the MLE. Small values $\tau_2$ are \emph{better}, the hypothesis test has greater power and the width of the confidence interval is smaller.

    \item[(c)] For $\bm \Sigma=\mathrm{diag}(\sigma_1^2,\ldots,\sigma_n^2)$, we have
    \begin{align*}
        \tau_1&=\frac{1}{n-1}\left(\frac{n\tr({\bm \Sigma})}{\mathbbm{1}^T\bm{\Sigma}\mathbbm{1}}-1\right)=1\\
        \tau_2&=\frac{1}{n^2}(\mathbbm{1}^T\bm{\Sigma}\mathbbm{1})(\mathbbm{1}^T\bm{\Sigma}^{-1}\mathbbm{1})=\frac{\left(\sum_{i=1}^n \sigma_i^2\right)\left(\sum_{i=1}^n \sigma_i^{-2}\right)}{n^2}.
    \end{align*}
    In other words, your hypothesis test will still be valid and have proper Type I error, however the test may be inefficient if the values of $\sigma_i^2$ are on different scales. If the $\sigma_i^2$ are all close to each other, then you will have a more powerful test with narrower intervals.
    \item[(d)] Note that $\mathbbm{1}$ is an eigenvector of $\Sigma$ with eigenvalue $(1+(n-1)\rho)$. Therefore
    \begin{align*}
        \tau_1&=\frac{1}{n-1}\left(\frac{n\tr({\bm \Sigma})}{\mathbbm{1}^T\bm{\Sigma}\mathbbm{1}}-1\right)=\frac{1}{n-1}\left(\frac{n^2}{n+(n^2-n)\rho}-1\right)=\frac{1-\rho}{1+(n-1)\rho}\\
        \tau_2&=\frac{1}{n^2}(\mathbbm{1}^T\bm{\Sigma}\mathbbm{1})(\mathbbm{1}^T\bm{\Sigma}^{-1}\mathbbm{1})=\frac{1}{n^2}(\mathbbm{1}^T(1+(n-1)\rho)\mathbbm{1})(\mathbbm{1}^T(1+(n-1)\rho)^{-1}\mathbbm{1})=1.
    \end{align*}
    Regardless of the value of $\rho$, the least squares estimate is as efficient as the MLE. For small values of $\rho$, the data is more similar to the homoskedastic case, resulting in increased validity of the hypothesis test. As $\rho$ increases to $1$, the validity and coverage drop.

\end{enumerate}
\end{sol}

\begin{prob} \label{prob:robust-standard-errors}\textbf{Comparing constructions of heteroskedasticity-robust standard errors.} \\

\noindent Suppose that
\begin{equation}
\bm y = \bm X \bm \beta + \bm \epsilon, \quad \epsilon_i \overset{\text{ind}}\sim N(0, \sigma^2_i).
\end{equation}
Two approaches to obtaining heteroskedasticity-robust standard errors are the pairs bootstrap and Huber-White standard errors. The goal of this problem is to compare the coverage and width of confidence intervals obtained from these two approaches.

\begin{enumerate}
\item[(a)] Write a function called \verb|pairs_bootstrap|, which inputs arguments $\bm X$, $\bm y$, and $B$ and outputs an estimated $p \times p$ covariance matrix $\widehat{\text{Var}}[\widehat{\bm \beta}]$ based on $B$ resamples of the pairs bootstrap.

\item[(b)] Write a function called \verb|huber_white|, which inputs arguments $\bm X$ and $\bm y$ and outputs an estimated $p \times p$ covariance matrix $\widehat{\text{Var}}[\widehat{\bm \beta}]$ based on the Huber-White formula.

\item[(c)] Generate $n = 50$ $(x,y)$ pairs by setting $x$ to be equally-spaced values between 0 and 1 and drawing $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$, where $\epsilon_i \overset{\text{ind}}\sim N(0, 9x_i^2)$, $\beta_0 = 2, \beta_1 = 3$. Create a scatter plot of these points, the least squares line, and three confidence bands: the standard least squares confidence band as well as those resulting from the pairs bootstrap (with $B = 500$) and the Huber-White formula. Comment on the relative widths of these three bands depending on the value of $x$.

\item[(d)] Repeat the experiment from part (c) 100 times to compute the coverage and average width of the three confidence bands for each value of $x$. Plot these two metrics as a function of $x$, and comment on the results.

\end{enumerate}

\end{prob}

\begin{sol}
\begin{enumerate}
\item
<<>>=

library(tidyverse)


pairs_bootstrap <- function(X,Y,B){
  # Initialize matrix for sampled betas
  betas <- matrix(ncol=ncol(X),nrow=B)
  for(i in 1:B){
    # Construct random sample
    samples <- sample(nrow(X),nrow(X),replace = T)
    X_boot <- X[samples,]
    Y_boot <- Y[samples]

    # Computed bootstrapped beta
    betas[i,] <- coef(lm(Y_boot~X_boot-1))
  }
  # Return empirical covariance matrix
  return(cov(betas))
}
@

\item
<<>>=


huber_white <- function(X,Y){
  # Compute beta hat and Huber-White standard errors
  hatbeta <- coef(lm(Y~X-1))
  hat_sigmasq <- as.numeric((Y-X%*%hatbeta)^2)
  # Compute helper variables
  hat_sigma <- diag(hat_sigmasq,)
  gram <- solve(t(X)%*%X)
  # Output final variance estimate
  return(gram %*% (t(X)%*% hat_sigma %*% X) %*% gram)
}

@


\item
<<fig=TRUE,height=4,width=5>>=
n <- 50
betas <- as.matrix(c(2,3))

# Data generation function
generate_data <- function(n,betas){
  # Construct X and Y matrices
  X <- as.matrix(data.frame(x0=rep(1,n),x1=runif(n)))
  Y <- as.numeric(X%*% (betas)) +
    rnorm(n,mean=rep(0,n),sd=sqrt(X[,2]^2*9))
  data <- data.frame(x0=rep(1,n),x1=X[,2],y=Y)
  # Return all objects
  return(list(X=X,Y=Y,data=data))
}

# Generate Data
all_vals <- generate_data(n,betas)
X <- all_vals$X
Y <- all_vals$Y
data <- all_vals$data

# Construct confidence bands for Pairs Bootstrap and
# Huber-White
compute_bands <- function(X,Y,B,pts){

  # Compute beta hat
  hatbeta <-  as.numeric(coef(lm(Y~X-1)))
  # Compute covariance matrices
  hw <- huber_white(X,Y)
  pbs <- pairs_bootstrap(X,Y,B)

  hw_band <- matrix(nrow=npoints,ncol=2)
  pbs_band <- matrix(nrow=npoints,ncol=2)
  quants <- qt(c(0.025,0.975),df=n-2)

  # Compute confidence band per value of x
  for(i in 1:npoints){
    x_vec <- as.matrix(c(1,pts[i]))
    # Confidence in terms of t distribution
    hw_band[i,]  <- quants*sqrt(t(x_vec)%*% hw %*% x_vec)[1]+
      (t(hatbeta)%*%x_vec)[1]
    pbs_band[i,] <- quants*sqrt(t(x_vec)%*% pbs %*% x_vec)[1]+
      (t(hatbeta)%*%x_vec)[1]
  }

  # Reformulate objects as data frames and rename
  pbs_band <- data.frame(pbs_band)
  hw_band <- data.frame(hw_band)
  colnames(hw_band) <- c("lower","upper")
  colnames(pbs_band) <- c("lower","upper")
  hw_band$x <- pts
  pbs_band$x <- pts
  return(list(hw_band=hw_band,pbs_band=pbs_band))
}

# Run compute_bands to obtain bands
npoints <- 100
B <- 500
pts <- seq(from=0,to=1,length.out=npoints)
bands <- compute_bands(X,Y,B,pts)
hw_band <- bands$hw_band
pbs_band <- bands$pbs_band

# Define colors
colors <- c(
  "Huber-White" = "tomato",
  "Pairs Bootstrap" = "seagreen1",
  "Least Squares" = "gray70"
)
# Plot points, confidence bands, linear model
ggplot() +
  geom_line(data=pbs_band,
      aes(y=upper,x=x,color="Pairs Bootstrap"),alpha=0.8)+
  geom_line(data=pbs_band,
      aes(y=lower,x=x,color="Pairs Bootstrap"),alpha=0.8)+
  geom_line(data=hw_band,
      aes(y=upper,x=x,color="Huber-White"),alpha=0.8)+
  geom_line(data=hw_band,
      aes(y=lower,x=x,color="Huber-White"),alpha=0.8)+
  geom_point(data=data,aes(x=x1,y=y))+
  geom_smooth(data=data,
      aes(x=x1,y=y),alpha=0.4,method=lm)+
  labs(x="X",y="Y",fill="Confidence Band",
       title = "Confidence Bands for Heteroskedastic Data")+
  scale_color_manual(values=colors,name="test")+theme_classic()

@

Comparing the relative widths, the Huber-White and pairs bootstraps are almost identical, whereas the least squares band is larger for smaller values of x and smaller for larger values of x. This makes sense as it overestimates the variance of y for x close to 0 and over estimates the variance of y for x close to 1.

\item
<<fig=TRUE,height=4,width=5>>=

# Initialize variables for storing
npoints <- 100
pts <- seq(from=0,to=1,length.out=npoints)
lm_widths <- rep(0,npoints)
hw_widths <- rep(0,npoints)
pbs_widths <- rep(0,npoints)

lm_coverage <- rep(0,npoints)
hw_coverage <- rep(0,npoints)
pbs_coverage <- rep(0,npoints)

# Run simulation 200 times
ntrials <- 200
for(i in 1:ntrials){
  # Generate Data
  all_vals <- generate_data(n,betas)
  X <- all_vals$X
  Y <- all_vals$Y
  data <- all_vals$data

  # Least Squares Confidence Bands
  fit <- lm(y~x0+x1-1,data=data)
  new_x <- data.frame(x0=rep(1,npoints),x1=pts)
  lm_ints <- data.frame(predict(fit,new_x,interval="confidence"))
  lm_widths <- lm_widths + lm_ints$upr-lm_ints$lwr

  # Confidence bands for HW and PBS
  bands <- compute_bands(X,Y,B,pts)
  hw_band <- bands$hw_band
  pbs_band <- bands$pbs_band
  hw_widths <- hw_widths + hw_band$upper-hw_band$lower
  pbs_widths <- pbs_widths + pbs_band$upper-pbs_band$lower

  # Compute Coverages
  true_val <-  betas[1]+betas[2]*pts
  lm_coverage <- lm_coverage +
    as.numeric((true_val < lm_ints$upr) & (true_val > lm_ints$lwr))
  hw_coverage <- hw_coverage +
    as.numeric((true_val < hw_band$upper) & (true_val > hw_band$lower))
  pbs_coverage <- pbs_coverage +
    as.numeric((true_val < pbs_band$upper) & (true_val > pbs_band$lower))

}

# Divide by number of trials to compute average
lm_widths <- lm_widths/ntrials
hw_widths <- hw_widths/ntrials
pbs_widths <- pbs_widths/ntrials

lm_coverage <- lm_coverage/ntrials
hw_coverage <- hw_coverage/ntrials
pbs_coverage <- pbs_coverage/ntrials

# Store in data frame
all_metrics <- data.frame(band=c(rep("Least Squares",npoints),
                                 rep("Huber-White",npoints),
                                 rep("Pairs Bootstrap",npoints)),
           widths=c(lm_widths,hw_widths,pbs_widths),
           coverage=c(lm_coverage,hw_coverage,pbs_coverage),
           x=rep(pts,3)
           )

# Plot Average Width
all_metrics%>% ggplot(aes(x=x,y=widths,color=band))+
  geom_line()+
  labs(x="X",y="Width",title="Average Confidence Band Width for X",
       color="Confidence Band")+
  theme_classic()

@

<<fig=TRUE,height=5>>=
# Plot Average Coverage
all_metrics%>% ggplot(aes(x=x,y=coverage,color=band))+
  geom_line()+
  labs(x="X",y="Coverage",
       title="Average Confidence Band Coverage % for X",
       color="Confidence Band")+theme_classic()
@

We see that the pairs bootstrap and Huber-White perform very similarly, with the Huber-White method having a slightly more narrow confidence band. As before, we see that the standard least squares estimate is much wider than necessary for small values of x and not wide enough for large values.

In terms of coverage, as expected the standard least squares estimate covers $\beta$ about 100\% of the time for small values of x due to a superfluously large interval, while dropping to about 85\% for large values of x. The pairs bootstrap and Huber-White methods both perform well close to 95\% coverage, although they seem to decrease in coverage percentage as X increase, approaching about 93\%.

\end{enumerate}

\end{sol}

\begin{prob} \label{prob:ad-data}\textbf{Case study: Advertising data..} \\

\noindent In this problem, we will analyze a data set related to advertising spending. It contains the sales of a product (in thousands of units) in 200 different markets, along with advertising budgets (in thousands of dollars) for the product in each of those markets for three different media: TV, radio, and newspaper. The goal is to learn about the relationship between these three advertising budgets (predictors) and sales (response).

<<message = FALSE>>=
ad_data = read_tsv("../../data/Advertising.tsv")
print(ad_data, n = 5)
@

\begin{enumerate}

\item[(a)] Run a linear regression of \verb|sales| on \verb|TV|, \verb|radio|, and \verb|newspaper|, and produce a set of standard diagnostic plots. What model misspecification issue(s) appear to be present in these data?

\item[(b)] Address the above misspecification issues using one or more of the strategies discussed in Unit 3. Report a set of statistical estimates, confidence intervals, and test results you think you can trust.

\item[(c)] Discuss the findings from part (b) in language that a policymaker could comprehend, including any caveats or limitations of the analysis.

\end{enumerate}

\end{prob}

\begin{sol}
\begin{enumerate}
\item
<<fig=True,height=6,width=6>>=
library(GGally)
GGally::ggpairs(ad_data)+
    theme_classic()
@

<<fig=True,height=5,width=5>>=
fit <- lm(sales~TV+radio+newspaper,data=ad_data)
plot(fit,which=1)
@

<<fig=True,height=5,width=5>>=

plot(fit,which=2)
@

<<fig=True,height=5,width=5>>=

plot(fit,which=3)
@

<<fig=True,height=5,width=5>>=

plot(fit,which=4)
@


The data exhibits nonnormality of the residuals from the QQ plot, and seems to suggest the presence of outliers. Looking at the individual scatter plots, the variance of the data seems to be heteroskedastic.

\item
I propose to remove the observation 131 as it is highly anomalous compared to the other observations. To account for heteroskedasticity, I propose to use the Huber-White estimator.

<<>>=
library(lmtest)
ad_data_subset <- ad_data[-131,]
fit_subset <- lm(sales~TV+radio+newspaper,data=ad_data_subset)
coeftest(fit_subset,voc.=vovHC)
coefci(fit_subset,voc.=vovHC)
@

\item
Our analysis suggests that radio and television ads have a statistical significant relationship with sales. Our linear model suggests radio ads increase 200 units of sales per thousand dollars of investment, while radio ads increase 44 units of sales per thousand dollars of investment. Therefore our model suggests to invest in radio ads, and then television ads and not newspaper ads.

This analysis is limited by the fact it only considers a linear relationship between the covariates and response. It may be that there is a diminishing effect in ad spending, as consumers become desensitized or even annoyed at constant bombardment of ads. Furthermore, there may be some form of interaction between advertisements, e.g. if a consumer is exposed to more forms of advertisement, then the consumer is more likely to be affected and adjust their spending habits. This could be remedied with introducing interaction terms or more complex models.
\end{enumerate}

\end{sol}



\end{document}
