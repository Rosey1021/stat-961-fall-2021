% (C) Brett Klamer - MIT - http://opensource.org/licenses/MIT
% Please contact me if you find any errors or make improvements
% Contact details at brettklamer.com

\documentclass[11pt,letterpaper,english,oneside]{article} % article class is a standard class
%==============================================================================
%Load Packages
%==============================================================================
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % easy page margins
\usepackage[utf8]{inputenc} % editor uses utf-8 encoding
\usepackage[T1]{fontenc} % T1 font pdf output
\usepackage{lmodern} % Latin modern roman font
\usepackage{bm, bbm} % bold and blackboard bold math symbols
\usepackage{amsmath, amsfonts, amssymb, amsthm} % math packages
\usepackage[final]{microtype} % better microtypography
\usepackage{graphicx} % for easier grahics handling
\usepackage[hidelinks, colorlinks=true, linkcolor = blue, urlcolor = blue]{hyperref} % to create hyperlinks
\usepackage{float} % tells floats to stay [H]ere!
\usepackage{mdframed} % it's better than framed. knitr uses framed so settings won't conflict
\usepackage{enumitem} % nice lists
\usepackage{fancyhdr} % nice headers
\usepackage{caption}  % to control figure and table captions

\captionsetup{width=0.9\textwidth, justification = raggedright}

%==============================================================================
% Enter name and homework title here
%==============================================================================
\author{Sam Rosenberg}
\title{Homework 3}
\date{Due Sunday, October 24 at 11:59pm}

%==============================================================================
% Put title and author in PDF properties
%==============================================================================
\makeatletter % change interpretation of @
\hypersetup{pdftitle={\@title},pdfauthor={\@author}}


%==============================================================================
% Header settings
%==============================================================================
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @

%==============================================================================
% List spacing
%==============================================================================
\setlist[itemize]{parsep=0em} % fix itemize spacing
\setlist[enumerate]{parsep=0em} % fix enumerate spacing

%==============================================================================
% Float spacing (changes spacing of tables, graphs, etc)
%==============================================================================
%\setlength{\textfloatsep}{3pt}
%\setlength{\intextsep}{3pt}

%==============================================================================
% Define Problem and Solution Environments
%==============================================================================
\theoremstyle{definition} % use definition's look
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\mdfsetup{ % box margin fix for mdframe and how it plays with parskip and others.
innerleftmargin=4pt,
innerrightmargin=4pt,
innertopmargin=-1pt,
innerbottommargin=4pt}
% \newenvironment{prob}{\begin{mdframed}\begin{problem}\hspace{0pt}}{\end{problem}\end{mdframed}}
\newenvironment{prob}{\clearpage \begin{problem}\hspace{0pt}}{\end{problem}}
\newenvironment{sol}{\begin{solution}\hspace{0pt}}{\end{solution}}

%==============================================================================
% New commands
%==============================================================================
\newcommand{\var}{\text{Var}}
\newcommand{\betahatls}{\widehat{\beta}_{0}^{\text{LS}}}
\newcommand{\betahatmle}{\widehat{\beta}_{0}^{\text{MLE}}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\epsilonhat}{\widehat{\bm{\epsilon}}}
\newcommand{\Tr}{\text{Tr}}
\newcommand{\diag}{\text{diag}}
\newcommand{\id}{\bm{I}_n}

%==============================================================================
% set knitr options
%==============================================================================
% latex (change space before and after knitr kframe; based on framed package)
\setlength{\OuterFrameSep}{0.3em}
% R
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
# inline hook to process the output of \Sexpr{} statements to just 2 digits
inline_hook <- function(x) {
  if(is.numeric(x)) x <- round(x, 2)
  paste(as.character(x), collapse=", ")
}
knit_hooks$set(inline = inline_hook)

# cache all chunks
opts_chunk$set(cache = FALSE)

# create directory for figures
if(!dir.exists("figures")){
  dir.create("figures")
}
@

\begin{document}


\maketitle

\section{Instructions}

\paragraph{Setup.} Pull the latest version of this assignment from Github and set your working directory to \texttt{stat-961-fall-2021/homework/homework-3}. Consult the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/getting-started.pdf}{getting started guide} if you need to brush up on \texttt{R}, \texttt{LaTeX}, or \texttt{Git}.

\paragraph{Collaboration.} The collaboration policy is as stated on the Syllabus:

\begin{quote}
``Students are permitted to work together on homework assignments, but solutions must be written up and submitted individually. Students must disclose any sources of assistance they received; furthermore, they are prohibited from verbatim copying from any source and from consulting solutions to problems that may be available online and/or from past iterations of the course.''
\end{quote}

\noindent In accordance with this policy, \\

\noindent \textit{Please list anyone you discussed this homework with:} \\
James Blume

\noindent \textit{Please list what external references you consulted (e.g. articles, books, or websites):} 

\paragraph{Writeup.} Use this document as a starting point for your writeup, adding your solutions between \verb|\begin{sol}| and \verb|\end{sol}|. See the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/preparing-reports.pdf}{preparing reports guide} for guidance on compilation, creation of figures and tables, and presentation quality. Show all the code you wrote to produce your numerical results, and include complete derivations typeset in LaTeX for the mathematical questions. 

\paragraph{Programming.}

The \texttt{tidyverse} paradigm for data manipulation (\texttt{dplyr}) and plotting (\texttt{ggplot2}) are strongly encouraged, but points will not be deducted for using base \texttt{R}. 
<<message=FALSE, cache = FALSE>>=
library(tidyverse)
@

\paragraph{Grading.} Each sub-part of each problem will be worth 3 points: 0 points for no solution or completely wrong solution; 1 point for some progress; 2 points for a mostly correct solution; 3 points for a complete and correct solution modulo small flaws. The presentation quality of the solution for each problem (as exemplified by the guidelines in Section 3 of the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/preparing-reports.pdf}{preparing reports guide}) will be evaluated out of an additional 3 points.

\paragraph{Submission.} Compile your writeup to PDF and submit to \href{https://www.gradescope.com/courses/284562}{Gradescope}.

\clearpage

\begin{prob} \label{prob:intercept-only}\textbf{Heteroskedasticity and correlated errors in the intercept-only model.} \\

\noindent Suppose that 
\begin{equation}
y_i = \beta_0 + \epsilon_i, \quad \text{where } \bm \epsilon \sim N(0, \bm \Sigma)
\label{eq:arbitrary-correlation}
\end{equation}
for some positive definite $\bm \Sigma \in \mathbb R^{n \times n}$. The goal of this problem is to investigate the effects of heteroskedasticity and correlated errors on the validity and efficiency of least squares estimation and inference.

\begin{enumerate}

\item[(a)] (Validity of least squares inference) What is the usual least squares estimate $\widehat \beta^{\text{LS}}_0$ for $\beta_0$ (from Unit 2)? What is its variance under the model~\eqref{eq:arbitrary-correlation}? What is the usual variance estimate $\widehat{\text{Var}}[\widehat \beta^{\text{LS}}_0]$ (from Unit 2), and what is this estimator's expectation under~\eqref{eq:arbitrary-correlation}? The ratio 
\begin{equation}
\tau_1 \equiv \frac{\mathbb E[\widehat{\text{Var}}[\widehat \beta^{\text{LS}}_0]]}{\text{Var}[\widehat \beta^{\text{LS}}_0]}
\label{eq:tau-1}
\end{equation}
is a measure of the validity of usual least squares inference under~\eqref{eq:arbitrary-correlation}. Write down an expression for $\tau_1$, and discuss the implications of $\tau_1$ for the Type-I error of the hypothesis test of $H_0: \beta_0 = 0$ and for the coverage of the confidence interval for $\beta_0$.

\item[(b)] (Efficiency of least squares estimator) Let's assume $\bm \Sigma$ is known. We could get valid inference based on $\widehat \beta^{\text{LS}}_0$ by using the variance formula from part (a). Alternatively, we could use the maximum likelihood estimate $\widehat \beta^{\text{ML}}_0$ for $\beta_0$. What is the variance of $\widehat \beta^{\text{ML}}_0$? The ratio
\begin{equation}
\tau_2 \equiv \frac{\text{Var}[\widehat \beta^{\text{LS}}_0]}{\text{Var}[\widehat \beta^{\text{ML}}_0]}
\label{eq:tau-2}
\end{equation}
is a measure of the efficiency of the usual least squares estimator under~\eqref{eq:arbitrary-correlation}, recalling that the maximum likelihood estimator is most efficient. Write down an expression for $\tau_2$, and discuss the implications of $\tau_2$ for the power of the hypothesis test of $H_0: \beta_0 = 0$ and for the width of the confidence interval for $\beta_0$.

\item[(c)] (Special case: Heteroskedasticity) Suppose $\bm \Sigma = \text{diag}(\sigma^2_1, \dots, \sigma^2_n)$ for some $\sigma^2_1, \dots, \sigma^2_n > 0$. Compute the ratios $\tau_1$ and $\tau_2$ defined in equations~\eqref{eq:tau-1} and~\eqref{eq:tau-2}, respectively. How do these ratios depend on $(\sigma^2_1, \dots, \sigma^2_n)$, and what are the implications for validity and efficiency?

\item[(d)] (Special case: Correlated errors) Suppose $(\epsilon_1, \dots, \epsilon_n)$ are \textit{equicorrelated}, i.e.
\begin{equation}
\Sigma_{j_1j_2} = 
\begin{cases}
1, \quad &\text{if } j_1 = j_2; \\
\rho, \quad &\text{if } j_1 \neq j_2. 
\end{cases}
\end{equation}
for some $\rho \geq 0$. Compute the ratios $\tau_1$ and $\tau_2$ defined in equations~\eqref{eq:tau-1} and~\eqref{eq:tau-2}, respectively. How do these ratios depend on $\rho$, and what are the implications for validity and efficiency?
\end{enumerate}

\end{prob}


\begin{sol}
\item[(a)] We know that the traditional least squares estimate for $\beta_0$ under the intercept-only model is $\betahatls = \overline{y} = (\bm{1}^T \bm{1})^{-1} \bm{1}^T \bm{y}$. 

Then 
\begin{align*}
  \var[\betahatls] &= \var[(\bm{1}^T \bm{1})^{-1} \bm{1}^T \bm{y}] \\
  &= (\bm{1}^T \bm{1})^{-1} \bm{1}^T \var[\bm{y}] [(\bm{1}^T \bm{1})^{-1} \bm{1}^T]^T \\
  &= \frac{1}{n^2} \bm{1}^T \var[\bm{y}] \bm{1} \\
  &= \frac{1}{n^2} \sum_{i,j = 1}^n (\bm{\Sigma})_{ij}.
\end{align*}
  
Also, we know that the traditional variance estimate is $\widehat{\var}[\betahatls] = \frac{\widehat{\sigma}^2}{n} = \frac{||\epsilonhat||^2}{n(n-p)}$.

Under~\eqref{eq:arbitrary-correlation}, we have that 
\begin{align*}
  \E[\widehat{\var}[\betahatls]] &= \E[\frac{||\epsilonhat||^2}{n(n-p)}] \\
  &= \frac{1}{n(n-p)} \E[\epsilonhat^T \epsilonhat] \\
  &= \frac{1}{n(n-p)} \E[\Tr(\epsilonhat^T \epsilonhat)] \\
  &= \frac{1}{n(n-p)} \E[\Tr(\epsilonhat \epsilonhat^T)] \\
  %&= \frac{1}{n(n-p)} \Tr(\E[\epsilonhat \epsilonhat^T]).
  &= \frac{1}{n(n-p)} \E[\sum_{i=1}^n \widehat{\epsilon}_i^2] \\
  &= \frac{1}{n(n-p)} \sum_{i=1}^n \var[\widehat{\epsilon}_i] \\
  &= \frac{1}{n(n-p)} \sum_{i=1}^n \sigma_i^2.
\end{align*}

We then have that 
\begin{align*}
  \tau_1 &= \frac{\E[\widehat{\var}[\betahatls]]}{\var[\betahatls]} \\
  &= \frac{n^{-1}(n-p)^{-1} \sum_{i=1}^n \sigma_i^2}{n^{-2} \sum_{i,j = 1}^n (\bm{\Sigma})_{ij}} \\
  &= (\frac{n}{n-p}) \frac{\sum_{i=1}^n \sigma_i^2}{\sum_{i,j=1}^n (\bm{\Sigma})_{ij}}
\end{align*}

When $\tau_1 > 1$, we have that $\E[\widehat{\var}[\betahatls]] > \var[\betahatls]$. Then the expected estimated variance of the OLS estimator exceeds that of the true variance. Consequently, we would have that both the Type-I error of the hypothesis test $H_0: \beta_0 = 0$ and the coverage for the confidence interval for $\beta_0$ increase when $\tau > 1$, relative to the case in which we have homoskedasticity (likewise these quantities decrease for $\tau < 1$).

\item[(b)] Recall that the general likelihood function for a multivariate normal with mean $\bm{\beta}$ and covariance $\bm{\Sigma}$ is 
\[L(\bm{y}) = \frac{1}{\sqrt{(2 \pi)^n |\bm{\Sigma}|}} \exp[-\frac{1}{2}(\bm{y} - \bm{X} \bm{\beta})^T \bm{\Sigma}^{-1} (\bm{y} - \bm{X} \bm{\beta})].\] 
Since $y \sim N(\beta_0, \bm{\Sigma})$, the likelihood function for the intercept-only linear model with generalized covariance is \[L(\bm{y}) = (2 \pi)^{-n/2} |\bm{\Sigma}|^{-1/2} \exp[-\frac{1}{2}(\bm{y} - \bm{X} \bm{\beta})^T \bm{\Sigma}^{-1} (\bm{y} - \bm{X} \bm{\beta})]\] and the log-likelihood is
\[-\frac{n}{2}\log(2 \pi) - \frac{1}{2} \log|\bm{\Sigma}| - \frac{1}{2}[\bm{y} - \beta_0 \bm{1})^T \bm{\Sigma}^{-1} (\bm{y} - \beta_0 \bm{1})].\]

So,
\begin{align*}
  \frac{\partial \ell}{\partial \beta_0}(\bm{y}) &= - \frac{1}{2} \frac{\partial}{\partial \beta_0} [(\bm{y} - \beta_0 \bm{1})^T \bm{\Sigma}^{-1} (\bm{y} - \beta_0 \bm{1})] \\
  &= -\beta_0 (\bm{1}^T \bm{\Sigma}^{-1} \bm{1}) + \bm{1}^T \bm{\Sigma}^{-1} \bm{y}.
\end{align*}
Then $\frac{\partial \ell}{\partial \beta_0}(\bm{y}) = 0$ if and only if $\beta_0 = (\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-1} \bm{1}^T \bm{\Sigma}^{-1} \bm{y}$, so 
\[\betahatmle = (\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-1} \bm{1}^T \bm{\Sigma}^{-1} \bm{y}.\]
So,
\begin{align*}
  \var[\betahatmle] &= \var[(\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-1} \bm{1}^T \bm{\Sigma}^{-1} \bm{y}] \\
  &= [(\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-1} \bm{1}^T \bm{\Sigma}^{-1}] \var[\bm{y}] [(\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-1} \bm{1}^T \bm{\Sigma}^{-1}]^T \\
  &= [(\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-1} \bm{1}^T \bm{\Sigma}^{-1}] \bm{\Sigma} [(\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-1} \bm{1}^T \bm{\Sigma}^{-1}]^T \\
  &= (\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-1} \bm{1}^T [\bm{1}^T \bm{\Sigma}^{-1}]^T [(\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-1}]^T \\
  &= (\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-2} \bm{1}^T (\bm{\Sigma}^{-1})^T \bm{1} \\
  &= (\bm{1}^T \bm{\Sigma}^{-1} \bm{1})^{-1} \\
  &= (\sum_{i,j=1}^n(\bm{\Sigma}^{-1})_{ij})^{-1}.
\end{align*}
We then have that 
\[\tau_2 = \frac{\var[\betahatls]}{\var[\betahatmle]} = \frac{1}{n^2} (\sum_{i,j=1}^n(\bm{\Sigma}^{-1})_{ij}) (\sum_{i,j=1}^n(\bm{\Sigma})_{ij}).\]

When $\tau_2 > 1$, we have that $\var[\betahatls] > \var[\betahatmle]$. This means that we will have lower variance as compared to the homoskedastic case and thus have higher power. Additionally, the lower variance means that the CI widths will also be narrower as compared to the homoskedastic case.

\item[(c)] In the case of heteroskedasticity, we have that that since $\bm{\Sigma} = \diag(\sigma_1^2, \dots, \sigma_n^2)$, $\widehat{\bm{\Sigma}} = \diag(\widehat{\sigma}_1^2, \dots, \widehat{\sigma}_n^2)$, where $\widehat{\sigma}_i^2 = (y_i - \widehat{\beta}_0)^2$, and $\bm{\Sigma}^{-1} = \diag(\sigma_1^{-2}, \dots, \sigma_n^{-2})$.

So, \[\tau_1 = (\frac{n}{n-p}) \frac{\sum_{i=1}^n \sigma_i^2}{\sum_{i=1}^n \sigma_i^2} = \frac{n}{n-p}\]

Also, \[\tau_2 = \frac{1}{n^2} (\sum_{i=1}^n \sigma_i^{-2}) (\sum_{i=1}^n \sigma_i^2).\]

Note that because $1 \leq p < n$, we have that $n-p < n$, so $\tau_1 > 1$. That means we will have a higher Type-I error as compared to the case in which we have homoskedasticity.

When $(\sum_{i=1}^n \sigma_i^{-2}) (\sum_{i=1}^n \sigma_i^2) > n^2$, $\tau_2 > 1$ and we have that power increases relative to the homoskedastic case.



\item[(d)] We have \[\tau_1 = (\frac{n}{n-p}) \bigg( \frac{\sum_{i=1}^n \sigma_i^2}{\sum_{i,j=1}^n (\bm{\Sigma})_{ij}} \bigg) = (\frac{n}{n-\rho}) (1 + \rho (n-1))^{-1}.\]

Note that we can write $\bm{\Sigma} = (1 - \rho) \bm{I}_n + \rho \bm{1}\bm{1}^T$. Then 
\[\bm{\Sigma}^{-1} = [(1-\rho)(\id + \frac{\rho}{1-\rho} \bm{1}\bm{1}^T)]^{-1} = (1-\rho)^{-1}[\id + (\frac{\rho}{1-\rho} \bm{1}\bm{1}^T)]^{-1}.\]
Using the Sherman Morrison formula we can simplify find a more explicit solution:
\begin{align*}
  \bm{\Sigma}^{-1} &= (1-\rho)^{-1}[\id + (\frac{\rho}{1-\rho} \bm{1}\bm{1}^T)]^{-1} \\
  &= (1-\rho)^{-1} \bigg[ \id - \frac{\id [(\frac{\rho}{1-\rho}) \bm{1} \bm{1}^T] \id}{1 + \bm{1}^T \id (\frac{\rho}{1-\rho}) \bm{1}} \bigg] \\
  &= (1-\rho)^{-1} \bigg[\id - \frac{(\frac{\rho}{1-\rho}) \bm{1} \bm{1}^T}{1 + (\frac{\rho}{1-\rho}) \bm{1}^T  \bm{1}} \bigg] \\
  &= (1-\rho)^{-1} \bigg[\id - \frac{(\frac{\rho}{1-\rho}) \bm{1} \bm{1}^T}{1 + (\frac{\rho}{1-\rho}) n } \bigg] \\
  &= (1-\rho)^{-1} \bigg[\id - \frac{\rho}{1 + \rho(n-1)} \bm{1}\bm{1}^T \bigg].
\end{align*}
So, 
\begin{align*}
  \sum_{i,j=1}^n (\bm{\Sigma}^{-1})_{ij} &= (1-\rho)^{-1} \bigg[ n - \frac{n^2 \rho}{1 + \rho(n-1)} \bigg] \\
  &= (1-\rho)^{-1} \bigg[ \frac{n + n^2\rho - n\rho - n^2\rho}{1 + \rho(n-1)} \bigg] \\
  &= (1-\rho)^{-1} \bigg[ \frac{n(1-\rho)}{1 + \rho(n-1)} \bigg] \\
  &= \frac{n}{1 + \rho(n-1)}.
\end{align*}

Then 
\begin{align*}
  \tau_2 &= \frac{1}{n^2} (\sum_{i,j=1}^n(\bm{\Sigma}^{-1})_{ij}) (\sum_{i,j=1}^n(\bm{\Sigma})_{ij}) \\
  &= \frac{1}{n^2} \bigg[ \frac{n}{1 + \rho(n-1)} \bigg][n + n(n-1)\rho] \\
  &= 1.
\end{align*}

As before, we have $\frac{n}{n-p} > 1$. Note that $(1 + \rho(n-1))^{-1} < 1$, but tends toward $0$ as $n \rightarrow \infty$. Since $\tau_1$ is simply $\frac{n}{n-p}$ scaled by this quantity, this means that the Type-I error rate decreases relative to the homoskedastic case (and more generally, toward 0) as sample size increases.

Because $\tau_2 = 1$, we have that the power is the same relative to the homoskedastic case.

\end{sol}

\begin{prob} \label{prob:robust-standard-errors}\textbf{Comparing constructions of heteroskedasticity-robust standard errors.} \\

\noindent Suppose that
\begin{equation}
\bm y = \bm X \bm \beta + \bm \epsilon, \quad \epsilon_i \overset{\text{ind}}\sim N(0, \sigma^2_i).
\end{equation}
Two approaches to obtaining heteroskedasticity-robust standard errors are the pairs bootstrap and Huber-White standard errors. The goal of this problem is to compare the coverage and width of confidence intervals obtained from these two approaches.

\begin{enumerate}
\item[(a)] Write a function called \verb|pairs_bootstrap|, which inputs arguments $\bm X$, $\bm y$, and $B$ and outputs an estimated $p \times p$ covariance matrix $\widehat{\text{Var}}[\widehat{\bm \beta}]$ based on $B$ resamples of the pairs bootstrap.

\item[(b)] Write a function called \verb|huber_white|, which inputs arguments $\bm X$ and $\bm y$ and outputs an estimated $p \times p$ covariance matrix $\widehat{\text{Var}}[\widehat{\bm \beta}]$ based on the Huber-White formula.

\item[(c)] Generate $n = 50$ $(x,y)$ pairs by setting $x$ to be equally-spaced values between 0 and 1 and drawing $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$, where $\epsilon_i \overset{\text{ind}}\sim N(0, 9x_i^2)$, $\beta_0 = 2, \beta_1 = 3$. Create a scatter plot of these points, the least squares line, and three confidence bands: the standard least squares confidence band as well as those resulting from the pairs bootstrap (with $B = 500$) and the Huber-White formula. Comment on the relative widths of these three bands depending on the value of $x$.

\item[(d)] Repeat the experiment from part (c) 100 times to compute the coverage and average width of the three confidence bands for each value of $x$. Plot these two metrics as a function of $x$, and comment on the results.

\end{enumerate}

\end{prob}

\begin{sol}

\item[(a)]
<<pairs bootstrap>>=
pairs_bootstrap <- function(X, y, B){
  # Vector to hold estimates of beta
  beta_b <- 
    data.frame(
      matrix(
        NA, 
        nrow=length(y), 
        ncol=ncol(X)+1))

  # Run bootstrap B times
  for(i in 1:B){
    # Get length(y) rows sampled uniformly
    sample_ind <- sample(x = 1:length(y), size = length(y), replace = TRUE)
    X_b <- data.frame(X[sample_ind, ])
    y_b <- y[sample_ind]
    
    # Combine sample data into new dataframe
    df_b <- X_b
    df_b[, "y_b"] <- y_b
   
    # Run lm and add resulting estimates to beta_b dataframe
    lm_b <- lm(y_b ~ ., data = df_b)
    
    # Store coefficients
    beta_b[i, ] <- lm_b$coefficients
  }
  
  # Get estimated covariance matrix
  var_hat_beta_hat <- cov(beta_b)
  
  return(var_hat_beta_hat)
}
@

\item[(b)]
<<huber white standard errors>>=
huber_white <- function(X, y){
  # Combine X, y
  df_comb <- X
  df_comb$y <- y
  
  # Get linear model, extract residuals
  lm_hw <- lm(y ~ ., data = df_comb)
  
  epsilon_hat <- lm_hw$residuals

  # Augment X to add intercept column
  X_aug <- model.matrix(lm_hw)
  
  # Sandwich estimator is  X^T sigma_hat X
  sigma_hat <- diag(epsilon_hat^2)
  sandwich <- t(X_aug) %*% sigma_hat %*% X_aug
  bread <- solve(t(X_aug) %*% X_aug)
  
  var_hat_beta_hat <- bread %*% sandwich %*% bread
  
  return(var_hat_beta_hat)
}
@

\item[(c)] 
<<robust sim>>=
# Function for computing confidence intervals
get_ci <- function(x_i, beta_hat, sigma_hat, quantile){
  # Get fitted values
  y_hat <- t(x_i) %*% beta_hat
  
  
  # Get SEs
  se <- sqrt(t(x_i) %*% sigma_hat %*% x_i)
  
  # Compute CI bounds
  ci <- data.frame(cb_lower = y_hat - quantile * se, 
                   cb_upper = y_hat + quantile * se)
  
  return(ci)
}

# Function for computing confidence bounds
get_cb <- function(data, beta_hat, sigma_hat, quantile){
  # Get CI bounds for each x_i in data
  ci <- apply(data, MARGIN=1, 
              FUN=function(x){get_ci(x, beta_hat, sigma_hat, quantile)})
  
  # Combine list of bounds into dataframe
  cb <- bind_rows(ci, .id="column_label") %>% select(-column_label)
  
  return(cb)
}


# Simulation parameters
#  - n: Number of data points
#  - beta_0: Intercept
#  - beta_1: Slope
#  - B: Number of times bootstrap run
#  - quantile: For constructing CIs
n <- 50
beta_0 <- 2
beta_1 <- 3

B <- 500
quantile <- 2


# Sample (x,y)
x <- runif(n = n, min = 0, max = 1)
epsilon_i <- rnorm(n = n, mean = 0, sd = 9 * x)
y <- beta_0 + beta_1 * x + epsilon_i

# Combine simulation data together
df_sim <- data.frame(y=y, x=x)
                     
# Run lm, extract coefficients
lm_sim <- lm(y ~ ., df_sim)
coeff_sim <- lm_sim$coefficients

# Get model matrix
model_mx <- model.matrix(lm_sim$model)


# Get pairs bootstrap SEs
pairs_sigma_hat <- 
  pairs_bootstrap(X=data.frame(x = df_sim$x), y=df_sim$y, B=B)

# Get pairs bootstrap CBs
pairs_cb <-
  get_cb(model_mx, coeff_sim, pairs_sigma_hat, quantile)
colnames(pairs_cb) <- paste0("pairs_", colnames(pairs_cb))

# Get HW SEs
hw_sigma_hat <- 
  huber_white(X=data.frame(x = df_sim$x), y=df_sim$y)

# Get HW CBs
hw_cb <-
  get_cb(model_mx, coeff_sim, hw_sigma_hat, quantile)
colnames(hw_cb) <- paste0("hw_", colnames(hw_cb))


# Combine CBs with data
df_sim1 <-
  cbind(df_sim, pairs_cb, hw_cb)

# Make plot
sim_plt <-
  df_sim1 %>%
    ggplot(aes(x=x, y=y, color=NA, fill=NA), alpha=0.2) + 
    # Scatter plot points
    geom_point(color="black") +
    # LS line and confidence band
    stat_smooth(aes(color="OLS Line", fill="OLS confidence band"), 
                method='lm', formula=y~x, level=0.95) +
  # Pairs bootstrap confidence band
    geom_ribbon(aes(ymin=pairs_cb_lower, ymax=pairs_cb_upper, 
                    color="Boostrap confidence band", fill=NA), alpha=0.2) +
  # HW SEs confidence band
    geom_ribbon(aes(ymin=hw_cb_lower, ymax=hw_cb_upper, 
                    color="HW confidence band", fill=NA), alpha=0.2) +
    # Labels
    xlab("x") +
    ylab("y") + 
    # Manual scales
    scale_color_manual(
      breaks = c("OLS Line", 
                 "Boostrap confidence band", 
                 "HW confidence band"), 
      values = c("black", 
                 "green", 
                 "blue")) +
    # Fix legend
    guides(
      color=guide_legend(override.aes=list(fill=NA), order=1, title=NULL),
      fill=guide_legend(override.aes=list(color=NA), order=2, title=NULL)
    )

ggsave(plot = sim_plt, filename = "./figures/sim_plt.png",
       device = "png", width = 9, height = 6)
@

\begin{figure}[h!]
\centering
\includegraphics[width = 1\textwidth]{figures/sim_plt.png}
\caption{Simulated comparison of confidence bands for robust SE estimates.}
\label{fig:sim_plt}
\end{figure}

We see that the bootstrap and Huber-White confidence bands have similar widths and are narrower than the OLS confidence band up to about $x = 0.6$ and are wider for $x > 0.6$. The width of the OLS confidence band decreases until about $x = 0.65$ and increases thereafter, while the robust methods have bands that decrease in width up to $x = 0.4$ and increase after.

\item[(d)] 
<<sim 2>>=
# Function for getting estimated covariance with OLS
ols <- function(X, y){
  # Combine X, y
  df_comb <- X
  df_comb$y <- y
  
  # Get linear model, extract residuals
  lm_hw <- lm(y ~ ., data = df_comb)
  
  epsilon_hat <- lm_hw$residuals

  # OLS variance estimator is ||epsilon_hat||^2/(n-p)
  sigma_hat_2 <- sum(epsilon_hat^2)/(length(y) - ncol(X))
  
  # Augment X to add intercept column
  X_aug <- model.matrix(lm_hw)
  
  # Sandwich estimator
  var_hat_beta_hat <- sigma_hat_2 * solve(t(X_aug) %*% X_aug)
  
  return(var_hat_beta_hat)
}


# Number of times simulation is run
n_sim <- 100

# Sample x
x <- runif(n = n, min = 0, max = 1)


# Function to run simulation
do_sim <- function(beta_0, beta_1, n, B, quantile){
  # Sample y
  epsilon_i <- rnorm(n = n, mean = 0, sd = 9 * x)
  y <- beta_0 + beta_1 * x + epsilon_i

  # Combine simulation data together
  df_sim <- data.frame(y=y, x=x)
  
  # Run lm, extract coefficients
  lm_sim <- lm(y ~ ., df_sim)
  coeff_sim <- lm_sim$coefficients

  # Get model matrix
  model_mx <- model.matrix(lm_sim$model)
  
  
  # Get OLS SEs
  ols_sigma_hat <- 
    ols(X=data.frame(x = df_sim$x), y=df_sim$y)
  
  # Get OLS CBs
  ols_cb <-
    get_cb(model_mx, coeff_sim, ols_sigma_hat, quantile)
  colnames(ols_cb) <- paste0("ols_", colnames(ols_cb))
  
  # Get pairs bootstrap SEs
  pairs_sigma_hat <- 
    pairs_bootstrap(X=data.frame(x = df_sim$x), y=df_sim$y, B=B)

  # Get pairs bootstrap CBs
  pairs_cb <-
    get_cb(model_mx, coeff_sim, pairs_sigma_hat, quantile)
  colnames(pairs_cb) <- paste0("pairs_", colnames(pairs_cb))
  
  # Get HW SEs
  hw_sigma_hat <- 
    huber_white(X=data.frame(x = df_sim$x), y=df_sim$y)
  
  # Get HW CBs
  hw_cb <-
    get_cb(model_mx, coeff_sim, hw_sigma_hat, quantile)
  colnames(hw_cb) <- paste0("hw_", colnames(hw_cb))
  
  
  # Combine CBs with data
  df_sim1 <-
    cbind(df_sim, ols_cb, pairs_cb, hw_cb)

  # Get widths, coverage for each CB
  cb_results <- 
    df_sim1 %>%
      # Column for computing CB widths
      mutate(
        ols_width = ols_cb_upper-ols_cb_lower,
        ols_covered = 
           (y >= ols_cb_lower & 
            y <= ols_cb_upper),
        pairs_width = pairs_cb_upper-pairs_cb_lower,
        pairs_covered = 
           (y >= pairs_cb_lower & 
            y <= pairs_cb_upper),
         hw_width = hw_cb_upper-hw_cb_lower,
         hw_covered = 
           (y >= hw_cb_lower & 
            y <= hw_cb_upper)) 
  
  
  # Get widths in long format
  cb_widths <- 
    cb_results %>% 
      select(x, ols_width, pairs_width, hw_width) %>% 
      # Reshape to long format
      pivot_longer(c(ols_width, pairs_width, hw_width), 
                   names_to = "method", 
                   values_to = "width") %>%
      # Subset of strings in method column
      mutate(method = str_replace(method, "_.*", ""))
  
  # Get coverage in long format
  cb_covered <-
    cb_results %>%
      select(x, ols_covered, pairs_covered, hw_covered) %>% 
      # Reshape to long format
      pivot_longer(c(ols_covered, pairs_covered, hw_covered),
                   names_to = "method", 
                   values_to = "covered") %>%
      # Subset of strings in method column
      mutate(method = str_replace(method, "_.*", ""))
  
  # Combine long formats
  cb_results1 <- 
    cb_widths %>%
      inner_join(cb_covered, by=c("x", "method"))
  
  return(cb_results1)
}


# Get CB average widths and coverage for each simulation run
cb_list <- lapply(1:n_sim, 
                  function(x){
                    do_sim(beta_0, beta_1, n, B, quantile)
                  })
# Combine list of simulation results together
cb_df <- do.call(rbind, cb_list)

ci_results <- 
  cb_df %>%
    # Group by x, method
    group_by(x, method) %>%
    # Compute average width and coverage for each x, method
    mutate(avg_width = mean(width),
           coverage = mean(covered)) %>%  
    # Ungroup
    ungroup() %>%
    # Drop width, covered columnd
    select(-width, -covered) %>%
    # Take only distinct rows
    distinct() %>%
    # Replace method with full method names
    mutate(
      method = 
        str_replace(
          str_replace(
            str_replace(method, 
            "pairs", "Pairs bootstrap"), 
          "hw", "Huber-White"), 
        "ols", "OLS")) %>%
    rename(Method = method)

# plot for average width by method
cb_width_plt <-
  ci_results %>%
    ggplot(aes(x=x, y=avg_width, color=Method)) +
      geom_point() +
      ylab("Average CB width")

# Plot for coverage by method
cb_coverage_plt <-
  ci_results %>%
    ggplot(aes(x=x, y=coverage, color=Method)) +
      geom_point() +
      ylab("CB coverage")
  
ggsave(plot = cb_width_plt, filename = "./figures/cb_width_plt.png",
       device = "png", width = 6, height = 4)

ggsave(plot = cb_coverage_plt, filename = "./figures/cb_coverage_plt.png",
       device = "png", width = 6, height = 4)
@

\begin{figure}[h!]
\centering
\includegraphics[width = .8\textwidth]{figures/cb_width_plt.png}
\caption{Average CB width as a function of $x$ ($n=100$).}
\label{fig:cb_width_plt}
\end{figure}

Here we have that for the pairs bootstrap and Huber-White robust estimators, the average confidence bound width decreases as $x$ approaches $0.3$ and then begins to increase again. A similar pattern occurs with the OLS average confidence bound widths with the switch from decreasing to increasing occurs around $x = 0.5$.

\begin{figure}[h!]
\centering
\includegraphics[width = .8\textwidth]{figures/cb_coverage_plt.png}
\caption{CB coverage as a function of $x$ ($n=100$).}
\label{fig:cb_coverage_plt}
\end{figure}

We have that the coverage of the OLS is greater than that of the robust methods up to $x = 0.5$, but the robust methods have generally better coverage for $x > 0.5$.

\end{sol}

\begin{prob} \label{prob:ad-data}\textbf{Case study: Advertising data..} \\

\noindent In this problem, we will analyze a data set related to advertising spending. It contains the sales of a product (in thousands of units) in 200 different markets, along with advertising budgets (in thousands of dollars) for the product in each of those markets for three different media: TV, radio, and newspaper. The goal is to learn about the relationship between these three advertising budgets (predictors) and sales (response).

<<message = FALSE>>=
ads_data = read_tsv("../../data/Advertising.tsv")
print(ads_data, n = 5)
@

\begin{enumerate}

\item[(a)] Run a linear regression of \verb|sales| on \verb|TV|, \verb|radio|, and \verb|newspaper|, and produce a set of standard diagnostic plots. What model misspecification issue(s) appear to be present in these data?

\item[(b)] Address the above misspecification issues using one or more of the strategies discussed in Unit 3. Report a set of statistical estimates, confidence intervals, and test results you think you can trust. 

\item[(c)] Discuss the findings from part (b) in language that a policymaker could comprehend, including any caveats or limitations of the analysis.

\end{enumerate}

\end{prob}

\begin{sol}

\item[(a)]
<<ads lm, warning=FALSE, message=FALSE, results='hide'>>=
ads_lm <- lm(sales ~ TV + radio + newspaper, ads_data)

png("./figures/residual_plt.png")
plot(ads_lm, which = 3)

png("./figures/qq_plt.png")
plot(ads_lm, which = 2)
dev.off()

png("./figures/leverage_plt.png")
plot(ads_lm, which = 5)
dev.off()
@

\begin{figure}[h!]
\centering
\includegraphics[width = .7\textwidth]{figures/residual_plt.png}
\caption{Standardized residuals vs. fitted values.}
\label{fig:residual_plt}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width = .7\textwidth]{figures/qq_plt.png}
\caption{Quantile-quantile plot.}
\label{fig:qq_plt}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width = .7\textwidth]{figures/leverage_plt.png}
\caption{Standardized residuals vs. leverage.}
\label{fig:leverage_plt}
\end{figure}

In the residual plot we can clearly see that there is heteroskedasticity, where the residuals appear to be approximately a quadratic function of the fitted values $\hat{t}$. Furthermore, the QQ-plot deviates significantly from the QQ-line at the tails, indicating that the residuals are not normally distributed. Based off Cook's distance, it appears that the data may not contain outliers.

\item[(b)]
A quick glance at the pairs plot with the log- and square root-transforms of sales indicates that each of these transforms does little to account for heteroskedasticity when looking at the marginal distribution of each variable of interest.
<<misspecification fixes, warning=FALSE, message=FALSE, results='hide'>>=
ads_data_aug <- 
  ads_data %>%
  mutate(sqrt_sales = sqrt(sales),
         log_sales = log(sales))

pairs_plt <- 
  GGally::ggpairs(
    ads_data_aug, 
    columnLabels = 
      c("TV",
        "Radio",
        "Newspaper",
        "Sales",
        # ggpairs does not provide a way to use LaTeX expressions when changing column names
        #latex2exp::TeX("\\sqrt{\\text{Sales}}"),
        #latex2exp::TeX("\\log{\\text{Sales}}")))
        "(Sales)^(1/2)",
        "log(Sales)"))
ggsave("./figures/pairs_plt.png", pairs_plt)
@

\begin{figure}[h!]
\centering
\includegraphics[width = .75\textwidth]{figures/pairs_plt.png}
\caption{Pairs plot.}
\label{fig:pairs_plt}
\end{figure}

We instead perform our statistical inference using standard errors derived from the pairs bootstrap in order to account for apparent the heteroskedasticity.

<<robust inference, sanitize=TRUE>>=
X_ads <- ads_data %>% select(-sales)
sales <- ads_data$sales
B <- 500


# Run lm, extract coefficients
lm_ads <- lm(sales ~ ., ads_data)
coeff_ads <- lm_ads$coefficients

# Get model matrix
ads_model_mx <- cbind(rep(1, length(sales)), X_ads)
colnames(ads_model_mx)[1] <- "(Intercept)"


# Get sigma_hat
ads_pairs_sigma_hat <- 
  pairs_bootstrap(X_ads, sales, B)

# Extract standard errors
ads_se <- 
  ads_pairs_sigma_hat %>% 
    diag() %>% 
    sqrt()

# Dataframe to hold all estimated values
est_df <- 
  # Add beta_hat
  data.frame(beta_i_hat = coeff_ads,
             sigma_i_hat = ads_se) %>%
    # Compute CI bounds, z-statistic
    mutate(CI_lower = beta_i_hat - 2 * sigma_i_hat,
           CI_upper = beta_i_hat + 2 * sigma_i_hat,
           z = beta_i_hat/sigma_i_hat) %>%
    # Compute approximate p-value
    mutate(p_val = 2 * pnorm(-abs(z)))

colnames(est_df) <-
  c("$\\widehat{\\beta}_i$",
    "$\\widehat{\\sigma}_i$",
    "CI lower bound",
    "CI upper bound",
    "$z$",
    "p-value")
rownames(est_df) <-
  c("Intercept",
    "TV",
    "Radio",
    "Newspaper")

est_df %>%
  kableExtra::kable(format = "latex", booktabs = TRUE, digits = 4, escape=FALSE) %>%
  kableExtra::save_kable("figures/robust_est_tbl.png")
@

\begin{table}[h!]
\centering
\includegraphics[width = 1\textwidth]{figures/robust_est_tbl.png}
\caption{Heteroskedastic robust inference estimates.}
\label{tab:robust_est_tbl}
\end{table}

The outcomes of the robust inference, including coefficient estimates, CI bounds, and p-values are provided in table~\ref{tab:robust_est_tbl}.

% TODO: Summarize inference
\item[(c)] The model suggests that both TV and radio ads are associated with increased sales, though there is no evidence to suggest that newspapers ads are associated with higher sales. Assuming that both TV and radio ads are measured on the same scale, it seems that a one unit increase in radio ads is associated with a larger increase in sales than that of a one unit increase in TV ads. 

One thing to note is that this model is not causal and thus looks simply at associations between variables rather than whether one causes another. Additionally, this model assumes a linear relationship between the variables of interest, which may not be correct.


\end{sol}



\end{document}