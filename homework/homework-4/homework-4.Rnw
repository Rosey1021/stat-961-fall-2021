% (C) Brett Klamer - MIT - http://opensource.org/licenses/MIT
% Please contact me if you find any errors or make improvements
% Contact details at brettklamer.com

\documentclass[11pt,letterpaper,english,oneside]{article} % article class is a standard class
%==============================================================================
%Load Packages
%==============================================================================
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % easy page margins
\usepackage[utf8]{inputenc} % editor uses utf-8 encoding
\usepackage[T1]{fontenc} % T1 font pdf output
\usepackage{lmodern} % Latin modern roman font
\usepackage{bm, bbm} % bold and blackboard bold math symbols
\usepackage{amsmath, amsfonts, amssymb, amsthm} % math packages
\usepackage[final]{microtype} % better microtypography
\usepackage{graphicx} % for easier grahics handling
\usepackage[hidelinks, colorlinks=true, linkcolor = blue, urlcolor = blue]{hyperref} % to create hyperlinks
\usepackage{float} % tells floats to stay [H]ere!
\usepackage{mdframed} % it's better than framed. knitr uses framed so settings won't conflict
\usepackage{enumitem} % nice lists
\usepackage{fancyhdr} % nice headers
\usepackage{caption}  % to control figure and table captions

\captionsetup{width=0.9\textwidth, justification = raggedright}

%==============================================================================
% Enter name and homework title here
%==============================================================================
\author{Sam Rosenberg}
\title{STAT 961: Homework 4}
\date{Due Friday, November 19 at 11:59pm}

%==============================================================================
% Put title and author in PDF properties
%==============================================================================
\makeatletter % change interpretation of @
\hypersetup{pdftitle={\@title},pdfauthor={\@author}}


%==============================================================================
% Header settings
%==============================================================================
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @

%==============================================================================
% List spacing
%==============================================================================
\setlist[itemize]{parsep=0em} % fix itemize spacing
\setlist[enumerate]{parsep=0em} % fix enumerate spacing

%==============================================================================
% Float spacing (changes spacing of tables, graphs, etc)
%==============================================================================
%\setlength{\textfloatsep}{3pt}
%\setlength{\intextsep}{3pt}

%==============================================================================
% Define Problem and Solution Environments
%==============================================================================
\theoremstyle{definition} % use definition's look
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\mdfsetup{ % box margin fix for mdframe and how it plays with parskip and others.
innerleftmargin=4pt,
innerrightmargin=4pt,
innertopmargin=-1pt,
innerbottommargin=4pt}
% \newenvironment{prob}{\begin{mdframed}\begin{problem}\hspace{0pt}}{\end{problem}\end{mdframed}}
\newenvironment{prob}{\clearpage \begin{problem}\hspace{0pt}}{\end{problem}}
\newenvironment{sol}{\begin{solution}\hspace{0pt}}{\end{solution}}

%==============================================================================
% New commands
%==============================================================================

\newcommand{\bbetahatmle}{\widehat{\bm{\beta}}^{\text{MLE}}}
\newcommand{\betahatmle}{\widehat{\beta}^{\text{MLE}}}
\newcommand{\muhatmle}{\widehat{\mu}^{\text{MLE}}}
\newcommand{\bmuhatmle}{\widehat{\bm{\mu}}^{\text{MLE}}}
\newcommand{\bbetahatconst}{\widehat{\bm{\beta}}^{\text{const}}}
\newcommand{\betahatconst}{\widehat{\beta}_{0}^{\text{const}}}
\newcommand{\muhatconst}{\widehat{\mu}^{\text{const}}}
\newcommand{\bmuhatconst}{\widehat{\bm{\mu}}^{\text{const}}}
\newcommand{\epsilonhat}{\widehat{\bm{\epsilon}}}

\newcommand{\Var}{\text{Var}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Tr}{\text{Tr}}
\newcommand{\diag}{\text{diag}}
\newcommand{\id}{\bm{I}_n}

\newcommand{\CI}{\text{CI}}
\newcommand{\SE}{\text{SE}}
\newcommand{\LRT}{\text{LRT}}
\newcommand{\R}{\mathbb{R}}

\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\bW}{\bm{W}}
\newcommand{\bx}{\bm{x}}
\newcommand{\bX}{\bm{X}}
\newcommand{\by}{\bm{y}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bzero}{\bm{0}}

%==============================================================================
% set knitr options
%==============================================================================
% latex (change space before and after knitr kframe; based on framed package)
\setlength{\OuterFrameSep}{0.3em}
% R
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
# inline hook to process the output of \Sexpr{} statements to just 2 digits
inline_hook <- function(x) {
  if(is.numeric(x)) x <- round(x, 2)
  paste(as.character(x), collapse=", ")
}
knit_hooks$set(inline = inline_hook)

# cache all chunks
opts_chunk$set(cache = TRUE)

# create directory for figures
if(!dir.exists("figures")){
  dir.create("figures")
}
@

\begin{document}


\maketitle

\section{Instructions}

\paragraph{Setup.} Pull the latest version of this assignment from Github and set your working directory to \texttt{stat-961-fall-2021/homework/homework-4}. Consult the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/getting-started.pdf}{getting started guide} if you need to brush up on \texttt{R}, \texttt{LaTeX}, or \texttt{Git}.

\paragraph{Collaboration.} The collaboration policy is as stated on the Syllabus:

\begin{quote}
``Students are permitted to work together on homework assignments, but solutions must be written up and submitted individually. Students must disclose any sources of assistance they received; furthermore, they are prohibited from verbatim copying from any source and from consulting solutions to problems that may be available online and/or from past iterations of the course.''
\end{quote}

\noindent In accordance with this policy, \\

\noindent \textit{Please list anyone you discussed this homework with:} \\

\noindent \textit{Please list what external references you consulted (e.g. articles, books, or websites):} 

\paragraph{Writeup.} Use this document as a starting point for your writeup, adding your solutions between \verb|\begin{sol}| and \verb|\end{sol}|. See the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/preparing-reports.pdf}{preparing reports guide} for guidance on compilation, creation of figures and tables, and presentation quality. Show all the code you wrote to produce your numerical results, and include complete derivations typeset in LaTeX for the mathematical questions. 

\paragraph{Programming.}

The \texttt{tidyverse} paradigm for data manipulation (\texttt{dplyr}) and plotting (\texttt{ggplot2}) are strongly encouraged, but points will not be deducted for using base \texttt{R}. 
<<message=FALSE, cache = FALSE>>=
library(tidyverse)
@

\paragraph{Grading.} Each sub-part of each problem will be worth 3 points: 0 points for no solution or completely wrong solution; 1 point for some progress; 2 points for a mostly correct solution; 3 points for a complete and correct solution modulo small flaws. The presentation quality of the solution for each problem (as exemplified by the guidelines in Section 3 of the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/preparing-reports.pdf}{preparing reports guide}) will be evaluated out of an additional 3 points.

\paragraph{Submission.} Compile your writeup to PDF and submit to \href{https://www.gradescope.com/courses/284562}{Gradescope}.

\clearpage

\begin{prob} \label{prob:ci-calculation}\textbf{Inverting the Wald, likelihood ratio, and score tests for a Poisson GLM.} \\

\noindent You have two email accounts: your personal one and your academic one. Last month, you received $y_1$ and $y_2$ emails in your personal and academic inboxes, respectively. Interested in the extent to which you receive more (or less) email in your academic inbox, you set up the following Poisson regression model:
\begin{equation*}
y_i \overset{\text{ind}} \sim \text{Poi}(\mu_i); \quad \log \mu_i = \beta_0 + \beta_1 x_i; \quad i \in \{1,2\},
\end{equation*}
where $x_i \in \{0,1\}$ is an indicator for your academic inbox. Your goal is to build a level-$\alpha$ confidence interval for $e^{\beta_1}$ (the factor by which the expected number of emails in your academic inbox exceeds that in your personal inbox), and to this end you will invert the Wald, likelihood ratio, and score tests.

\begin{enumerate}

\item[(a)] What is the unrestricted maximum likelihood estimate $(\widehat \beta_0, \widehat \beta_1)$? What are the corresponding fitted means $(\widehat \mu_1, \widehat \mu_2)$? What is the maximum likelihood estimate for $\beta_0$ if $\beta_1$ is fixed at some value $\beta_1^0 \in \mathbb R$? What are the corresponding fitted means? What do the fitted means reduce to when $\beta_1^0 = 0$, and why does this make sense?

\item[(b)] What is the large-sample normal approximation to the sampling distribution of $\bm{\widehat \beta}$? What is the resulting level-$\alpha$ Wald confidence interval for $e^{\beta_1}$ (defined by transforming the endpoints of the Wald confidence interval for $\beta_1$)? Express your answer explicitly.

\item[(c)] Given some $\beta_1^0 \in \mathbb R$, what is the likelihood ratio test statistic for $H_0: \beta_1 = \beta_1^0$? What is the level-$\alpha$ confidence interval for $e^{\beta_1}$ that results from inverting this test? The endpoints of your interval may be specified as solutions to a nonlinear equation.

\item[(d)] Formulate the test $H_0: \beta_1 = \beta_1^0$ as a goodness of fit test. What is the corresponding score test statistic? What is the level-$\alpha$ confidence interval for $e^{\beta_1}$ that results from inverting this test? Express your answer explicitly.


\end{enumerate}

\end{prob}

\begin{sol}

\begin{enumerate}
\item[(a)] We know that the unrestricted MLE $\bbetahatmle = \begin{pmatrix} \betahatmle_0 \\ \betahatmle_1 \end{pmatrix}$ solves $\bX^T[\by - \bmu(\bbetahatmle)] = \bzero$. Here, we have that
\[\bX = \begin{pmatrix} 1 & x_1 \\ 1 & x_2 \end{pmatrix} = \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix}, \ \by = \begin{pmatrix} y_1 \\ y_2 \end{pmatrix}, \ \mu_i = \exp[\bx_{i*}^T \bbetahatmle].\]
So,
\begin{align*}
  \bzero &= \bX^T[\by - \bmu(\bbetahatmle)] \\
  &= \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} \bigg[\begin{pmatrix} y_1 \\ y_2 \end{pmatrix} - \begin{pmatrix} \exp[\betahatmle_0 + \betahatmle_1] \\ \exp[\betahatmle_0] \end{pmatrix}\bigg] \\
  &= \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} y_1 - \exp[\betahatmle_0 + \betahatmle_1] \\ y_2 - \exp[\betahatmle_0] \end{pmatrix} \\
  &= \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} y_1' \\ y_2' \end{pmatrix} \\
  &= \begin{pmatrix} y_1' + y_2' \\ y_1' \end{pmatrix}.
\end{align*}
Then we must have $0 = y_1' = y_2'$. 

As a result, $\log y_i = \betahatmle_0 + \betahatmle_1 x_i$ and $\betahatmle_1 = \log y_1 - \log y_2 = \log(y_1/y_2)$. 

Finally, we obtain that 
\[\betahatmle_0 = \log y_2, \ \betahatmle_1 = \log y_1 - \log y_2 = \log(y_1/y_2).\]

Then
\begin{align*}
  \muhatmle_i &= \exp[\bx_{i*}^T \bbetahatmle] \\
  &= \exp[\betahatmle_0 + \betahatmle_1 x_i] \\
  &= \exp[\log y_2 + x_i(\log y_1 - \log y_2)] \\
  &= \exp[\log y_i] \\
  &= y_i.
\end{align*}

Suppose we now restrict the model space such that $\beta_1$ is fixed at $\beta_1^0 \in \R$ and we wish to find the constrained maximum likelihood estimate $\bbetahatconst = (\betahatconst, \beta_1^0)$. Then our log-likelihood is
\[\log \mathcal{L} (\bbeta) = \sum_{i=1}^2 [\bx_{i*}^T \bbeta y_i - \psi(\bx_{i*}^T \bbeta)] + \log h(y_i)\]
and the partial derivative with respect to $\beta_0$ is 
\begin{align*}
  \frac{\partial}{\partial \beta_0} \log \mathcal{L}(\bbeta) &= \frac{\partial}{\partial \beta_0} \bigg[\sum_{i=1}^2 [\bx_{i*}^T \bbeta y_i - \psi(\bx_{i*}^T \bbeta)] + \log h(y_i)\bigg] \\
  &= \sum_{i=1}^2 \frac{\partial}{\partial \beta_0} [(\beta_0 + \beta_1^0 x_i) y_i - \psi(\beta_0 + \beta_1^0 x_i)] \\
  &= \sum_{i=1}^2 y_i - \dot{\psi}(\bx_{i*}^T \bbeta).
\end{align*}

We know from properties of GLMs that $\dot{\psi}(\bx_{i*}^T \bbeta) = \mu_i$, so
\begin{align*}
  \frac{\partial}{\partial \beta_0} \log \mathcal{L}(\bbeta) &= \sum_{i=1}^2 y_i - \dot{\psi}(\bx_{i*}^T \bbeta) \\
  &= y_1 + y_2 - \sum_{i=1}^2 \exp[\bx_{i*}^T \bbeta] \\
  &= y_1 + y_2 - \exp[\beta_0 + \beta_1^0] - \exp[\beta_0] \\
  &= y_1 + y_2 - \exp[\beta_0][\exp[\beta_1^0] + 1]. 
\end{align*}

Then the constrained MLE $\betahatconst$ satisfies
\[\frac{\partial}{\partial \beta_0} \log \mathcal{L}(\bbeta)|_{\betahatconst} = 0,\]
so 
\[\betahatconst = \log\bigg(\frac{y_1 + y_2}{\exp[\beta_1^0] + 1}\bigg).\]

The corresponding means are
\begin{align*}
  \muhatconst_i &= \exp[\bx_{i*}^T \bbeta] \\
  &= \exp[\betahatconst + \beta_1^0 x_i] \\
  &= \exp[\betahatconst] \exp[\beta_1^0 x_i] \\
  &= (y_1 + y_2) \bigg(\frac{\exp[\beta_1^0 x_i]}{\exp[\beta_1^0] + 1}\bigg)
\end{align*}

When $\beta_1^0 = 0$, we have $\muhatconst_1 = \muhatconst_2 = \frac{y_1 + y_2}{2}$. Intuitively, $\beta_1^0 = 0$ means that $\mu_i$ is independent of $x_i$, so our best guess for the (equal) mean of the two inboxes is their sample mean.

\item[(b)] We know from large-sample asymptotics that $\bbetahatmle \dot{\sim} N(\bbeta, \widehat{\Var}[\bbetahatmle])$. Further, 
\[\widehat{\Var}[\bbetahatmle]) = (\bX^T \bW(\bbetahatmle) \bX)^{-1} = \bigg[\begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} \bW(\bbetahatmle) \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix}\bigg]^{-1}.\]

Recall that since we are dealing with a Poisson model, the mean of $y_i$ is equal to its variance. Thus,
\begin{align*}
  \bW(\bbetahatmle) &= \diag(\Var_{\bbetahatmle}[y_i]) \\
  &= \diag(\muhatmle_i) \\
  &= \begin{pmatrix} \muhatmle_1 & 0 \\ 0 & \muhatmle_2 \end{pmatrix} \\
  &= \begin{pmatrix} y_1 & 0 \\ 0 & y_2 \end{pmatrix}.
\end{align*}

So, 
\begin{align*}
  \bX^T \bW(\bbetahatmle) \bX &= \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} \bW(\bbetahatmle) \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} \\
  &= \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} \begin{pmatrix} y_1 & 0 \\ 0 & y_2 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} \\
  &= \begin{pmatrix} y_1 & y_2 \\ y_1 & 0 \end{pmatrix} \begin{pmatrix} 1 & 1 \\ 1 & 0 \end{pmatrix} \\
  &= \begin{pmatrix} y_1 + y_2 & y_1 \\ y_1 & y_1 \end{pmatrix}.
\end{align*}

This means that
\begin{align*}
  \widehat{\Var}[\bbetahatmle]) &= (\bX^T \bW(\bbetahatmle) \bX)^{-1} \\
  &= \bigg[\begin{pmatrix} y_1 + y_2 & y_1 \\ y_1 & y_1 \end{pmatrix}\bigg]^{-1} \\
  &= \frac{1}{y_1(y_1+y_2)-y_1^2} \begin{pmatrix} y_1 & -y_1 \\ -y_1 & y_1+y_2 \end{pmatrix} \\
  &= \begin{pmatrix} y_2^{-1} & -y_2^{-1} \\ -y_2^{-1} & y_1^{-1} + y_2^{-1} \end{pmatrix}.
\end{align*}

As a result, the Wald CI for $\beta_1$ is  
\begin{align*}
  \CI(\beta_1) &= \betahatmle_1 \pm Q_{1-\alpha/2}[N(0,1)] \SE(\betahatmle_1) \\
  &= \betahatmle_1 \pm Q_{1-\alpha/2}[N(0,1)] \sqrt{\widehat{\Var}[\bbetahatmle]_{22}} \\
  &= \log(y_1/y_2) \pm Q_{1-\alpha/2}[N(0,1)] \sqrt{y_1^{-1} + y_2^{-1}}.
\end{align*}

Transforming the endpoints, we get a Wald CI for $\exp[\beta_1]$:
\[\CI(\exp[\beta_1]) = \bigg(\frac{y_1}{y_2}\bigg) \exp\bigg[\pm Q_{1-\alpha/2}[N(0,1)] \sqrt{y_1^{-1} + y_2^{-1}}\bigg].\]


\item[(c)] We have that the test statistic for the likelihood ratio test is given by a difference in deviances: $T^\LRT_{\beta_1^0} = D(y; \bmuhatconst) - D(y; \bmuhatmle) \dot{\sim} \chi_1^2$.

Note that in a Poisson model with intercept, 
\[D(y; \bmuhatmle) = 2 \bigg[\sum_{i=1}^2 y_i \log\bigg(\frac{y_i}{\muhatmle_i}\bigg) = 2 \bigg[\sum_{i=1}^2 y_i \log(1) \bigg] = 0.\]

For the constrained Poisson, we have
\begin{align*}
  D(y; \bmuhatconst) &= 2\bigg[\sum_{i=1}^2 y_i \log\bigg(\frac{y_i}{\muhatconst_i}\bigg) - (y_i - \muhatconst_i)\bigg] \\
  &= 2 \bigg[\sum_{i=1}^2 y_i \log\bigg(\frac{y_i}{\muhatconst_i}\bigg)\bigg] - 2\bigg[(y_1 + y_2) - (y_1 + y_2)\bigg(\frac{\exp[\beta_1^0] + 1}{\exp[\beta_1^0] + 1}\bigg)\bigg] \\
  &= 2 \bigg[\sum_{i=1}^2 y_i \log\bigg(\frac{y_i}{\muhatconst_i}\bigg)\bigg] \\
  &= 2 \bigg[\sum_{i=1}^2 y_i \log\bigg[\bigg(\frac{y_i}{y_1 + y_2}\bigg) \bigg(\frac{\exp[\beta_1^0] + 1}{\exp[\beta_1^0 x_i]}\bigg) \bigg]\bigg] \\
  &= 2 \bigg[\sum_{i=1}^2 y_i[\log y_i - \log(y_1 + y_2) + \log[\exp\beta_1^0 + 1] - \beta_1^0 x_i] \bigg] \\
  &= 2 [y_1 \log y_1 + y_2 \log y_2 - (y_1 + y_2) \log(y_1 + y_2) + (y_1 + y_2) \log(\exp[\beta_1^0] + 1) - \beta_1^0] \\.
\end{align*}

So, our confidence region for $\exp[\beta_1^0]$ is 
\begin{align*} 
  \CI(\exp[\beta_1^0] = \{\exp[\beta_1^0] : &2 [y_1 \log y_1 + y_2 \log y_2 - (y_1 + y_2) \log(y_1 + y_2) + \\ &(y_1 + y_2) \log(\exp[\beta_1^0] + 1) - \beta_1^0] < Q_{1-\alpha}[\chi_1^2]\}.
\end{align*}


\item[(d)] The null hypothesis is $H_0: \widehat{\theta}_0 = (\betahatconst, \beta_1^0)$ and the alternative hypothesis is $H_1: \widehat{\theta} = (\betahatmle_0, \betahatmle_1)$. Because we are using a Poisson model, we know that test statistic is 
\begin{align*}
  X^2 &= \sum_{i=1}^2 \frac{(y_i - \muhatconst_i)^2}{\muhatconst_i} \\
  &= \sum_{i=1}^2 y_i^2/\muhatconst_i - 2y_i + \muhatconst_i \\
  &= y_1^2/\muhatconst_1 + y_2^2/\muhatconst_2 - 2(y_1 + y_2) + \muhatconst_1 + \muhatconst_2 \\
  &= y_1^2/\muhatconst_1 + y_2^2\muhatconst_2 - (y_1 + y_2) \\
  &= y_1^2 \bigg(\frac{\exp[\beta_1^0] + 1}{\exp[\beta_1^0] (y_1+y_2)}\bigg) + y_2^2 \bigg(\frac{\exp[\beta_1^0] + 1}{(y_1+y_2)}\bigg) - (y_1 + y_2) \\
  &= \frac{y_1^2(1 + \exp[-\beta_1^0]) + y_2^2(1 + \exp[\beta_1^0]) - (y_1 + y_2)^2}{y_1 + y_2} \\
  &= \frac{y_1^2 \exp[-\beta_1^0] - 2 y_1 y_2 + y_2^2 \exp[\beta_1^0]}{y_1 + y_2} \\
  &= \frac{(y_1 \exp[-\frac{1}{2} \beta_1^0] - y_2 \exp[\frac{1}{2} \beta_1^0])^2}{y_1 + y_2}.
\end{align*}

So, our confidence region is defined by
\begin{align*}
  \{\beta_1^0: X^2 < Q_{1-\alpha}[\chi_1^2]\} &= \bigg\{\beta_1^0: \frac{(y_1 \exp[-\frac{1}{2} \beta_1^0] - y_2 \exp[\frac{1}{2} \beta_1^0])^2}{y_1 + y_2} < Q_{1-\alpha}[\chi_1^2]\bigg\} \\
  &= \bigg\{\beta_1^0: y_1 \exp\bigg[-\frac{1}{2} \beta_1^0\bigg] - y_2 \exp\bigg[\frac{1}{2} \beta_1^0\bigg] < \sqrt{(y_1 + y_2) Q_{1-\alpha}[\chi_1^2]}\bigg\}.
\end{align*}
\end{enumerate}

\end{sol}

\begin{prob} \label{prob:ci-simulation}\textbf{Comparing the three confidence interval constructions from Problem~\ref{prob:ci-calculation}.} \\

\noindent Let's use a numerical simulation to compare the three confidence interval constructions from Problem~\ref{prob:ci-calculation} in finite samples.

\begin{enumerate}

\item[(a)] Write functions called \verb|get_ci_wald|, \verb|get_ci_lrt|, and \verb|get_ci_score| that take as arguments (\verb|y_1|, \verb|y_2|, \verb|alpha|) and return the corresponding confidence intervals for $e^{\beta_1}$. If the confidence interval is undefined for a given pair $(y_1, y_2)$, your function should return $(-\infty, \infty)$.

\item[(b)] To get a first sense of how the three intervals compare, compute level $\alpha = 0.05$ intervals for $(y_1, y_2) = (10^1, 10^1), (10^{1.5}, 10^{1.5}), \dots, (10^5, 10^5)$. Plot the lower and upper endpoints of these intervals as functions of $y_1$ (you should arrive at a plot containing six curves, corresponding to the lower and upper endpoints of the three methods). Add a dashed horizontal line at the MLE for $e^{\beta_1}$ (which is the same for each given pair $(y_1, y_2)$). How do the interval widths compare, both across methods and across $(y_1, y_2)$ values?

\item[(c)] Next, calculate the average length and coverage of the three level $\alpha = 0.05$ confidence intervals for $e^{\beta_1}$ in the following simulation setting. Set $(\mu_1, \mu_2) = (10^1, 10^1), (10^{1.5}, 10^{1.5}), \dots, (10^5, 10^5)$. For each pair $(\mu_1, \mu_2)$, generate 5000 realizations of $(y_1, y_2)$ and compute the three confidence intervals for each realization. Plot the average length and coverage for each of the three interval constructions as a function of $\mu_1$ (please omit the undefined/infinite-length intervals from the calculations of length and coverage). Compare and contrast the average lengths and coverages of the three constructions, both across methods and across $(\mu_1, \mu_2)$ values.

\item[(d)] Last month you received 60 emails in your personal inbox and 90 in your academic inbox. Pick one of the three confidence interval constructions above that you feel has good coverage and small width. According to this construction, what is the confidence interval for $e^{\beta_1}$? Can you reject the null hypothesis that the two inboxes receive emails at the same rate?
\end{enumerate}


\end{prob}

\begin{sol}
\begin{enumerate}
  \item[(a)]
<<>>=
# Function to compute Wald CI
get_ci_wald <- function(y_1, y_2, alpha){
  # If undefined, return (-infinty, infinity)
  if(y_1 == 0 || y_2 == 0){
    ci <- c(-Inf, Inf)
  }
  # Else, return the defined Wald CI
  else{
    bnd1 <- (y_1/y_2) * exp(qnorm(1-alpha/2)*sqrt(1/y_1 + 1/y_2))
    bnd2 <- (y_1/y_2) * exp(-qnorm(1-alpha/2)*sqrt(1/y_1 + 1/y_2))
  
    ci <- c(min(bnd1, bnd2), max(bnd1, bnd2))
  }
  
  return(ci)
}

# Function to approximate LRT CI
get_ci_lrt <- function(y_1, y_2, alpha){
  # Sequence of betas to test 
  test_beta <- seq(from=-5, to=5, by=0.01)
  
  # Calculate test statistic for each test beta
  t_lrt <- 
    2*(y_1 * (log(y_1) - log(y_1+y_2) + log(1 + exp(-test_beta))) + 
       y_2 * (log(y_2) - log(y_1+y_2) + log(exp(test_beta) + 1)))
  
  # Get subset of betas that exceed quantile of null distribution
  con_reg <- test_beta[t_lrt > qchisq(1-alpha, df=1)]
  
  # The MLE for beta
  mle <- log(y_1/y_2)
  
  # Get min and max betas from confidence region as approximate CI bounds
  ci <- 
    c(max(con_reg[con_reg <= mle]), 
      min(con_reg[con_reg >= mle]))
  
  # Return exponentiated CI
  return(exp(ci))
}

# Function to approximate score test CI
get_ci_score <- function(y_1, y_2, alpha){
  # Sequence of betas to test 
  test_beta <- seq(from=-5, to=5, by=0.01)
  
  # Calculate test statistic for each beta
  X_2 <- (y_1*exp(-test_beta/2) - y_2*exp(test_beta/2))^2 / (y_1+y_2)
  
  # Get subset of betas that exceed quantile of null distribution
  con_reg <- test_beta[X_2 > qchisq(1-alpha, df=1)]
  
  # The MLE for beta
  mle <- log(y_1/y_2)
  
  # Get min and max betas from confidence region as approximate CI bounds
  ci <- 
    c(max(con_reg[con_reg <= mle]), 
      min(con_reg[con_reg >= mle]))
  
  # Return exponentiated CI
  return(exp(ci))
}
@
  
  % TODO
  \item[(b)]
<<>>=
# Significance level
alpha <- 0.05

# Dataframe to hold all info
ci_comparison <-
  data.frame(y_1 = round(10^seq(from=1, to=5, by=0.5))) %>%
  mutate(y_2 = y_1) %>%
  mutate(
    wald_lower = 0,
    wald_upper = 0,
    lrt_lower = 0,
    lrt_upper = 0,
    score_lower = 0,
    score_upper = 0)

# For each row, get and store CIs
for(i in 1:nrow(ci_comparison)){
#for(i in 1:1){
  # Get CIs
  wald_ci <- 
    get_ci_wald(
      ci_comparison$y_1[i], 
      ci_comparison$y_2[i], 
      alpha)
  lrt_ci <- 
    get_ci_lrt(
      ci_comparison$y_1[i], 
      ci_comparison$y_2[i], 
      alpha)
  score_ci <- 
    get_ci_score(
      ci_comparison$y_1[i], 
      ci_comparison$y_2[i], 
      alpha)
  
  
  # Store results
  ci_comparison[i, c("wald_lower", "wald_upper",
                    "lrt_lower", "lrt_upper",
                    "score_lower", "score_upper")] <-
    c(wald_ci[1], wald_ci[2], 
      lrt_ci[1], lrt_ci[2],
      score_ci[1], score_ci[2])
}

ci_plt <-
  ci_comparison %>%
    ggplot() +
      geom_point(aes(x=y_1, y=wald_lower, color="Wald CI")) +
      geom_point(aes(x=y_1, y=wald_upper, color="Wald CI")) +
      geom_point(aes(x=y_1, y=lrt_lower, color="LRT CI")) +
      geom_point(aes(x=y_1, y=lrt_upper, color="LRT CI")) +
      geom_point(aes(x=y_1, y=score_lower, color="Score CI")) +
      geom_point(aes(x=y_1, y=score_upper, color="Score CI")) +
      # MLE for e^(beta_1) is 1 for every pair since y_1 = y_2
      geom_hline(aes(yintercept=1, color="MLE"), linetype="dashed") +
      ylab("CI Bounds") +
      xlab("$y_1$") 
      
@
  
In general, the widths get narrower as $y_1$ increases, converging around the MLE. We have that the Wald CI is generally narrower than the other two methods which have comparable widths.

  % TODO
  \item[(c)]
<<>>=
# Values of mu for simulation
# Only using up to mu = 2.5 because exp(10^mu) "=" Inf for mu > 2.5
mu <- 10^seq(from=1, to=2.5, by=0.5)
  
# Function to run simulation
do_sim <- function(mu, n=5000, alpha=0.05){
  # Vectors to store widths and coverage
  wald_widths <- c()
  lrt_widths <- c()
  score_widths <- c()
  
  wald_coverages <- c()
  lrt_coverages <- c()
  score_coverages <- c()
  
  # Runs simulation n times
  for(i in 1:n){
    # Sample y_1, y_2
    y_1 <- rpois(1, lambda=exp(mu))
    y_2 <- rpois(1, lambda=exp(mu))
    
    # Get CIs
    wald_ci <- 
      get_ci_wald(y_1, y_2, alpha)
    lrt_ci <- 
      get_ci_lrt(y_1, y_2, alpha)
    score_ci <- 
      get_ci_score(y_1, y_2, alpha)
  
    # Get CI widths
    wald_width <- wald_ci[2]-wald_ci[1]
    lrt_width <- lrt_ci[2]-lrt_ci[1]
    score_width <- score_ci[2]-score_ci[1]
  
    # True beta_1 = 0 since mu_1 = mu_2 and log is injective
    beta_1 <- 0
  
    # Get CI coverage
    wald_coverage <- (wald_ci[1] <= 0 & wald_ci[2] >= 0)
    lrt_coverage <- (lrt_ci[1] <= 0 & wald_ci[2] >= 0)
    score_coverage <- (score_ci[1] <= 0 & score_ci[2] >= 0) 
    
    # Only store results if CI finite
    if(!any(is.infinite(wald_ci))){
      wald_widths <- c(wald_widths, wald_width)
      wald_coverages <- c(wald_coverages, wald_coverage)
    }
    if(!any(is.infinite(lrt_ci))){
      lrt_widths <- c(lrt_widths, lrt_width)
      lrt_coverages <- c(lrt_coverages, lrt_coverage)
    }
    if(!any(is.infinite(score_ci))){
      score_widths <- c(score_widths, score_width)
      score_coverages <- c(score_coverages, score_coverage)
    }
  }
  
  # Compute average widths
  wald_avg_width <- mean(wald_widths)
  lrt_avg_width <- mean(lrt_widths)
  score_avg_width <- mean(score_widths)
  
  # Compute coverages
  wald_coverage <- mean(wald_coverages)
  lrt_coverage <- mean(lrt_coverages)
  score_coverage <- mean(score_coverages)
  
  # Combine results in dataframe
  results <- 
    data.frame(
      mu_1 = rep(mu, times=3),
      mu_2 = rep(mu, times=3),
      avg_width = c(wald_avg_width, lrt_avg_width, score_avg_width),
      coverage = c(wald_coverage, lrt_coverage, score_coverage),
      Method=c("Wald", "LRT", "Score"))
}

# Run simulation
sim_results <- lapply(X=mu, FUN=do_sim)

# Convert results to dataframe
sim_df <- do.call(rbind, sim_results)

# Coverage plot
coverage_plt <-
  sim_df %>%
    ggplot(aes(x=mu_1, y=coverage, color=Method)) +
    geom_point() +
    ylab("CI coverage")

# Width plot
width_plt <-
  sim_df %>%
    ggplot(aes(x=mu_1, y=avg_width, color=Method)) +
    geom_point() +
    ylab("Average CI width")
@
  
  
  % TODO
  \item[(d)]
\end{enumerate}
\end{sol}


\begin{prob} \label{prob:blocks-data}\textbf{Case study: Child development.} \\

\noindent Children were asked to build towers as high as they could out of cubical and cylindrical blocks.\footnote{Johnson, B., Courtney, D.M.: Tower building. Child Development 2(2), 161â€“162 (1931).} The number of blocks used and the time taken were recorded (see \verb|blocks_data| below). In this problem, only consider the number of blocks used and the age of the child.
<<message = FALSE>>=
blocks_data = read_tsv("../../data/blocks.tsv")
print(blocks_data, n = 5)
@

\begin{enumerate}

\item[(a)] Create a scatter plot of blocks used versus age; since there are exact duplicates of \verb|(Number, Age)| in the data, use \verb|geom_count()| instead of \verb|geom_point()|. Propose a GLM to model the number of blocks used as a function of age.

\item[(b)] Fit this GLM using R, and write down the fitted model. Determine the standard error for each regression parameter, and find the 95\% Wald confidence intervals for the regression coefficients.

\item[(c)] Use Wald, score, and likelihood ratio tests to determine if age seems necessary in the model. Compare the results and comment.

\item[(d)] Plot the number of blocks used against age as in part (a), adding the relationship described by the fitted model as well as lines indicating the lower and upper 95\% confidence intervals for these fitted values.

\end{enumerate}

\noindent \textit{Acknowledgment: This problem was drawn from ``Generalized Linear Models With Examples in R'' (Dunn and Smyth, 2018).}


\end{prob}

\begin{sol}
\begin{enumerate}
% TODO
\item[(a)]
% TODO: Add plot
<<scatter plot>>=
scatter_plt <-
  blocks_data %>%
    ggplot(aes(y=Number, x=Age)) +
    geom_count() + 
    ylim(0, ceiling(max(blocks_data$Number)/5)*5) +
    ylab("Number of blocks") +
    xlab("Child's age")
@

Note that the dependent variable of interest, the number of blocks, is a nonnegative integer. As a result, it makes sense to model the number of blocks using either a Poisson or NBD distribution , as these have support on the nonnegative integers. For model parsimony, we propose using a Poisson distribution; i.e. if $y_i$ is the number of blocks used by the $i$-th child and $x_i$ is the corresponding child's age then
\[y_i \stackrel{\text{ind}}{\sim} \text{Poi}(\mu_i); \ \log \mu_i = \beta_0 + \beta_1 x_i.\]

% TODO
\item[(b)]
<<glm>>=
# Fit Poisson regression
glm_fit <- 
  glm(Number ~ Age, family=poisson, data=blocks_data)
@

The fitted model is 
\[y_i \stackrel{\text{ind}}{\sim} \text{Poi}(\mu_i); \ \log \mu_i = 1.3447 + 0.1415 x_i.\]

% TODO: Add table
<<>>=
# Extract coefficient standard errors
coeff_summary <- 
  summary(glm_fit)$coefficients[, c("Estimate", "Std. Error")]

wald_ci <-
  confint.default(glm_fit)

coeff_tbl <-
  cbind(coeff_summary, wald_ci)
@

% TODO
\item[(c)]
<<wald test>>=
wald_result <-
  summary(glm_fit)$coefficients
@

% TODO: Add table for LRT
<<lrt>>=
# Get fit for model without age
glm_fit_partial <-
  glm(Number ~ 1, family=poisson, data=blocks_data)

# Do LRT
anova_fit <-
  anova(glm_fit_partial, glm_fit, test="LRT")
@

Based on both the LRT and Wald test, we have that the age variable seems to be significant at the $\alpha = 0.01$ level, indicating that age does seem necessary to the model.

% TODO
\item[(d)]
% TODO: Fix coloring
<<fitted plot>>=
blocks_fitted <-
  predict(
    glm_fit,
    newdata = blocks_data %>% select(Number, Age),
    se.fit=TRUE,
    type="response") %>%
  as.data.frame() %>%
  as_tibble() %>%
  select(-residual.scale) %>%
  rename(Fitted = fit) %>%
  mutate(
    Number = blocks_data$Number,
    Age= blocks_data$Age,
    `CI lower` = Fitted-qnorm(1-0.05/2) * se.fit,
    `CI upper` = Fitted+qnorm(1-0.05/2) * se.fit)

scatter_plt_ext <-
  blocks_fitted %>%
    ggplot() +
    geom_count(aes(x=Age, y=Number)) + 
    geom_smooth(method="lm", aes(x=Age, y=Number, color="$E[x_{i*}^T \\beta]$"), formula=(y ~ exp(coeff_tbl[1,1] + coeff_tbl[1,2] * x)), se=FALSE) +
    geom_ribbon(aes(x=Age, y=Fitted, ymin=`CI lower`, ymax=`CI upper`, color="95% CB"), alpha = 0.2) +
    ylim(min(blocks_fitted$Number)-1, max(blocks_fitted$Number)+1) +
    ylab("Number of blocks") +
    xlab("Child's age")

@

\end{enumerate}
\end{sol}


\end{document}