% (C) Brett Klamer - MIT - http://opensource.org/licenses/MIT
% Please contact me if you find any errors or make improvements
% Contact details at brettklamer.com

\documentclass[11pt,letterpaper,english,oneside]{article} % article class is a standard class
%==============================================================================
%Load Packages
%==============================================================================
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % easy page margins
\usepackage[utf8]{inputenc} % editor uses utf-8 encoding
\usepackage[T1]{fontenc} % T1 font pdf output
\usepackage{lmodern} % Latin modern roman font
\usepackage{bm, bbm} % bold and blackboard bold math symbols
\usepackage{amsmath, amsfonts, amssymb, amsthm} % math packages
\usepackage[final]{microtype} % better microtypography
\usepackage{graphicx} % for easier grahics handling
\usepackage[hidelinks, colorlinks=true, linkcolor = blue, urlcolor = blue]{hyperref} % to create hyperlinks
\usepackage{float} % tells floats to stay [H]ere!
\usepackage{mdframed} % it's better than framed. knitr uses framed so settings won't conflict
\usepackage{enumitem} % nice lists
\usepackage{fancyhdr} % nice headers
\usepackage{caption}  % to control figure and table captions

\captionsetup{width=0.9\textwidth, justification = raggedright}

%==============================================================================
% Enter name and homework title here
%==============================================================================
\author{Sam Rosenberg}
\title{Homework 2}
\date{Due October 4 at 11:59pm}

%==============================================================================
% Put title and author in PDF properties
%==============================================================================
\makeatletter % change interpretation of @
\hypersetup{pdftitle={\@title},pdfauthor={\@author}}


%==============================================================================
% Header settings
%==============================================================================
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @

%==============================================================================
% List spacing
%==============================================================================
\setlist[itemize]{parsep=0em} % fix itemize spacing
\setlist[enumerate]{parsep=0em} % fix enumerate spacing

%==============================================================================
% Float spacing (changes spacing of tables, graphs, etc)
%==============================================================================
%\setlength{\textfloatsep}{3pt}
%\setlength{\intextsep}{3pt}

%==============================================================================
% Define Problem and Solution Environments
%==============================================================================
\theoremstyle{definition} % use definition's look
\newtheorem{problem}{Problem}
\newtheorem{solution}{Solution}
\mdfsetup{ % box margin fix for mdframe and how it plays with parskip and others.
innerleftmargin=4pt,
innerrightmargin=4pt,
innertopmargin=-1pt,
innerbottommargin=4pt}
% \newenvironment{prob}{\begin{mdframed}\begin{problem}\hspace{0pt}}{\end{problem}\end{mdframed}}
\newenvironment{prob}{\clearpage \begin{problem}\hspace{0pt}}{\end{problem}}
\newenvironment{sol}{\begin{solution}\hspace{0pt}}{\end{solution}}

%==============================================================================
% Custom commands
%==============================================================================
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

%==============================================================================
% set knitr options
%==============================================================================
% latex (change space before and after knitr kframe; based on framed package)
\setlength{\OuterFrameSep}{0.3em}
% R
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)
# inline hook to process the output of \Sexpr{} statements to just 2 digits
inline_hook <- function(x) {
  if(is.numeric(x)) x <- round(x, 2)
  paste(as.character(x), collapse=", ")
}
knit_hooks$set(inline = inline_hook)

# cache all chunks
opts_chunk$set(cache = TRUE)

# create directory for figures
if(!dir.exists("figures")){
  dir.create("figures")
}
@

\begin{document}


\maketitle

\section{Instructions}

\paragraph{Setup.} Pull the latest version of this assignment from Github and set your working directory to \texttt{stat-961-fall-2021/homework/homework-2}. Consult the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/getting-started.pdf}{getting started guide} if you need to brush up on \texttt{R}, \texttt{LaTeX}, or \texttt{Git}.

\paragraph{Collaboration.} The collaboration policy is as stated on the Syllabus:

\begin{quote}
``Students are permitted to work together on homework assignments, but solutions must be written up and submitted individually. Students must disclose any sources of assistance they received; furthermore, they are prohibited from verbatim copying from any source and from consulting solutions to problems that may be available online and/or from past iterations of the course.''
\end{quote}

\noindent In accordance with this policy, \\

\noindent \textit{Please list anyone you discussed this homework with:} \\

\noindent \textit{Please list what external references you consulted (e.g. articles, books, or websites):} 

\paragraph{Writeup.} Use this document as a starting point for your writeup, adding your solutions between \verb|\begin{sol}| and \verb|\end{sol}|. See the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/preparing-reports.pdf}{preparing reports guide} for guidance on compilation, creation of figures and tables, and presentation quality. Show all the code you wrote to produce your numerical results, and include complete derivations typeset in LaTeX for the mathematical questions. 

\paragraph{Programming.}

The \texttt{tidyverse} paradigm for data manipulation (\texttt{dplyr}) and plotting (\texttt{ggplot2}) are strongly encouraged, but points will not be deducted for using base \texttt{R}. 
<<message=FALSE, cache = FALSE>>=
library(tidyverse)
@

\paragraph{Grading.} Each sub-part of each problem will be worth 3 points: 0 points for no solution or completely wrong solution; 1 point for some progress; 2 points for a mostly correct solution; 3 points for a complete and correct solution modulo small flaws. The presentation quality of the solution for each problem (as exemplified by the guidelines in Section 3 of the \href{https://github.com/Katsevich-Teaching/stat-961-fall-2021/blob/main/getting-started/preparing-reports.pdf}{preparing reports guide}) will be evaluated out of an additional 3 points.

\paragraph{Submission.} Compile your writeup to PDF and submit to \href{https://www.gradescope.com/courses/284562}{Gradescope}.

\clearpage

\begin{prob} \label{prob:likelihood}\textbf{Likelihood inference in linear regression.} \\

\noindent Let's consider the usual linear regression setup. Given a full-rank $n \times p$ model matrix $\bm X$, a coefficient vector $\bm \beta \in \mathbb R^p$, and a noise variance $\sigma^2 > 0$, suppose
\begin{equation}
\bm y = \bm X \bm \beta + \bm \epsilon, \quad \bm \epsilon \sim N(0, \sigma^2 \bm I_n).
\label{eq:linear-model}
\end{equation}
The goal of this problem is to connect linear regression inference with classical likelihood-based inference (below is a quick refresher).

\begin{enumerate}
\item[(a)] For the sake of simplicity, let's start by assuming $\sigma^2$ is known. Under the fixed-design model, why does the linear regression model~\eqref{eq:linear-model} not fit into the classical inferential setup~\eqref{eq:iid-sampling}? Write the linear model in as close a form as possible to~\eqref{eq:iid-sampling}.

\item[(b)] Continue assuming that $\sigma^2$ is known. Why does the Fisher information~\eqref{eq:fisher-info} not immediately make sense for the linear regression model? Propose and compute an analog to this quantity, and using this quantity exhibit a result analogous to the asymptotic normality~\eqref{eq:asymptotic-normality}.

\item[(c)] Now assume that neither $\bm{\beta}$ nor $\sigma^2$ is known. Derive the maximum likelihood estimates for $(\bm \beta, \sigma^2)$. How do these compare to the estimates $(\bm{\widehat \beta}, \widehat \sigma^2)$ discussed in class?

\item[(d)] Continuing to assume that neither $\bm{\beta}$ nor $\sigma^2$ is known, consider the null hypothesis $H_0: \bm{\beta}_S = \bm 0$ for some $S \subseteq \{1, \dots, p\}$. Write this hypothesis in the form~\eqref{eq:general-null-hypothesis}, and derive the likelihood ratio test for this hypothesis. Discuss the connection of this test with the $F$-test.

\end{enumerate}

\noindent\fbox{\begin{minipage}{\textwidth}
\paragraph{Refresher on likelihood inference.} In classical likelihood inference, we have observations 
\begin{equation}
y_i \overset{\text{i.i.d.}}\sim p_{\bm \theta}, \quad i = 1, \dots, n
\label{eq:iid-sampling}
\end{equation}
from some model parameterized by a vector $\bm \theta \in \Theta \subseteq \mathbb R^d$. Under regularity conditions, the maximum likelihood estimate $\bm{\widehat \theta}_n$ is known to converge to a normal distribution centered at its true value:
\begin{equation}
\sqrt n(\bm{\widehat \theta}_n - \bm \theta) \overset d \rightarrow N(0, \bm I(\bm \theta)^{-1}),
\label{eq:asymptotic-normality}
\end{equation}
where 
\begin{equation}
\bm I(\bm \theta) \equiv -\mathbb E_{\bm \theta}\left[\frac{\partial^2}{\partial \bm \theta^2} \log p_{\bm \theta}(y) \right]
\label{eq:fisher-info}
\end{equation}
is the Fisher information matrix.
Furthermore, an optimal test of the null hypothesis 
\begin{equation}
H_0: \bm \theta \in \Theta_0 \quad \text{versus} \quad H_1: \bm \theta \in \Theta_1 
\label{eq:general-null-hypothesis}
\end{equation}
for some $\Theta_0 \subseteq \Theta_1 \subseteq \Theta$ is the likelihood ratio test based on the test statistic
\begin{equation}
\Lambda = \frac{\max_{\bm \theta \in \Theta_1}\prod_{i = 1}^n p_{\bm \theta}(y_i)}{\max_{\bm \theta \in \Theta_0}\prod_{i = 1}^n p_{\bm \theta}(y_i)}.
\end{equation}
Under $H_0$, we have the convergence 
\begin{equation}
2 \log \Lambda \overset d \rightarrow \chi^2_k, \quad \text{where} \quad k \equiv \text{dim}(\Theta_1) - \text{dim}(\Theta_0).
\end{equation}
\end{minipage}
}

\end{prob}
  
\begin{sol}
\begin{enumerate}
  \item[(a)] We can rewrite~\eqref{eq:linear-model} as follows: 
    \begin{align*}
      \begin{pmatrix} y_1 \\ \vdots \\ y_n \end{pmatrix} &= \begin{pmatrix} x_{11} & \dots & x_{1p} \\ \vdots & \ddots & \vdots \\ x_{n1} & \hdots & x_{np} \end{pmatrix} \begin{pmatrix} \beta_1 \\ \vdots \\ \beta_p \end{pmatrix} + \begin{pmatrix} \epsilon_1 \\ \vdots \\ \epsilon_n \end{pmatrix} \\
      &= \begin{pmatrix} \bm{x_1} \cdot \bm{\beta} + \epsilon_1 \\ \vdots \\ \bm{x_n} \cdot \bm{\beta} + \epsilon_n \end{pmatrix},
    \end{align*}
    where the covariance matrix of $\bm{\epsilon}$ is $\sigma^2 \bm{I}_n$. Note that this means the $\epsilon_i$ are all uncorrelated, but the fact that they are multivariate normal and uncorrelated implies that the $\epsilon_i$ are independent standard normal random variables. So, $y_i \sim \bm{x}_i \cdot \bm{\beta} + \epsilon_i$, where $\epsilon_i \stackrel{\text{i.i.d.}}{\sim} N(0, \sigma^2)$.
    
    Though each $y_i$ is parametrized by $\bm{\beta} \in \mathbb{R}^p$, it is also a function of $\bm{x}_i$, which is not necessarily the same for each $i$ since the $\bm{x}_i$ are regarded as fixed. As a result, the fixed-design model does not fit into the classical inferential setup, since the $y_i$ need not be identically distributed. 
  
  \item[(b)] The Fisher information doe not make sense in this context because the $y_i$ each have different PDFs, since they are not i.i.d. as noted in (a). We have previously seen that $\widehat{\beta}_j \sim N(\beta_j, \sigma^2/s_j^2)$, so $\sqrt{n}(\widehat{\beta}_j - \beta_j) \sim N(0, \sigma^2/(s_j/n)^2$. So, an analog for the Fisher information would be $(s_j/n)^2/\sigma^2$.
  
  %We know from Unit 1 that when $\sigma^2$ is assumed to be known, the maximum likelihood estimator for $\bm{\beta}$ is the same as the least squares estimate $\widehat{\bm{\beta}} = (\bm{X}^T \bm{X})^{-1} \bm{X}^T \bm{y} = (\bm{X}^T \bm{X})^{-1} \bm{X}^T (\bm{X} \bm{\beta} + \bm{\epsilon}) = \bm{\beta} + (\bm{X}^T \bm{X})^{-1} \bm{X}^T \bm{\epsilon}$. So, $\widehat{\bm{\beta}} - \bm{\beta} = (\bm{X}^T \bm{X})^{-1} \bm{X}^T \bm{\epsilon}$ and $\sqrt{n}(\widehat{\bm{\beta}} - \bm{\beta}) = (\frac{\bm{X}^T \bm{X}}{n})^{-1} \frac{1}{\sqrt{n}} \bm{X}^T \bm{\epsilon}$. Note that since $\bm{\epsilon} \sim N(0, \sigma^2 \bm{I}_n), \frac{1}{\sqrt{n}} \bm{X}^T \epsilon \sim N(0, \sigma^2 \frac{\bm{X}^T \bm{X}}{n})$.
  % Q := (X^T X / n)^{-1} exist?
  
  %We have also seen in this unit that $\widehat{\bm{\beta}} \sim N(\bm{\beta}, \sigma^2(\bm{X}^T \bm{X})^{-1})$. So, $\widehat{\bm{\beta}} - \bm{\beta} \sim N(0, \sigma^2(\bm{X}^T \bm{X})^{-1})$.
  
  \item[(c)] % NOTE: FINISH
  Define our parameter vector of interest to be $\bm{\theta} := (\bm{\beta}, \sigma^2)$. We saw in Unit 2 that $\bm{y} \sim N(\bm{X \beta}, \sigma^2 \bm{I}_n)$. So, we know that the joint density (equivalent to the likelihood function) for the fixed-design model is 
  \begin{align*}
    p_{\bm{\theta}}(\bm{y}) &= \frac{1}{\sqrt{(2 \pi)^n |\sigma^2 \bm{I}_n|}} \exp[-\frac{1}{2} (\bm{y} - \bm{X \beta})^T (\sigma^2 \bm{I}_n)^{-1} (\bm{y} - \bm{X \beta})] \\
    &= \frac{1}{(2 \pi \sigma^2)^{n/2}} \exp[-\frac{1}{2 \sigma^2} ||y - \bm{X \beta}||^2].
  \end{align*}
    Because $\log(x)$ is a monotone increasing function, any maximum of $p_{\bm{\theta}}(\bm{y})$ is also a maximum of $\ell_{\bm{\theta}}(\bm{y}) := \log[p_{\bm{\theta}}(\bm{y})]$ and vice versa.
   
    We can simplify as follows: 
    \begin{align*}
      \ell_{\bm{\theta}}(\bm(y)) &= \log[p_{\bm{\theta}}(\bm{y})] \\
      &= -\frac{1}{2}[n \log(2 \pi \sigma^2) + \frac{1}{\sigma^2} ||\bm{y} - \bm{X \beta}||^2].
    \end{align*}
    
    So, 
      \[\frac{\partial}{\partial \bm{\beta}} \ell_{\bm{\theta}}(\bm{y}) = \frac{1}{\sigma^2} \bm{X}^T(\bm{y} - \bm{X \beta}).\]
    Under the full rank assumption we receive the same maximum likelihood estimator for $\bm{\beta}$ by setting the partials equal to 0 and solving for $\bm{\beta}$; that is, $\widehat{\bm{\beta}} = (\bm{X}^T \bm{X})^{-1} \bm{X}^T \bm{y}$. 
    
    We also have that \[\frac{\partial}{\partial \sigma} \ell_{\bm{\theta}}(\bm{y}) = \frac{1}{\sigma^3} ||\bm{y} - \bm{X \beta}||^2 - \frac{n}{\sigma}.\]
    Again setting the partial equal to 0 and solving for $\sigma^2$ while substituting our estimator $\widehat{\bm{\beta}}$, we see that the maximum likelihood estimator is $\widehat{\sigma}^2 = \frac{1}{n} ||\bm{y} - \bm{X \widehat{\beta}}||^2 = \frac{1}{n} ||(\bm{I}- \bm{H})\bm{y}||^2  = \frac{1}{n} ||\bm{\widehat{\epsilon}}||^2$.
  
    Thus, our estimator for $\beta$ remains the same as the least squares estimator, while the estimator for $\epsilon$ differs from our unbiased estimator by a factor of $\frac{n}{n-p}$.
    
  \item[(d)] Define $\Theta := (\sigma^2, \beta_1, \dots, \beta_p) \in \mathbb{R}^{p+1}$. We also take $\Theta_1 := \Theta$ and $\Theta_0 := \{\theta = (\sigma^2, \beta_1, \dots, \beta_p) \in \mathbb{R}^{p+1} | \bm{\beta}_S = 0\}$. Then we have that we are testing
  \[H_0: \bm{\theta} \in \Theta_0 \text{ versus } H_1: \bm{\theta} \in \Theta_1.\]
  
  Define $\ell_i(\bm{y}) := \max_{\bm{\theta} \in \Theta_i} p_{\bm{\theta}}(\bm{y})$, where $p_{\bm{\theta}}(\bm{y})$ is the density for the linear model; i.e. 
  \[p_{\bm{\theta}}(\bm{y}) := \frac{1}{(2 \pi \sigma^2)^{n/2}} \exp[-\frac{1}{2 \sigma^2} ||\bm{y} - \bm{X \beta}||^2].\]
  We know from (c) that $(\widehat{\sigma}^2_1, \widehat{\beta}^1) := \argmax_{\bm{\theta} \in \Theta} p_{\bm{\theta}}(\bm{y}) = (\frac{1}{n}||\bm{y} - \bm{X} \widehat{\bm{\beta}}^1||^2, (\bm{X}^T \bm{X})^{-1} \bm{X} \bm{y})$. 
  Also, 
  \begin{align*}
    \argmax_{\bm{\theta} \in \Theta_0} p_{\bm{\theta}}(\bm{y}) &= \argmax_{\bm{\theta} \in \Theta : \bm{\beta}_S = 0} \frac{1}{(2 \pi \sigma^2)^{n/2}} \exp[-\frac{1}{2 \sigma^2} ||\bm{y} - \bm{X \beta}||^2] \\
    &= \argmax_{\bm{\theta} \in \Theta : \bm{\beta}_S = 0} \frac{1}{(2 \pi \sigma^2)^{n/2}} \exp[-\frac{1}{2 \sigma^2} ||\bm{y} - \bm{X}_{-S} \bm{\beta}_{-S}||^2].
  \end{align*}
  But this is exactly the MLE for the model given by the partial model matrix $\bm{X}_{-S}$. So, $(\widehat{\sigma}^2_0, \widehat{\bm{\beta}}^0) := \argmax_{\bm{\theta} \in \Theta_0} p_{\bm{\theta}}(\bm{y}) = (\frac{1}{n}||\bm{y} - \bm{X}_{-S} \widehat{\bm{\beta}}_{-S}||^2, (\bm{X}_{-S}^T \bm{X}_{-S})^{-1} \bm{X}_{-S} \bm{y})$.
  
  We then have
  \begin{align*}
    \Lambda :=& \frac{\ell_1(\bm{y})}{\ell_0(\bm{y})} \\
    =& \frac{(2 \pi \widehat{\sigma}_1^2)^{-n/2} \exp[-\frac{1}{2 \widehat{\sigma}_1^2} ||\bm{y} - \bm{X}\widehat{\bm{\beta}}^1||^2]}{(2 \pi \widehat{\sigma}_0^2)^{-n/2} \exp[-\frac{1}{2 \widehat{\sigma}_0^2} ||\bm{y} - \bm{X}\widehat{\bm{\beta}}^0||^2]} \\
    =& (\frac{\widehat{\sigma}_0^2}{\widehat{\sigma}_1^2})^{n/2} \exp[\frac{1}{2}(\frac{1}{\widehat{\sigma}_0^2} ||\bm{y} - \bm{X}\widehat{\bm{\beta}}^0||^2 - \frac{1}{\widehat{\sigma}_1^2} ||\bm{y} - \bm{X}\widehat{\bm{\beta}}^1||^2)].
  \end{align*}
  So,
  \begin{align*}
    2 \log\Lambda &= n \log(\frac{\widehat{\sigma}_0^2}{\widehat{\sigma}_1^2}) + (\frac{1}{\widehat{\sigma}_0^2} ||\bm{y} - \bm{X}\widehat{\bm{\beta}}^0||^2 - \frac{1}{\widehat{\sigma}_1^2} ||\bm{y} - \bm{X}\widehat{\bm{\beta}}^1||^2) \\
    &= n \log(\frac{||\bm{y} - \bm{X}_{-S} \widehat{\bm{\beta}}_{-S}||^2}{||\bm{y} - \bm{X} \widehat{\bm{\beta}}||^2}) + n (\frac{||\bm{y} - \bm{X}\widehat{\bm{\beta}}^0||^2}{||\bm{y} - \bm{X}_{-S} \widehat{\bm{\beta}}_{-S}||^2}  - \frac{||\bm{y} - \bm{X}\widehat{\bm{\beta}}^1||^2}{||\bm{y} - \bm{X} \widehat{\bm{\beta}}||^2} ) \\
    &= n \log(\frac{||\bm{y} - \bm{X}_{-S} \widehat{\bm{\beta}}_{-S}||^2}{||\bm{y} - \bm{X} \widehat{\bm{\beta}}||^2}) \\
  &= n \log(\frac{||(\bm{I} - \bm{H}_{-S}) \bm{y}||^2}{||(\bm{I} - \bm{H}) \bm{y}||^2}),
  \end{align*}
  since $||\bm{y} - \bm{X}\widehat{\bm{\beta}}^0||^2 = ||\bm{y} - \bm{X}_{-S} \widehat{\bm{\beta}}_{-S}||^2$ and $||\bm{y} - \bm{X}\widehat{\bm{\beta}}^1||^2 = ||\bm{y} - \bm{X} \widehat{\bm{\beta}}||^2$.
  Then \[\Lambda^{2/n} = \frac{||(\bm{I} - \bm{H}_{-S}) \bm{y}||^2}{||(\bm{I} - \bm{H}) \bm{y}||^2},\]
  so \[\frac{n-p}{|S|}(\Lambda^{2/n} - 1) = \frac{||(\bm{I} - \bm{H}_{-S}) \bm{y}||^2 - ||(\bm{I} - \bm{H}) \bm{y}||^2}{||(\bm{I} - \bm{H}) \bm{y}||^2} = F.\]
  Thus, we have that
  
\end{enumerate}  
\end{sol}

\begin{prob} \textbf{Relationships among $t$-tests, $F$-tests, and $R^2$.} \\

\noindent Consider the linear regression model~\eqref{eq:linear-model}, such that $\bm x_{*,0} = \bm 1_n$ is an intercept term (note that there are only $p-1$ other predictors, for a total of $p$).

\begin{itemize}
\item[(a)] Relate the $R^2$ of the linear regression to the $F$-statistic for a certain hypothesis test. What is the corresponding null hypothesis? What is the null distribution of the $F$-statistic? Are $R^2$ and $F$ positively or negative related, and why does this make sense?

\item[(b)] Use the relationship found in part (a) to simulate the null distribution of the $R^2$ by repeatedly sampling from an $F$ distribution (via \verb|rf|). Fix $n = 100$ and try $p \in \{2, 25, 50, 75, 99\}$. Comment on these null distributions, how they change as a function of $p$, and why. 

\item[(c)] Consider the null hypothesis $H_0: \beta_j = 0$, which can be tested using either a $t$-test or an $F$-test. Write down the corresponding $t$ and $F$ statistics, and prove that the latter is the square of the former. 

\item[(d)] Now suppose we are interested in testing the null hypothesis $H_0: \bm \beta_{-0} = \bm 0$. One way of going about this is to start with the usual test statistic $t(\bm c)$ for the null hypothesis $H_0: \bm c^T \bm \beta_{-0} = 0$, and then maximize over all $\bm c \in \mathbb R^{p-1}$:
\begin{equation}
t_{\max} \equiv \max_{\bm c \in \mathbb R^{p-1}} t(\bm c).
\end{equation}
What is the null distribution of $t_{\max}^2$? What $F$-statistic is $t_{\max}^2$ equivalent to? How does the null distribution of $t_{\max}^2$ compare to that of $t(\bm c)^2$?

\end{itemize}

\end{prob}

\begin{sol}
\begin{enumerate}
\item[(a)] Recall that \[R^2 = \frac{\text{SSR}}{\text{SST}} = \frac{||\bm{X} \widehat{\bm{\beta}} - \overline{y} \bm{1}_n||^2}{||\bm{y} - \overline{y} \bm{1}_n||^2} = \frac{||(\bm{H} - \bm{H}_1) \bm{y}||^2}{||(\bm{I} - \bm{H}_1) \bm{y}||^2}.\] Then we also have \[1 - R^2 = \frac{||(\bm{I} - \bm{H}_1) \bm{y}||^2 - ||(\bm{H} - \bm{H}_1) \bm{y}||^2}{||(\bm{I} - \bm{H}_1) \bm{y}||^2} = \frac{||(\bm{I} - \bm{H}) \bm{y}||^2}{||(\bm{I} - \bm{H}_1) \bm{y}||^2}.\]

So, $(\frac{n-p}{p-1}) \frac{R^2}{1-R^2} = \frac{||(\bm{H} - \bm{H}_1) \bm{y}||^2 / (p-1)}{||(\bm{I} - \bm{H}) \bm{y}||^2 / (n - p)} =: F$. But this is exactly the $F$-statistic for the hypothesis test with null hypothesis $H_0:\, \beta_1 = \dots = \beta_{p-1} = 0$. We can also invert this relationship and find that for $c := \frac{p-1}{n-p} F$, $R^2 = \frac{c}{c+1}$.

Under the null distribution, the $F$-statistic is $F$ distributed with $p - 1$ and $n - p$ degrees of freedom. Note that $R^2$ and $F$ are positively related which makes sense - a higher $R^2$ indicates that the full model explains more of the variance in the observed data, which in turn suggests that we do not have $\beta_1 = \dots = \beta_{p-1} = 0$.

\item[(b)] % NOTE: NEED TO FINISH
<<2b simulation, echo=TRUE, results=TRUE>>=
# Simulation parameters
n <- 100
p_list <- c(2, 25, 50, 75, 99)

# Dataframe for simulation data
sim_data <- data.frame()

# Run simulation for different values of p
for(p in p_list){
  # Sample F-statistics
  f <- rf(n = n, df1 = p-1, df2 = n-p)
  
  # Compute R^2 from F-statistics
  c <- (p-1)/(n-p)*f
  R_2 <- c/(c+1)
  
  # Add simulated data to dataframe
  df <- data.frame(p = rep(p, length(R_2)), R_2 = R_2)
  sim_data <- rbind(sim_data, df)
}

# Make plot
R_2_plt <- 
  sim_data %>%
    ggplot(aes(x = R_2)) +
      geom_histogram(bins = 30) +
      ylab("Count") + 
      xlab("R^2") +
      ylim(0, 75)
      xlim(0, 1) +
      facet_wrap( ~ p)

print(R_2_plt)
@

\item[(c)] Recall that $t = \frac{\widehat{\beta}_j}{\widehat{\sigma}/s_j}$, so $t^2 = \frac{\widehat{\beta}_j^2 s_j^2}{\widehat{\sigma}^2}$ and that $F = \frac{(||\bm{X} \widehat{\bm{\beta}} - \bm{X}_{-j} \widehat{\bm{\beta}}_{-j}||^2) / 1}{(||\bm{y} - \bm{X} \widehat{\bm{\beta}}||^2) / (n - p)}$. But, $\widehat{\sigma}^2 = \frac{1}{n-p} ||\bm{y} - \bm{X} \widehat{\bm{\beta}}||^2$, so 
\[t^2 = \frac{\widehat{\bm{\beta}}^2_j s_j^2}{||\bm{y} - \bm{X} \widehat{\bm{\beta}}||^2 / (n-p)}\] and thus it suffices to show that $\widehat{\bm{\beta}}^2_j s^2_j = ||\bm{X} \widehat{\bm{\beta}} - \bm{X}_{-j} \widehat{\bm{\beta}}_{-j}||^2 = ||(\bm{H} - \bm{H}_{-j}) \bm{y}||^2 = ||\bm{H}_j \bm{y}||^2 = ||\widehat{\beta}_{j | -j} \bm{x}_{*j}^{\bot}||^2$. Note that $s_j^2 = ||\bm{x}_{*j}^{\bot}||^2$, so this equivalent to proving that $||\widehat{\beta}_{j | -j} \bm{x}_{*j}^{\bot}||^2 = \widehat{\bm{\beta}}^2_j s^2_j = \widehat{\bm{\beta}}^2_j ||\bm{x}_{*j}^{\bot}||^2 = ||\widehat{\bm{\beta}}_j \bm{x}_{*j}^{\bot}||^2$.

\item[(d)]
\end{enumerate}
\end{sol}

\begin{prob} \label{prob:data}\textbf{Case study: Violent crime.}

\noindent The \texttt{Statewide\_crime.dat} file under \texttt{stat-961-fall-2021/data} contains information on the number of violent crimes and murders for each U.S. state in a given year, as well as three socioeconomic indicators: percent living in metropolitan areas, high school graduation rate, and poverty rate.
<<message = FALSE>>=
crime_data = read_tsv("../../data/Statewide_crime.dat")
print(crime_data, n = 5)
@

\noindent The goal of this problem is to study the relationship between the three socioeconomic indicators and the per capita violent crime rate.

\begin{itemize}
\item[(a)] These data contain the total number of violent crimes per state, but it is more meaningful to model violent crime rate per capita. To this end, go online to find a table of current populations for each state. Augment \verb|crime_data| with a new variable called \verb|Pop| with this population information (see \verb|dplyr::left_join|) and create a new variable called \verb|CrimeRate| defined as \verb|CrimeRate = Violent/Pop| (see \verb|dplyr::mutate|).

\item[(b)] Explore the variation and covariation among the variables \verb|CrimeRate|, \verb|Metro|, \verb|HighSchool|, \verb|Poverty| with the help of visualizations and summary statistics.

\item[(c)] Construct linear model based hypothesis tests and confidence intervals associated with the relationship between \verb|CrimeRate| and the three socioeconomic variables, printing and/or plotting your results. Discuss the results in technical terms.

\item[(d)] Discuss your interpretation of the results from part (c) in language that a policymaker could comprehend, including any caveats or limitations of the analysis. Comment on what other data you might want to gather for a more sophisticated analysis of violent crime.

\end{itemize}

\end{prob}
\begin{sol}
\end{sol}




\end{document}