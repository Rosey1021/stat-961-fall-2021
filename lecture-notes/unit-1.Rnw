% (C) Brett Klamer - MIT - http://opensource.org/licenses/MIT
% Please contact me if you find any errors or make improvements
% Contact details at brettklamer.com

\documentclass[11pt,letterpaper,english,oneside]{article} % article class is a standard class
%==============================================================================
%Load Packages
%==============================================================================
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % easy page margins
\usepackage[utf8]{inputenc} % editor uses utf-8 encoding
\usepackage[T1]{fontenc} % T1 font pdf output
\usepackage{lmodern} % Latin modern roman font
\usepackage{bm, bbm} % bold and blackboard bold math symbols
\usepackage{amsmath, amsfonts, amssymb, amsthm} % math packages
\usepackage[final]{microtype} % better microtypography
\usepackage{graphicx} % for easier grahics handling
\usepackage[hidelinks, colorlinks=true, linkcolor = blue, urlcolor = blue]{hyperref} % to create hyperlinks
\usepackage{float} % tells floats to stay [H]ere!
\usepackage{mdframed} % it's better than framed. knitr uses framed so settings won't conflict
\usepackage{enumitem} % nice lists
\usepackage{fancyhdr} % nice headers
\usepackage{caption}  % to control figure and table captions


\captionsetup{width=0.9\textwidth, justification = raggedright}

%==============================================================================
% Enter name and homework title here
%==============================================================================
\title{Unit 1: Linear models: Estimation}
\author{Eugene Katsevich}
% \date{August 31, 2021}

%==============================================================================
% Put title and author in PDF properties
%==============================================================================
\makeatletter % change interpretation of @
\hypersetup{pdftitle={\@title},pdfauthor={\@author}}


%==============================================================================
% Header settings
%==============================================================================
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @

%==============================================================================
% List spacing
%==============================================================================
\setlist[itemize]{parsep=0em} % fix itemize spacing
\setlist[enumerate]{parsep=0em} % fix enumerate spacing

%==============================================================================
% set knitr options
%==============================================================================
% latex (change space before and after knitr kframe; based on framed package)
\setlength{\OuterFrameSep}{0.3em}
% R
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)

# inline hook to process the output of \Sexpr{} statements to just 2 digits
inline_hook <- function(x) {
  if(is.numeric(x)) x <- round(x, 2)
  paste(as.character(x), collapse=", ")
}
knit_hooks$set(inline = inline_hook)

# cache all chunks
opts_chunk$set(cache = TRUE)

@

\begin{document}

\maketitle

\section{Types of predictors; interpreting linear model coefficients\\(Agresti 1.2)}

The types of predictors $x_j$ (e.g. binary or continuous) has less of an effect on the regression than the type of response, but it is still important to pay attention to the former.

\paragraph{Intercepts.} It is common to include an \textit{intercept} in a linear regression model, a predictor $x_0$ such that $x_{i0} = 1$ for all $i$. When an intercept is present, we index it as the 0th predictor. The simplest kind of linear model is the \textit{intercept-only model} or the \textit{one-sample model}:
\begin{equation}
y = \beta_0 + \epsilon.
\label{eq:one-sample-model}
\end{equation}
The parameter $\beta_0$ is the mean of the response. 

\paragraph{Binary predictors.} In addition to an intercept, suppose we have a binary predictor $x_1 \in \{0,1\}$ (e.g. $x_1 = 1$ for patients who took blood pressure medication and $x_1 = 0$ for those who didn't). This leads to the following linear model:
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \epsilon.
\label{eq:two-sample-model}
\end{equation}
Here, $\beta_0$ is the mean response (say blood pressure) for observations with $x_1 = 0$ and $\beta_0 + \beta_1$ is the mean response for observations with $x_1 = 1$. Therefore, the parameter $\beta_1$ is the difference in mean response between observations with $x_1 = 1$ and $x_1 = 0$. This parameter is sometimes called the \textit{effect} or \textit{effect size} of $x_1$, though a causal relationship might or might not be present. The model~\eqref{eq:two-sample-model} is sometimes called the \textit{two-sample model}, because the response data can be split into two ``samples'': those corresponding to $x_1 = 0$ and those corresponding to $x_1 = 1$.

\paragraph{Categorical predictors.} A binary predictor is a special case of a categorical predictor: A predictor taking two or more discrete values. Suppose we have a predictor $w \in \{w_0, w_1, \dots, w_{C-1}\}$, where $C \geq 2$ is the number of categories and $w_0, \dots, w_{C-1}$ are the \textit{levels} of $w$. E.g. suppose $\{w_0, \dots, w_{C-1}\}$ is the collection of U.S. states, so that $C = 50$. If we want to regress a response on the categorical predictor $w$, we cannot simply set $x_1 = w$ in the context of the linear regression~\eqref{eq:two-sample-model}. Indeed, $w$ does not necessarily take numerical values. Instead, we need to add a predictor $x_j$ for each of the levels of $w$. In particular, define $x_j \equiv \mathbbm 1(w = w_j)$ for $j = 1, \dots, C-1$ and consider the regression
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \cdots + \beta_{C-1}x_{C-1} + \epsilon.
\label{eq:C-sample-model}
\end{equation}
Here, category 0 is the \textit{base category}, and $\beta_0$ represents the mean response in the base category. The coefficient $\beta_j$ represents the difference in mean response between the $j$th category and the base category.

\paragraph{Quantitative predictors.} A quantitative predictor is one that can take on any real value. For example, suppose that $x_1 \in \mathbb R$, and consider the linear model
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \epsilon.
\label{eq:simple-regression-intro}
\end{equation}
Now, the interpretation of $\beta_1$ is that an increase in $x_1$ by 1 is associated with an increase in $y$ by $\beta_1$. We must be careful to avoid saying ``an increase in $x_1$ by 1 \textit{causes} $y$ to increase by $\beta_1$'' unless we make additional causal assumptions. Note that the units of $x_1$ matter. If $x_1$ is the height of a person, then the value and the interpretation of $\beta_1$ changes depending on whether that height is measured in feet or in meters. 

\paragraph{Ordinal predictors.} There is an awkward category of predictor in between categorical and continuous called \textit{ordinal}. An ordinal predictor is one that takes a discrete number of values, but these values have an intrinsic ordering, e.g. $x_1 \in \{\texttt{small}, \texttt{medium}, \texttt{large}\}$. It can be treated as categorical at the cost of losing the ordering information, or as continuous if one is willing to assign quantatitive values to each category.

\paragraph{Multiple predictors.} A linear regression need not contain just one predictor (aside from an intercept). For example, let's say $x_1$ and $x_2$ are two predictors. Then, a linear model with both predictors is
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon.
\label{eq:multiple-regression}
\end{equation}
When there are multiple predictors, the interpretation of coefficients must be revised somewhat. For example, $\beta_1$ in the above regression is the effect of an increase in $x_1$ by 1 \textit{while holding $x_2$ constant} or \textit{while adjusting for $x_2$} or \textit{while controlling for $x_2$}. If $y$ is blood pressure, $x_1$ is a binary predictor indicating blood pressure medication taken and $x_2$ is sex, then $\beta_1$ is the effect of the medication on blood pressure while controlling for sex. In general, the coefficient of a predictor depends on what other predictors are in the model. As an extreme case, suppose the medication has no actual effect, but that men generally have higher blood pressure and higher rates of taking the medication. Then, the coefficient $\beta_1$ in the single regression model~\eqref{eq:two-sample-model} would be nonzero but the coefficient in the multiple regression model~\eqref{eq:multiple-regression} would be equal to zero. In this case, sex acts as a \textit{confounder}.


\paragraph{Interactions.} Note that the multiple regression model~\eqref{eq:multiple-regression} has the built-in assumption that the effect of $x_1$ on $y$ is the same for any fixed value of $x_2$ (and vice versa). In some cases, the effect of one variable on the response may depend on the value of another variable. In this case, it's appropriate to add another predictor called an \textit{interaction}. Suppose $x_2$ is quantitative (e.g. years of job experience) and $x_2$ is binary (e.g. sex, with $x_2 = 1$ meaning male). Then, we can define a third predictor $x_3$ as the product of the first two, i.e. $x_3 = x_1x_2$. This gives the regression model
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2 + \epsilon.
\label{eq:interaction}
\end{equation}
Now, the effect of adding another year of job experience is $\beta_1$ for females and $\beta_1 + \beta_3$ for males. The coefficient $\beta_3$ is the difference in the effect of job experience between males and females.

\section{Model matrices, model vectors spaces, and identifiability\\(Agresti 1.3-1.4)}

The matrix $\bm X$ is called the \textit{model matrix} or the \textit{design matrix}. Concatenating the linear model equations across observations give us an equivalent formulation:
\begin{equation*}
\bm y = \bm X \bm \beta + \bm \epsilon; \quad \mathbb E[\bm \epsilon] = \bm 0,\ \text{Var}[\bm \epsilon] = \sigma^2 \bm I_n
\end{equation*}
or
\begin{equation*}
\mathbb E[\bm y] = \bm X \bm \beta = \bm \eta.
\end{equation*}
As $\bm \beta$ varies in $\mathbb R^p$, the set of possible vectors $\bm \eta \in \mathbb R^n$ is defined
\begin{equation*}
C(\bm X) \equiv \{\bm \eta = \bm X \bm \beta: \bm \beta \in \mathbb R^p\}. 
\end{equation*}
$C(\bm X)$, called the \textit{model vector space}, is a subspace of $\mathbb R^n$: $C(\bm X) \subseteq \mathbb R^n$. Since
\begin{equation*}
\bm X \bm \beta = \beta_1 \bm x_{*1} + \cdots + \beta_p \bm x_{*p},
\end{equation*}
the model vector space is the column space of the matrix $\bm X$.

The \textit{dimension} of $C(\bm X)$ is the rank of $\bm X$, i.e. the number of linearly independent columns of $\bm X$. If $\text{rank}(\bm X) < p$, this means that there are two different vectors $\bm \beta$ and $\bm \beta'$ such that $\bm X \bm \beta = \bm X \bm \beta'$. Therefore, we have two values of the parameter vector that give the same model for $\bm y$. This makes $\bm \beta$ \textit{not identifiable}, and makes it impossible to reliably determine $\bm \beta$ based on the data. For this reason, we will generally assume that $\bm \beta$ is \textit{identifiable}, i.e. $\bm X \bm \beta \neq \bm X \bm \beta'$ if $\bm \beta \neq \bm \beta'$. This is equivalent to the assumption that $\text{rank}(\bm X) = p$. Note that this cannot hold when $p > n$, so for the majority of the course we will assume that $p \leq n$. In this case, $\text{rank}(\bm X) = p$ if and only if $\bm X$ has \textit{full-rank}.

As an example when $p \leq n$ but when $\bm \beta$ is still not identifiable, consider the case of a categorical predictor. Suppose the categories of $w$ were $\{w_1, \dots, w_{C-1}\}$, i.e. the baseline category $w_0$ did not exist. In this case, the model~\eqref{eq:C-sample-model} would not be identifiable because $x_0 = 1 = x_1 + \cdots + x_{C-1}$ and thus $x_{*0} = 1 = x_{*1} + \cdots + x_{*,C-1}$. Indeed, this means that one of the predictors can be expressed as a linear combination of the others, so $\bm X$ cannot have full rank. A simpler way of phrasing the problem is that we are describing $C-1$ intrinsic parameters (the means in each of the $C-1$ groups) with $C$ model parameters. There must therefore be some redundancy. For this reason, if we include an intercept term in the model then we must designate one of our categories as the baseline and exclude its indicator from the model.

\section{Least squares estimation (Agresti 2.1.1, 2.7.1)}

Now, suppose that we are given a dataset $(\bm X, \bm y)$. How do we go about estimating $\bm \beta$ based on this data? The canonical approach is the \textit{method of least squares}:
\begin{equation}
\bm {\widehat \beta} \equiv \underset{\bm \beta}{\arg \min}\ \|\bm y - \bm X \bm \beta\|^2.
\end{equation}
The quantity 
\begin{equation}
\|\bm y - \bm X \bm{\widehat \beta}\|^2 = \|\bm y - \bm{\widehat \mu}\|^2 = \sum_{i = 1}^n (y_i - \widehat \mu_i)^2 
\end{equation}
is called the \textit{residual sum of squares (RSS)}, and it measures the lack of fit of the linear regression model. We therefore want to choose $\bm{\widehat \beta}$ to minimize this lack of fit. Note that if $\bm \epsilon$ is assumed to be $N(0,\sigma^2 \bm I_n)$, then the least squares solution would also be the maximum likelihood solution. Indeed, for $y_i \sim N(\mu_i, \sigma^2)$, the log-likelihood is
\begin{equation*}
\log \left[\prod_{i = 1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(y_i - \mu_i)^2}{2\sigma^2}\right)\right] = \text{constant} - \frac{1}{2\sigma^2}\sum_{i = 1}^n (y_i - \mu_i)^2.
\end{equation*}

Letting $L(\bm{\beta}) = \frac12\|\bm y - \bm X \bm \beta\|^2$, we can do some calculus to derive that
\begin{equation}
\frac{\partial}{\partial \bm \beta}L(\bm \beta) = -\bm X^T(\bm y - \bm X \bm \beta).
\end{equation}
Setting this vector of partial derivatives equal to zero, we arrive at the \textit{normal equations}:
\begin{equation}
-\bm X^T(\bm y - \bm X \bm{\widehat \beta}) = 0 \quad \Longleftrightarrow \quad \bm X^T \bm X \bm {\widehat \beta} = \bm X^T \bm y.
\label{eq:normal-equations}
\end{equation}
If $\bm X$ is full rank, the matrix $\bm X^T \bm X$ is invertible and we can therefore conclude that
\begin{equation}
\bm {\widehat \beta} = (\bm X^T \bm X)^{-1}\bm X^T \bm y.
\label{eq:beta-hat}
\end{equation}

Now that we have derived the least squares estimator, we can compute its bias and variance. To obtain the bias, we first calculate that
\begin{equation*}
\mathbb E[\widehat{\bm\beta}] = \mathbb E[(\bm X^T \bm X)^{-1}\bm X^T \bm y] = (\bm X^T \bm X)^{-1}\bm X^T \mathbb E[\bm y] = (\bm X^T \bm X)^{-1}\bm X^T \bm X \bm \beta = \bm \beta.
\end{equation*}
Therefore, the least squares estimator is unbiased. To obtain the variance, we compute
\begin{equation}
\begin{split}
\text{Var}[\bm{\widehat\beta}] &= \text{Var}[(\bm X^T \bm X)^{-1}\bm X^T \bm y] \\
&= (\bm X^T \bm X)^{-1}\bm X^T\text{Var}[\bm y]\bm X (\bm X^T \bm X)^{-1} \\
&= (\bm X^T \bm X)^{-1}\bm X^T(\sigma^2 \bm I_n)\bm X (\bm X^T \bm X)^{-1} \\
&= \sigma^2 (\bm X^T \bm X)^{-1}.
\label{eq:var-of-beta-hat}
\end{split}
\end{equation}
According to the Gauss-Markov theorm, this covariance matrix computed above is the smallest (in the sense of positive semidefinite matrices) among all linear unbiased estimates of $\bm \beta$.

\section{Least squares estimation R demo (Agresti 2.6)}

The R demo will be based on the \texttt{ScotsRaces} data from the textbook. Data description (quoted from the textbook):
\begin{quote}
``Each year the Scottish Hill Runners Association publishes a list of hill races in Scotland for the year. The table below shows data on the record time for some of the races (in minutes). Explanatory variables listed are the distance of the race (in miles) and the cumulative climb (in thousands of feet).''
\end{quote}

<<cache = FALSE, message = FALSE>>=
library(tidyverse)
@

<<>>=
# read the data into R
scots_races = read_tsv("../data/ScotsRaces.dat", col_types = "cddd")
scots_races
@

<<>>=
# Exploration

# pairs plot
GGally::ggpairs(scots_races %>% select(-race))


# Q: What are the typical ranges of the variables?
# Q: What are the relationships among the variables?


# mile time versus distance 

scots_races %>% 
  mutate(mile_time = time/distance) %>%
  ggplot(aes(x = distance, y = mile_time)) +
  geom_point()

scots_races %>% 
  mutate(mile_time = time/distance) %>%
  ggplot(aes(x = distance, y = mile_time, label = race,
             color = climb)) +
  geom_point()

# Q: How does mile time vary with distance? 
# Q: What races deviate from this trend?
# Q: How does climb play into it?

@

<<>>=
# Linear model

# Q: What is the effect of an extra mile of distance on time?
lm_fit = lm(time ~ distance + climb, data = scots_races)
summary(lm_fit)
@

<<>>=
# Linear model with interaction

# Q: What is the effect of an extra mile of distance on time 
#  for a run with low climb?

# Q: What is the effect of an extra mile of distance on time 
#  for a run with high climb?

lm_fit_int = lm(time ~ distance*climb, data = scots_races)
summary(lm_fit_int)

@

\section{Linear regression as orthogonal projection \\ (Agresti 2.2, 2.3, 2.4.2, 2.4.3, 2.4.4)}

Let's think about the mapping $\bm y \mapsto \bm{\widehat \mu} = \bm X\bm{\widehat \beta} \in C(\bm X)$. We claim that this mapping is an \textit{orthogonal projection}. Geometrically it makes sense, since we define $\bm{\widehat \beta}$ so that $\bm{\widehat \mu} \in C(\bm X)$ is as close to $\bm y$ as possible. The shortest path between a point and a plane is the perpendicular. One way of seeing this is to show that $\bm v^T (\bm y - \bm X \bm{\widehat \beta}) = 0$ for each $\bm v \in C(\bm X)$. Since the columns $\{\bm x_{*1}, \dots, \bm x_{*p}\}$ of $\bm X$ form a basis for $C(\bm X)$, it suffices to show that $\bm x_{*j}^T (\bm y - \bm X \bm{\widehat \beta}) = 0$ for each $j = 1, \dots, p$. This is a consequence of the normal equations $\bm X^T(\bm y - \bm X\bm{\widehat \beta}) = 0$ derived in~\eqref{eq:normal-equations}.

To derive the projection matrix corresponding to this orthogonal projection, we write
\begin{equation}
\bm{\widehat \mu} = \bm X\bm{\widehat \beta} = \bm X(\bm X^T \bm X)^{-1}\bm X^T \bm y = \bm H \bm y,
\end{equation}
where
\begin{equation}
\bm H \equiv  \bm X(\bm X^T \bm X)^{-1}\bm X^T
\end{equation}
is called the \textit{hat matrix}. This is the orthogonal projection matrix onto $C(\bm X)$. Recall that a matrix $\bm P$ is an orthogonal projection onto a subspace $\bm W$ if for all $\bm v \in \bm W$ we have $\bm P\bm v = \bm v$ and for all $\bm v \in \bm W^{\perp}$ we have $\bm P \bm v = 0$. We can check for example the first of these conditions by noting that if $\bm v \in C(\bm X)$, then $\bm v = \bm X \bm \beta$ for some $\bm \beta \in \mathbb R^p$. Therefore, we have 
\begin{equation*}
\bm H \bm v = \bm X(\bm X^T \bm X)^{-1}\bm X^T \bm X \bm \beta = \bm X \bm \beta = \bm v.
\end{equation*}
A simple example of $\bm H$ can be obtained by considering the intercept-only regression.

One consequence of this observation is that the fitted values $\bm{\widehat \mu}$ depend on $\bm X$ only through $C(\bm X)$. As we will see in Homework 1, there are many different model matrices $\bm X$ leading to the same model space. Essentially, this reflects the fact that there are many different bases for the same vector space. Consider for example changing the units on the columns of $\bm X$. It can be verified that not just the fitted values $\bm{\widehat \mu}$ but also the predictions on a new set of features remain invariant to reparametrization (this follows from parts (a) and (b) of Homework 1 Problem 1). Therefore, while reparametrization can have a huge impact on the fitted coefficients, it has no impact on the predictions of linear regression.

The orthogonality property of least squares, together with the Pythagorean theorem, leads to the following fundamental relationship. Let's say that $S \subset \{0, 1, \dots, p\}$ is a subset of the predictors. First regress $\bm y$ on $\bm X$ to get $\bm{\widehat \beta}$ as usual. Then, we consider the \textit{partial model matrix} $\bm X_{*S}$ obtained by selecting only the columns in $S$. Regression $\bm y$ on $\bm X_{*S}$ results in $\bm{\widehat \beta}_S$ (note: $\bm{\widehat \beta}_S$ is not necessarily obtained from $\bm{\widehat\beta}$ by extracting the coefficients corresponding to $S$). Now, consider the three points $\bm y, \bm X\bm{\widehat \beta}, \bm X_{*S}\bm{\widehat \beta}_S \in \mathbb R^n$. Since $\bm X\bm{\widehat \beta}$ and $\bm X_{*S}\bm{\widehat \beta}_S$ are both in $C(\bm X)$, it follows by the orthogonal projection properpty that $\bm y - \bm X\bm{\widehat \beta}$ is orthogonal to $\bm X\bm{\widehat \beta}- \bm X_{*S}\bm{\widehat \beta}_S$. In other words, these three points form a right triangle. By the Pythagorean theorem, we conclude that
\begin{equation}
\|\bm y -  \bm X_{*S}\bm{\widehat \beta}_S\|^2 = \|\bm X\bm{\widehat \beta}- \bm X_{*S}\bm{\widehat \beta}_S\|^2 + \|\bm y - \bm X\bm{\widehat \beta}\|^2.
\label{eq:pythagorean-theorem}
\end{equation}
We will rely on this fundamental relationship throughout this course.

For now, we can extract a few consequences of the relationship~\eqref{eq:pythagorean-theorem}. As a starting point, consider the case when $S = \{0\}$, i.e. the partial model is the intercept-only model. In this case, $\bm X_{*S} = \bm 1_n$ and $\bm{\widehat \beta_S} = \bar y$. Therefore, equation~\eqref{eq:pythagorean-theorem} implies that
\begin{equation}
\|\bm y -  \bar y \bm 1_n\|^2 = \|\bm X\bm{\widehat \beta}- \bar y \bm 1_n\|^2 + \|\bm y - \bm X\bm{\widehat \beta}\|^2.
\end{equation}
Equivalently, we can rewrite this equation as follows:
\begin{equation}
\text{SST} \equiv \sum_{i = 1}^n (y_i - \bar y)^2 = \sum_{i = 1}^n (\widehat \mu_i - \bar y)^2 + \sum_{i = 1}^n (y_i - \widehat \mu_i)^2 \equiv \text{SSR} + \text{SSE}.
\label{eq:anova}
\end{equation}

\section{Correlation, multiple correlation, and $R^2$ \\ (Agresti 2.1.3, 2.4.6)}

\paragraph{ANOVA decomposition for $C$ groups model.} Let's consider the special case of the ANOVA decomposition~\eqref{eq:anova} when the model matrix $\bm X$ represents a single categorical predictor $w$. In this case, each observation $i$ is associated to one of the $C$ classes of $w$, which we denote $c(i) \in \{1, \dots, C\}$. Let's consider the $C$ groups of observations $\{i: c(i) = c\}$ for $c \in \{1, \dots, C\}$. For example, $w$ may be the type of a car (compact, midsize, minivan, etc.) and $y$ might be its fuel efficiency in miles per gallon.
<<out.width = "75%", fig.width = 5, fig.height = 4, fig.align='center'>>=
mpg %>%
  ggplot() + 
  geom_boxplot(aes(x = fct_reorder(class, hwy), y = hwy)) + 
  labs(x = "Car class", y = "Gas mileage (mpg)") + 
  theme_bw() 
@

\noindent It is easy to check that the least squares fitted values $\widehat \mu_i$ are simply the means of the corresponding groups:
\begin{equation}
\widehat \mu_i = \bar y_{c(i)}, \quad \text{where}\ \bar y_{c(i)} \equiv \frac{\sum_{i: c(i) = c} y_i}{|\{i: c(i) = c\}|}.
\end{equation}
Therefore, we have
\begin{equation}
\text{SSR} = \sum_{i = 1}^n (\widehat \mu_i - \bar y)^2 = \sum_{i = 1}^n (\bar y_{c(i)} - \bar y)^2 \equiv \text{between-groups sum of squares (SSB)}
\end{equation}
and
\begin{equation}
\text{SSE} = \sum_{i = 1}^n (y_i - \widehat \mu_i)^2 = \sum_{i = 1}^n (y_i - \bar y_{c(i)})^2 \equiv \text{within-groups sum of squares (SSW)}.
\end{equation}
We therefore obtain the following corollary of the ANOVA decomposition~\eqref{eq:anova}:
\begin{equation}
\text{SST} = \text{SSB} + \text{SSW}.
\label{eq:anova-C-groups}
\end{equation}

\paragraph{$R^2$ definition and (multiple) correlation.} The ANOVA decompositions~\eqref{eq:anova} and~\eqref{eq:anova-C-groups} of the variation in $\bm y$ into that explained by the linear regression model (SSR) and that left over (SSE) leads naturally to the definition of $R^2$ as the fraction of variation in $\bm y$ explained by the linear regression model:
\begin{equation}
R^2 \equiv \frac{\text{SSR}}{\text{SST}} = \frac{\sum_{i = 1}^n (\widehat \mu_i - \bar y)^2}{\sum_{i = 1}^n (y_i - \bar y)^2} = \frac{\|\bm X\bm{\widehat \beta}- \bar y \bm 1_n\|^2}{\|\bm y -  \bar y \bm 1_n\|^2}.
\end{equation}
By the decomposition~\eqref{eq:anova}, we have $R^2 \in [0,1]$. The closer $R^2$ is to 1, the closely the data follow the fitted linear regression model. There is a connection between $R^2$ and correlation. To see this, let us first consider the case of the simple linear regression model with one predictor
\begin{equation}
y = \beta_0 + \beta_1 x_1 + \epsilon.
\label{eq:simple-regression}
\end{equation}
In this simple case, one can directly derive a formula for the fitted coefficients:
\begin{equation}
\widehat \beta_0 = \bar y - \widehat \beta_1 \bar x; \quad 
\widehat \beta_1 = \frac{\sum_{i = 1}^n (x_i - \bar x)(y_i - \bar y)}{\sum_{i = 1}^n (x_i - \bar x)^2}.
\label{eq:simple-regression-coefficients}
\end{equation}
Therefore,
\begin{equation*}
\bm{\widehat \mu}-\bar y \bm 1_n = \widehat \beta_0 \bm 1_n + \widehat \beta_1 \bm x_{*1} - \bar y \bm 1_n= \widehat \beta_1(\bm x_{*1} - \bar x \bm 1_n)
\end{equation*}
and thus
\begin{equation}
R^2 = \frac{\|\bm{\widehat \mu} - \bar y \bm 1_n\|^2}{\|\bm y - \bar y \bm 1_n\|^2} = \frac{\widehat \beta_1^2 \|\bm x_{*1} - \bar x \bm 1_n\|^2}{\|\bm y - \bar y \bm 1_n\|^2} = \left(\frac{\sum_{i = 1}^n (x_i - \bar x)(y_i - \bar y)}{\left(\sum_{i = 1}^n (x_i - \bar x)^2\right)^{1/2}\left(\sum_{i = 1}^n (y_i - \bar y)^2\right)^{1/2}}\right)^2 \equiv \rho_{xy}^2,
\end{equation}
where $\rho_{xy}$ is the sample correlation between $x_1$ and $y$. Therefore, in a simple linear regression, $R^2$ is the squared sample correlation between $x_1$ and $y$. For general regressions, one can derive that $R^2$ is the squared sample correlation between $\bm X \bm{\widehat \beta}$ and $\bm y$. For this reason, $R^2$ is sometimes called the \textit{multiple correlation coefficient}.

\paragraph{Regression to the mean.}

Let's go back to the simple regression model~\eqref{eq:simple-regression}, and let's take a closer look at $\widehat \beta_1$ in \eqref{eq:simple-regression-coefficients}. Denoting by $\rho_x$ is the sample standard deviation of $x_1$ and $\rho_y$ is the sample standard deviation of $y$, we can rewrite $\widehat \beta_1$ as
\begin{equation}
\widehat \beta_1 = \frac{\rho_y}{\rho_x}\cdot\rho_{xy}.
\label{eq:coefficient-as-correlation}
\end{equation}
Assuming that $\bm x_{*1}$ and $\bm y$ have been normalized to have the same sample standard deviation $\rho_x = \rho_y$, we find that the least squares coefficient $\widehat \beta_1$ is equal to the sample correlation $\rho_{xy}$ between $x$ and $y$. Since $|\rho_{xy}| < 1$ unless $\bm x_{*1}$ and $\bm y$ are perfectly correlated (by the Cauchy-Schwarz inequality), this means that 
\begin{equation}
|\widehat \mu_i - \bar y| < |x_i - \bar x| \quad \text{for each } i.
\label{eq:regression-to-the-mean}
\end{equation}
Therefore, we expect $y_i$ to be closer to its mean than $x_i$ is to its mean. This phenomenon is called \textit{regression to the mean} (and is in fact the origin of the term ``regression''). Many mistakenly attribute a causal mechanism to this phenomenon, when in reality it is simply a statistical artifact. For example, suppose $x_i$ is the number of games a sports team won last season and $y_i$ is the number of games it won this season. It is widely observed that teams with exceptional performance in a given season suffer a ``winner's curse'', performing worse in the next season. The reason for the winner's curse is simple: teams perform exceptionally well due to a combination of skill and luck. While skill stays roughly constant from year to year, the team which performed exceptionally well in a given season is unlikely to get as lucky as it did next season. 

\paragraph{$R^2$ increases as predictors are added.}

The $R^2$ is an \textit{in-sample} measure, i.e. it uses the same data to fit the model and to assess the quality of the fit. Therefore, it is generally an optimistic measure of the (out-of-sample) prediction error. One manifestation of this is that the $R^2$ increases if any predictors are added to the model (even if these predictors are ``junk''). To see this, it suffices to show that SSE decreases as we add predictors. Without loss of generality, suppose that we start with a model including predictors $S \subset \{0, 1, \dots, p\}$ and compare it to the model including all the predictors $\{0,1,\dots,p\}$. We can read off from the Pythagorean theorem~\eqref{eq:pythagorean-theorem} that
\begin{equation*}
\text{SSE}(\bm X_{*S}, \bm y) = \|\bm y -  \bm X_{*S}\bm{\widehat \beta}_S\|^2 \geq  \|\bm y -  \bm X\bm{\widehat \beta}\|^2 = \text{SSE}(\bm X, \bm y).
\end{equation*}
Adding many junk predictors will have the effect of degrading predictive performance but will nevertheless increase $R^2$.


\section{Collinearity, adjustment, and partial correlation (Agresti 2.2.4, 2.5.6, 2.5.7, 4.6.5)}

An important part of linear regression analysis is the dependence of the least squares coefficient for a predictor on what other predictors are in the model. This relationship is dictated by the extent to which the given predictor is correlated with the other predictors. In this section, we'll use some additional notation. Let $S \subset \{1, \dots, p\}$ be a group of predictors (we can assume without loss of generality that $S = \{1, \dots, s\}$ for some $1 \leq s < p$). Then, denote $\text{-}S \equiv \{1, \dots, p\} \setminus S$. Let $\bm{\widehat \beta}_S$ denote the least squares coefficients when regressing $\bm y$ on $\bm X_{*S}$ and let $\bm{\widehat \beta}_{S|\text{-}S}$ denote the least squares coefficients corresponding to $S$ when regressing $\bm y$ on $\bm X = (\bm X_{*S}, \bm X_{*,\text{-}S})$.

\paragraph{Least squares estimates in the orthogonal case.}

The simplest case to analyze is when a groups of predictors $\bm X_{*S}$ is orthogonal to the rest of the predictors $\bm X_{*,\text{-}S}$ in the sense that
\begin{equation}
\bm X_{*S}^T \bm X_{*,\text{-}S} = \bm 0.
\end{equation}
In this case, we can derive the least squares coefficient vector $\bm{\widehat \beta} = (\bm{\widehat \beta}_{S|\text{-}S}, \bm{\widehat \beta}_{\text{-}S|S})$ from the normal equations:
\begin{equation}
\begin{split}
{\bm{\widehat \beta}_{S|\text{-}S} \choose \bm{\widehat \beta}_{\text{-}S|S}} &= (\bm X^T \bm X)^{-1}\bm X^T \bm y \\
&= 
\begin{pmatrix}
\bm X_S^T \bm X_S & 0 \\
0 & \bm X_{\text{-}S}^T \bm X_{\text{-}S}
\end{pmatrix}^{-1}{\bm X_S^T \choose \bm X_{\text{-}S}}\bm y \\
&= {(\bm X_S^T \bm X_S)^{-1}\bm X_S^T\bm y \choose (\bm X_{\text{-}S}^T \bm X_{\text{-}S})^{-1}\bm X_{\text{-}S}^T\bm y} \\
&= {\bm{\widehat \beta}_{S} \choose \bm{\widehat \beta}_{\text{-}S}}.
\label{eq:orthogonality}
\end{split}
\end{equation}
Therefore, the least squares coefficients when regressing $\bm y$ on $(\bm X_S, \bm X_{\text{-}S})$ are the same as those obtained from regressing $\bm y$ separately on $\bm X_S$ and $\bm X_{\text{-}S}$, i.e.
\begin{equation}
\bm{\widehat \beta}_{S|\text{-}S} = \bm{\widehat \beta}_{S}.
\label{eq:orthogonality-consequence}
\end{equation}

\paragraph{Least squares estimates via orthogonalization.}

Let's now focus our attention on a single predictor $x_j$. If this predictor is orthogonal to the remaining predictors, then the result~\eqref{eq:orthogonality-consequence} states that $\widehat \beta_{j|\text{-}j}$ can be obtained from simply regressing $y$ on $x_j$. However, this is usually not the case. Usually, $\bm x_{*j}$ has a nonzero projection $\bm X_{*,\text{-}j}\bm{\widehat \gamma}$ onto $C(\bm X_{*,\text{-}j})$:
\begin{equation}
\bm x_{*j} = \bm X_{*,\text{-}j}\bm{\widehat \gamma} + \bm x^{\perp}_{*j},
\end{equation}
where $\bm x^{\perp}_{*j}$ is the residual from regressing $\bm x_{*j}$ onto $\bm X_{*,\text{-}j}$ and is therefore orthogonal to $C(\bm X_{*,\text{-}j})$. In other words, $\bm x^{\perp}_{*j}$ is the projection of $\bm x_{*j}$ onto the orthogonal complement of $C(\bm X_{*,\text{-}j})$. 

With this decomposition, let us change basis from $(\bm x_{*j}, \bm X_{*,\text{-}j})$ to $(\bm x^{\perp}_{*j}, \bm X_{*,\text{-}j})$ by the process explored in Homework 1 Question 1. Let us write
\begin{equation*}
\begin{split}
\bm y = \bm x_{*j} \beta_{j|\text{-}j} + \bm X_{*,\text{-}j}\bm \beta_{\text{-}j|j} + \bm \epsilon \ &\Longleftrightarrow \ \bm y = (\bm X_{*,\text{-}j}\bm{\widehat \gamma} + \bm x^{\perp}_{*j})\beta_{j|\text{-}j} + \bm X_{*,\text{-}j}\bm \beta_{\text{-}j|j} + \bm \epsilon\\
\ &\Longleftrightarrow \ \bm y = \bm x^{\perp}_{*j}\beta_{j|\text{-}j} + \bm X_{*,\text{-}j}\bm \beta'_{\text{-}j|j} + \bm \epsilon.
\end{split}
\end{equation*}
What this means is that $\widehat \beta_{j|\text{-}j}$, the least squares coefficient of $\bm x_{*j}$ in the regression of $\bm y$ on $(\bm x_{*j}, \bm X_{*,\text{-}j})$ is also the least squares coefficient of $\bm x^{\perp}_{*j}$ in the regression of $\bm y$ on $(\bm x^{\perp}_{*j}, \bm X_{*,\text{-}j})$. However, since $\bm x^{\perp}_{*j}$ is orthogonal to $\bm X_{*,\text{-}j}$ by construction, we can use the result~\eqref{eq:orthogonality} to conclude that 
\begin{equation*}
\widehat \beta_{j|\text{-}j} \text{ is the least squares coefficient of } \bm x^{\perp}_{*j} \text{ in the \textit{univariate} regression of } \bm y \text{ on } \bm x^{\perp}_{*j} \text{ (without intercept).} 
\end{equation*}
We can solve this univariate regression explicitly to obtain
\begin{equation}
\widehat \beta_{j|\text{-}j} = \frac{(\bm x^{\perp}_{*j})^T \bm y}{\|\bm x^{\perp}_{*j}\|^2}.
\label{eq:orthogonal-univariate}
\end{equation}

\paragraph{Adjustment and partial correlation.}

Equivalently, letting $\bm{\widehat \beta}_{\text{-}j}$ be the least squares estimate in the regression of $\bm y$ on $\bm X_{*,\text{-}j}$ (note that this is \textit{not} the same as $\bm{\widehat \beta}_{\text{-}j|j}$), we can write
\begin{equation}
\widehat \beta_{j|\text{-}j} = \frac{(\bm x^{\perp}_{*j})^T(\bm y - \bm X_{*,\text{-}j}\bm{\widehat \beta_{\text{-}j}})}{\|\bm x^{\perp}_{*j}\|^2} = \frac{(\bm x_{*j} - \bm X_{*,\text{-}j}\bm{\widehat \gamma})^T(\bm y - \bm X_{*,\text{-}j}\bm{\widehat \beta_{\text{-}j}})}{\|\bm x_{*j} -\bm X_{*,\text{-}j}\bm{\widehat \gamma}\|^2}.
\end{equation}
We can interpret this result as follows: The linear regression coefficient $\widehat \beta_{j|\text{-}j}$ results from first adjusting $\bm y$ and $\bm x_{*j}$ for the effects of all other variables, and then regressing the residuals from $\bm y$ onto the residuals from $\bm x_{*j}$. In this sense, \textit{the least squares coefficient for a predictor in a multiple linear regression reflects the effect of the predictor on the response after controlling for the effects of all other predictors.} A related quantity is the \textit{partial correlation} between $\bm x_{*j}$ and $\bm y$ after controlling for $\bm X_{*,\text{-}j}$, defined as the correlation between $\bm x_{*j} - \bm X_{*,\text{-}j}\bm{\widehat \gamma}$ and $\bm y - \bm X_{*,\text{-}j}\bm{\widehat \beta_{\text{-}j}}$. We can then connect the least squares coefficient $\widehat \beta_j$ to this partial correlation in a similar spirit to equation~\eqref{eq:coefficient-as-correlation}.

\paragraph{Aside: Average treatment effect estimation in causal inference.}

Suppose we'd like to study the effect of an exposure or treatment on a response $y$. Letting $y_1$ and $y_0$ denote the responses under treatment and control (Neyman-Rubin causal model), the most basic goal is to estimate the \textit{average treatment effect} $\tau \equiv \mathbb E[y_1 - y_0]$. Usually in observational studies we have \textit{confounding variables} $z_1, \dots, z_p$: variables that influence both the treatment assignment and the response. It is important to control for these confounders in order to get an unbiased estimate of the treatment effect. Suppose all confounders are measured (i.e. $(y_1, y_0) \perp\!\!\!\!\perp t \mid z_1, \dots, z_p$), the treatment effect is constant, and the response is a linear function of the outcome:
\begin{equation}
y = \beta t + \gamma_1 z_1 + \cdots + \gamma_p z_p + \epsilon.
\end{equation}
Then, the average treatment effect $\tau$ is identified as the coefficient $\beta$ in the above regression, i.e. $\tau = \beta$. Therefore, the least squares estimate $\widehat \beta_{t|z}$ is an unbiased estimate of the average treatment effect. (Causal inference is beyond the scope of STAT 961; see STAT 921 instead.)

\paragraph{Effects of collinearity.}

Collinearity between a predictor $x_j$ and the other predictors tends to make the estimate $\widehat \beta_{j|\text{-}j}$ unstable. Intuitively, this makes sense because it becomes harder to distinguish between the effects of predictor $x_j$ and those of the other predictors on the response. To find the variance of $\widehat \beta_{j|\text{-}j}$ for a model matrix $\bm X$, we could in principle use the formula~\eqref{eq:var-of-beta-hat}. However, this formula involves the inverse of the matrix $\bm X^T \bm X$, which is hard to reason about. Instead, we can employ the formula~\eqref{eq:orthogonal-univariate} to calculate directly that
\begin{equation}
\text{Var}[\widehat \beta_{j|\text{-}j}] = \frac{\sigma^2}{\|\bm x_{*j}^\perp\|^2}.
\label{eq:conditional-variance}
\end{equation}
We see that the variance of $\widehat \beta_{j|\text{-}j}$ is inversely proportional to $\|\bm x_{*j}^\perp\|^2$. This means that the greater the collinearity, the less of $\bm x_{*j}$ is left over after adjusting for $\bm X_{*,\text{-}j}$, and the greater the variance of $\widehat \beta_{j|\text{-}j}$. To quantify the effect of this adjustment, suppose there were no other predictors other than the intercept term. Then, we would have 
\begin{equation}
\text{Var}[\widehat \beta_j] = \frac{\sigma^2}{\|\bm x_{*j}-\bar x_j \bm 1_n\|^2}.
\end{equation}
Therefore, we can rewrite the variance~\eqref{eq:conditional-variance} as
\begin{equation}
\text{Var}[\widehat \beta_{j|\text{-}j}] = \frac{\|\bm x_{*j}-\bar x_j \bm 1_n\|^2}{\|\bm x_{*j}-\bm X_{*,\text{-}j}\bm{\widehat \gamma}\|^2} \cdot \text{Var}[\widehat \beta_j] = \frac{1}{1-R_j^2} \cdot \text{Var}[\widehat \beta_j] \equiv \text{VIF}_j \cdot \text{Var}[\widehat \beta_j],
\end{equation}
where $R_j^2$ is the $R^2$ value when regressing $\bm x_{*j}$ on $\bm X_{*,\text{-}j}$ and VIF stands for \textit{variance inflation factor}. The higher $R_j^2$, the more of the variance in $\bm x_{*j}$ is explained by other predictors, the higher the variance in $\widehat \beta_{j|\text{-}j}$.

\section{R demo}

TBD.


\end{document}