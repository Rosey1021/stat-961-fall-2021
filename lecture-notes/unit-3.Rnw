% (C) Brett Klamer - MIT - http://opensource.org/licenses/MIT
% Please contact me if you find any errors or make improvements
% Contact details at brettklamer.com

\documentclass[11pt,letterpaper,english,oneside]{article} % article class is a standard class
%==============================================================================
%Load Packages
%==============================================================================
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % easy page margins
\usepackage[utf8]{inputenc} % editor uses utf-8 encoding
\usepackage[T1]{fontenc} % T1 font pdf output
\usepackage{lmodern} % Latin modern roman font
\usepackage{bm, bbm} % bold and blackboard bold math symbols
\usepackage{amsmath, amsfonts, amssymb, amsthm} % math packages
\usepackage[final]{microtype} % better microtypography
\usepackage{graphicx} % for easier grahics handling
\usepackage[hidelinks, colorlinks=true, linkcolor = blue, urlcolor = blue]{hyperref} % to create hyperlinks
\usepackage{float} % tells floats to stay [H]ere!
\usepackage{mdframed} % it's better than framed. knitr uses framed so settings won't conflict
\usepackage{enumitem} % nice lists
\usepackage{fancyhdr} % nice headers
\usepackage{caption}  % to control figure and table captions
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{definition}[proposition]{Definition}



\captionsetup{width=0.9\textwidth, justification = raggedright}

%==============================================================================
% Enter name and homework title here
%==============================================================================
\title{Unit 3: Linear models: Misspecification}
\author{Eugene Katsevich}
% \date{August 31, 2021}

%==============================================================================
% Put title and author in PDF properties
%==============================================================================
\makeatletter % change interpretation of @
\hypersetup{pdftitle={\@title},pdfauthor={\@author}}


%==============================================================================
% Header settings
%==============================================================================
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @

%==============================================================================
% List spacing
%==============================================================================
\setlist[itemize]{parsep=0em} % fix itemize spacing
\setlist[enumerate]{parsep=0em} % fix enumerate spacing

%==============================================================================
% set knitr options
%==============================================================================
% latex (change space before and after knitr kframe; based on framed package)
\setlength{\OuterFrameSep}{0.3em}
% R
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)

# inline hook to process the output of \Sexpr{} statements to just 2 digits
inline_hook <- function(x) {
  if(is.numeric(x)) x <- round(x, 2)
  paste(as.character(x), collapse=", ")
}
knit_hooks$set(inline = inline_hook)

# cache all chunks
opts_chunk$set(cache = TRUE)

@

\begin{document}

\maketitle

In our discussion of linear model inference in Unit 2, we assumed the normal linear model throughout: 
\begin{equation}
\bm y = \bm X \bm \beta + \bm \epsilon, \quad \text{where} \ \bm \epsilon \sim N(\bm 0, \sigma^2 \bm I_n).
\end{equation}
In this unit, we will discuss what happens when this model is misspecified:
\begin{itemize}
\item Non-normality (Section~\ref{sec:non-normality}): $\bm \epsilon \sim (0, \sigma^2 \bm I_n)$ but not $N(0, \sigma^2 \bm I_n)$.
\item Heteroskedastic errors (Section~\ref{sec:heteroskedasticity}): $\epsilon_i \overset{\text{ind}}\sim N(0, \sigma^2_i)$, where it is not the case that $\sigma^2_1 = \cdots = \sigma^2_n$.
\item Correlated errors (Section~\ref{sec:correlated-errors}): It is not the case that $(\epsilon_1, \dots, \epsilon_n)$ are independent.
\item Model bias (Section~\ref{sec:model-bias}): It is not the case that $\mathbb E[\bm y] = \bm X \bm \beta$ for some $\bm \beta \in \mathbb R^p$.
\item Outliers (Section~\ref{sec:outliers}): For one or more $i$, it is not the case that $y_i \sim N(\bm x_{i*}^T \bm \beta, \sigma^2)$.
\end{itemize}
For each type of misspecification, we will discuss its origins, consequences, detection, and fixes (Sections~\ref{sec:non-normality}-\ref{sec:outliers}). We conclude with an R demo (Section~\ref{sec:R-demo-misspecification}).

\section{Non-normality} \label{sec:non-normality}

\subsection{Origin}

Non-normality occurs when the distribution of $y|\bm x$ is either skewed or has heavier tails than the normal distribution. This may happen, for example, if there is some discreteness in $y$.

\subsection{Consequences} \label{sec:nonnormality-consequences}

Non-normality is the most benign of linear model misspecifications. While we derived linear model inferences under the normality assumption, all the corresponding statements hold asymptotically without this assumption. Recall Homework 2 Question 1, or take for example the simpler problem of estimating the mean $\mu$ of a distribution based on $n$ samples from it: We can test $H_0: \mu = 0$ and build a confidence interval for $\mu$ even if the underlying distribution is not normal. So if $n$ is relatively large and $p$ is relatively small, you need not worry too much. If $n$ is small and the errors are highly skewed or heavy-tailed, there might be an issue. 

\subsection{Detection}

Non-normality is a property of the error-terms $\epsilon_i$. We do not observe these directly, but we can approximate these using the residuals
\begin{equation}
\widehat \epsilon_i = y_i - \bm x_{i*}^T \bm{\widehat \beta}.
\end{equation}
Recall from Unit 2 that $\text{Var}[\bm{\widehat \epsilon}] = \sigma^2(\bm I - \bm H)$. Letting $h_i$ be the $i$th diagonal entry of $\bm H$, it follows that $\widehat \epsilon_i \sim (0, \sigma^2(1-h_i))$. The \textit{standardized residuals} are defined as 
\begin{equation}
r_i = \frac{\widehat \epsilon_i}{\widehat \sigma \sqrt{1-h_i}}.
\label{eq:standardized-residuals}
\end{equation}
Under normality, we would expect $r_i \overset \cdot \sim N(0,1)$. We can therefore assess normality by producing a histogram or normal QQ-plot of these residuals (see Figure~\ref{fig:qqplot}).

\begin{figure}[h!]
\centering
\includegraphics[width = 0.9\textwidth]{figures/qqplot.png}
\caption{Histogram and normal QQ plot of standardized residuals.}
\label{fig:qqplot}
\end{figure}

\subsection{Fixes}

As mentioned in Section~\ref{sec:nonnormality-consequences}, non-normality is not necessarily a problem that needs to be fixed, except in small samples. In small samples, we can apply the bootstrap (Section~\ref{sec:bootstrap}) for robust standard error computation and a few different strategies (Section~\ref{sec:robust-tests}) for robust hypothesis testing.

\section{Heteroskedastic errors} \label{sec:heteroskedasticity}

\subsection{Origin}

Suppose each observation $y_i$ is actually the average of $n_i$ underlying observations, each with variance $\sigma^2$. Then, the variance of $y_i$ is $\sigma^2/n_i$, which will differ across $i$ if $n_i$ differ. It is also common to see the variance of a distribution increase as the mean increases (as in Figure~\ref{fig:heteroskedasticity}), whereas for a linear model the variance of $y$ stays constant as the mean of $y$ varies.

\subsection{Consequences}

All normal linear model inference from Unit 2 hinges on the assumption that $\epsilon_i \overset{\text{i.i.d.}} \sim N(0, \sigma^2)$. The coverage of confidence intervals and the levels of hypothesis tests may depart from their nominal levels. This is easiest to see if we consider the width of confidence intervals for $\bm x_0^T \bm \beta$; see Figure~\ref{fig:heteroskedasticity} for intuition.


\begin{figure}[h!]
\centering
\includegraphics[width = 0.6\textwidth]{figures/heteroskedasticity.png}
\caption{Heteroskedasticity in a simple bivariate linear model (\href{http://www3.wabash.edu/econometrics/EconometricsBook/chap19.htm}{image source}).}
\label{fig:heteroskedasticity}
\end{figure}

\subsection{Detection}

Heteroskedasticity is usually assessed via the \textit{residual plot} (Figure~\ref{fig:residual-plots}). In this plot, the standardized residuals $r_i$~\eqref{eq:standardized-residuals} are plotted against the fitted values $\widehat \mu_i$. In the absence of heteroskedasticity, the spread of the points around the origin should be roughly constant as a function of $\widehat \mu$ (Figure~\ref{fig:residual-plots}(a)). A common sign of heteroskedasticity is the fan shape where variance increases as a function of $\widehat \mu$ (Figure~\ref{fig:residual-plots}(c)).

\begin{figure}[h!]
\centering
\includegraphics[width = \textwidth]{figures/residual-plots.png}
\caption{Residuals plotted against linear-model fitted values that reflect (a) model ade- quacy, (b) quadratic rather than linear relationship, and (c) nonconstant variance(image source: Agresti Figure 2.8).}
\label{fig:residual-plots}
\end{figure}

\subsection{Fixes}

Heteroskedasticity-robust standard errors for hypothesis testing and confidence intervals can be obtained using a number of strategies, including the Huber-White sandwich estimator~\ref{sec:huber-white}, the bootstrap~\ref{sec:bootstrap}, and permutation tests~\ref{sec:permutation-tests}. 

\section{Correlated errors} \label{sec:correlated-errors}

\subsection{Origin} \label{sec:origin-correlated-errors}

Correlated errors can arise when observations have group, spatial, or temporal structure. Below are examples:
\begin{itemize}
\item Group/clustered structure: We have 10 samples $(\bm x_{i*}, y_i)$ each from 100 schools. 
\item Spatial structure: We have 100 soil samples from a 10$\times$10 grid on a 1km$\times$1km field. 
\item Temporal structure: We have 366 COVID positivity rate measurements, one from each day of the year 2020.
\end{itemize}
The issue arises because there are common sources of variation among sample that are in the same group or spatially/temporally close to one another. 

\subsection{Consequences}

Like with heteroskedastic errors, correlated errors can cause invalid standard errors. In particular, positively correlated errors typically cause standard errors to be smaller than they should be, leading to inflated Type-I error rates. For intuition, consider estimating the mean of a distribution based on $n$ samples. Consider the cases when these samples are independent, compared to when they are perfectly correlated. The effective sample size in the former case is $n$ and in the latter case is 1.

\subsection{Detection}

Residual plots once again come in handy to detect correlated errors. Instead of plotting the standardized residuals against the fitted values, we should plot the residuals against whatever variables we think might explain variation in the response that the regression does not account for. In the presence of group structures, we can plot residuals versus group (via a boxplot); in the presence of spatial or temporal structure, we can plot residuals as a function of space or time. If the residuals show a dependency on these variables, this suggests they are correlated.

\subsection{Fixes}

There are a few approaches to addressing correlated errors:
\begin{enumerate}
\item Estimate the covariance matrix $\bm \Sigma$ of the observations, so that $\bm y \sim N(\bm X \bm \beta, \Sigma)$. This is a \textit{generalized least squares} problem for which inference can be carried out. The generalized least squares estimate is $\bm{\widehat \beta} = (\bm X^T \bm \Sigma^{-1}\bm X)^{-1}\bm X^T \bm \Sigma^{-1}\bm y$, which is distributed as $\bm{\widehat \beta} \sim N(\bm \beta, (\bm X^T \bm \Sigma^{-1}\bm X)^{-1})$. We can carry out inference based on the latter distributional result analogously to how we did so in Unit 2. A special case of this is the \textit{linear mixed effects model}, which hopefully we will have time to discuss in Unit 6.
\item Use the Liang-Zeger variance estimator; see Section~\ref{sec:huber-white}.
\item Apply a clustered or block bootstrap; see Section~\ref{sec:bootstrap}.
\end{enumerate}

\section{Model bias} \label{sec:model-bias}

\subsection{Origin}

Model bias arises when predictors are left out of the regression model:
\begin{equation}
\text{assumed model: } \bm y = \bm X \bm \beta + \bm \epsilon; \quad \text{actual model: } \bm y = \bm X \bm \beta + \bm Z \bm \gamma + \bm \epsilon.
\label{eq:confounding}
\end{equation}
We may not always know about or measure all the variables that impact a response $\bm y$.

Model bias can also arise when the predictors do not impact the response on the linear scale. For example:
\begin{equation}
\text{assumed model: } \mathbb E[\bm y] = \bm X \bm \beta; \quad \text{actual model: } g(\mathbb E[\bm y]) = \bm X \bm \beta.
\label{eq:wrong-scale}
\end{equation}

\subsection{Consequences}

In cases of model bias, the parameters $\bm \beta$ in the assumed linear model lose their meanings. The least squares estimate $\bm{\widehat \beta}$ will be a biased estimate for the parameter we probably actually want to estimate. In the case~\eqref{eq:confounding} when predictors are left out of the regression model, these additional predictors $\bm Z$ will act as confounders and create bias in $\bm{\widehat \beta}$ as an estimate of the $\bm \beta$ parameters in the true model, unless $\bm X^T \bm Z = 0$. As discussed in Unit 2, this can lead to misleading conclusions.

\subsection{Detection}

Similarly to the detection of correlated errors, we can try to identify model bias by plotting the standardized residuals against predictors that may have been left out of the model. A good place to start is to plot standardized residuals against the predictors $\bm X$ (one at a time) that are in the model, since nonlinear transformations of these might have been left out. In this case, you would see something like Figure~\ref{fig:residual-plots}(b).

It is possible to formally test for model bias in cases when we have repeated observations of the response for each value of the predictor vector. In particular, suppose that $\bm x_{i*} = \bm x_c$ for $c = c(i)$ and predictor vectors $\bm x_1, \dots, \bm x_C \in \mathbb R^p$. Then, consider testing the following hypothesis:
\begin{equation}
H_0: y_i = \bm x_{i*}^T \bm \beta + \epsilon_i \quad \text{versus} \quad H_1: y_i = \beta_{c(i)} + \epsilon_i.
\end{equation}
The model under $H_0$ (the linear model) is nested in the model for $H_1$ (the saturated model), and we can test this hypothesis using an $F$-test called the \textit{lack of fit $F$-test}. 


\subsection{Fixes}

To fix model bias in the case~\eqref{eq:confounding}, ideally we would identify the missing predictors $\bm Z$ and add them to the regression model. This may not always be feasible or possible. To fix model bias in the case~\eqref{eq:wrong-scale}, it is sometimes advocated to find a transformation $g$ (e.g. a square root or a logarithm) of $\bm y$ such that $\mathbb E[g(\bm y)] = \bm X \bm{\beta}$. However, a better solution is to use a \textit{generalized linear model}, which we will discuss starting in Unit 4. 

\section{Outliers} \label{sec:outliers}

\subsection{Origin}

Outliers often arise due to measurement or data entry errors. An observation can be an outlier in $\bm x$, in $y$, or both.

\subsection{Consequences}

An outlier can have the effect of biasing the estimate $\bm{\widehat \beta}$. This occurs when an observation has outlying $\bm x$ as well as outlying $y$. 

\subsection{Detection}

There are a few measures associated to an observation that can be used to detect outliers, though none are perfect. The first quantity is called the \textit{leverage}, defined as 
\begin{equation}
\text{leverage of observation } i \equiv \text{corr}(y_i, \widehat \mu_i)^2.
\end{equation}
This quantity measures the extent to which the fitted value $\widehat \mu_i$ is sensitive to the (noise in the) observation $y_i$. It can be derived that
\begin{equation}
\text{leverage of observation } i = h_{ii},
\end{equation}
which is the $i$th diagonal element of the hat matrix $\bm H$. This is related to the fact that $\text{Var}[\widehat \epsilon_i] = \sigma^2(1-h_{ii})$. The larger the leverage, the smaller the variance of the residual, so the closer the line passes to the $i$th observation. The leverage of an observation is larger to the extent that $\bm x_{i*}$ is far from $\bm{\bar x}$. For example, in the bivariate linear model $y_i = \beta_0 + \beta_1 x_i + \epsilon_i$, 
\begin{equation*}
h_{ii} = \frac{1}{n} + \frac{(x_i - \bar x)^2}{\sum_{i' = 1}^n (x_{i'} - \bar x)^2}.
\end{equation*}

Note that the leverage is not a function of $y_i$, so a high-leverage point might or might not be an outlier in $y_i$ and therefore might or might not have a strong impact on the regression. To assess more directly whether an observation is \textit{influential}, we can compare the least squares fits with and without that observation. To this end, we define the \textit{Cook's distance}
\begin{equation}
D_i = \frac{\sum_{i' = 1}^n (\widehat \mu_{i'} - \widehat \mu^{\text{-}i}_{i'})^2}{p\widehat \sigma^2},
\end{equation}
where $\widehat \mu^{\text{-}i}_{i'} = \bm x_{i*}^T \bm{\widehat{\beta}}^{\text{-}i}$ and $\bm{\widehat{\beta}}^{\text{-}i}$ is the least squares estimate based on $(\bm X_{\text{-}i,*}, \bm y_{\text{-}i})$. An observation is considered influential if it has Cooks distance greater than one.

There is a connection between Cook's distance and leverage:
\begin{equation}
D_i = \left(\frac{y_i - \widehat \mu_i}{\widehat \sigma \sqrt{1-h_{ii}}}\right)^2 \cdot \frac{h_{ii}}{p(1-h_{ii})}.
\end{equation}
We recognize the first term as the standardized residual; therefore a point is influential if its residual and leverage are large.

Note that Cook's distance may not successfully identify outliers. For example, if there are groups of outliers, then they will \textit{mask} each other in the calculation of Cook's distance.

\subsection{Fixes}

If outliers can be detected, then the fix is to remove them from the regression. But, we need to be careful. Definitively determining whether observations are outliers can be tricky. Outlier detection can even be used as a way to commit fraud with data, as now-defunct blood testing start-up \href{https://arstechnica.com/tech-policy/2021/09/cherry-picking-data-was-routine-practice-at-theranos-former-lab-worker-says/}{Theranos is alleged to have done}. 

As an alternative to removing outliers, we can fit estimators $\bm{\widehat \beta}$ that are less sensitive to outliers; see Section~\ref{sec:robust-estimation}.

\section{Robust inference}

There are a number of strategies designed to address one or more of the misspecification issues listed above. These fall into the categories of robust estimation (to get better estimates of $\bm{\widehat \beta}$ in the presence of outliers; see Section~\ref{sec:robust-estimation}), robust standard error computation (to get more reliable standard errors in the presence of heteroskedasticity or correlated errors; see Section~\ref{sec:robust-standard-errors}), and robust hypothesis testing (to get more reliable hypothesis tests in the presence of heteroskedasticity, correlated errors, and sometimes even model bias; see Section~\ref{sec:robust-tests}).

\subsection{Robust estimation} \label{sec:robust-estimation}

The squared error loss $\sum_{i = 1}^n (y_i - \bm x_{i*}^T \bm \beta)^2$ is sensitive to outliers in the sense that a large value of $y_i - \bm x_{i*}^T \bm \beta$ can have a significant impact on the loss function. The least squares estimate, as the minimizer of this loss function, is therefore sensitive to outliers. One way of addressing this challenge is to replace the squared error loss by a different loss that does not grow so quickly in $y_i - \bm x_{i*}^T \bm \beta$. A popular choice for such a loss function is the Huber loss:
\begin{equation}
L_\delta(y_i - \bm x_{i*}^T \bm \beta) = 
\begin{cases}
\frac{1}{2}(y_i - \bm x_{i*}^T \bm \beta)^2, \quad &\text{if } |y_i - \bm x_{i*}^T \bm \beta| \leq \delta; \\
\delta(|y_i - \bm x_{i*}^T \bm \beta|-\delta), \quad &\text{if } |y_i - \bm x_{i*}^T \bm \beta| > \delta.
\end{cases}
\end{equation}
This function is differentiable, like the squared error loss, but grows linearly as opposed to quadratically. We can then define
\begin{equation*}
\bm{\widehat{\beta}}^{\text{Huber}} \equiv \underset{\bm \beta}{\arg \min}\ \sum_{i = 1}^n L_\delta(y_i - \bm x_{i*}^T \bm \beta).
\end{equation*}
This is an \textit{M-estimator}; it is consistent and has an asymptotic normal distribution that can be used for inference.

\subsection{Robust standard error computation} \label{sec:robust-standard-errors}

When the error terms in a regression are not homoskedastic and independent, the usual standard errors are invalid. There are several strategies to computing valid standard errors in such situations.

\subsubsection{Huber-White and Liang-Zeger sandwich estimators} \label{sec:huber-white}

Let's say that $\bm y = \bm X \bm \beta + \bm \epsilon$, where $\bm \epsilon \sim N(\bm 0, \bm \Sigma)$. Then, we can compute that the covariance matrix of the least squares estimate $\bm{\widehat \beta}$ is 
\begin{equation}
\text{Var}[\bm{\widehat \beta}] = (\bm X^T \bm X)^{-1}(\bm X^T \bm \Sigma \bm X)(\bm X^T \bm X)^{-1}.
\label{eq:sandwich}
\end{equation}
Note that this expression reduces to the usual $\sigma^2(\bm X^T \bm X)^{-1}$ when $\bm \Sigma = \sigma^2 \bm I$. It is called the sandwich variance between we have the $(\bm X^T \bm \Sigma \bm X)$ term sandwiched between two $(\bm X^T \bm X)^{-1}$ terms. If we have some estimate $\bm{\widehat \Sigma}$ of the covariance matrix, we can construct
\begin{equation}
\widehat{\text{Var}}[\bm{\widehat \beta}] \equiv (\bm X^T \bm X)^{-1}(\bm X^T \bm{\widehat\Sigma} \bm X)(\bm X^T \bm X)^{-1}.
\end{equation}
Different estimates $\bm{\widehat \Sigma}$ are appropriate in different situation. Below we consider two of the most common choices: one for heteroskedasticity (due to Huber-White) and one for correlated errors (due to Liang-Zeger).

\paragraph{Huber-White standard errors.}

Now, suppose $\bm \Sigma = \text{diag}(\sigma_1^2, \dots, \sigma_n^2)$ for some variances $\sigma_1^2, \dots, \sigma_n^2 > 0$. The Huber-White sandwich estimator is defined by~\eqref{eq:sandwich}, with
\begin{equation}
\bm{\widehat\Sigma} \equiv \text{diag}(\widehat \sigma_1^2, \dots, \widehat \sigma_n^2), \quad \text{where} \quad \widehat \sigma_i^2 = (y_i - \bm x_{i*}^T \bm{\widehat \beta})^2.
\end{equation}
While each estimator $\widehat \sigma_i^2$ is very poor, Huber and White's insight was that the resulting estimate of the (averaged) quantity $\bm X^T \bm{\widehat \Sigma}\bm X$ is not bad.

\paragraph{Liang-Zeger standard errors.}

Next, let's consider the case of correlated errors. Specifically, suppose that the observations are \textit{clustered}, with correlated errors among clusters but not between clusters (recall Section~\ref{sec:origin-correlated-errors}). Suppose there are $C$ clusters of observations, with the $i$th observation belonging to cluster $c(i) \in \{1, \dots, C\}$. Suppose for the sake of simplicity that the observations are ordered so that clusters are contiguous. Let $\bm{\widehat \epsilon}_c$ be the vector of residuals in cluster $c$, so that $\bm{\widehat \epsilon} = (\bm{\widehat \epsilon}_1, \dots, \bm{\widehat \epsilon}_C)$. Then, the true covariance matrix is $\bm \Sigma = \text{block-diag}(\bm \Sigma_1, \dots, \bm \Sigma_C)$ for some positive definite $\bm \Sigma_1, \dots, \bm \Sigma_C$. The Liang-Zeger estimator is then defined by~\eqref{eq:sandwich}, with
\begin{equation}
\bm{\widehat\Sigma} \equiv \text{block-diag}(\bm{\widehat\Sigma_1}, \dots, \bm{\widehat \Sigma_C}), \quad \text{where} \quad  \bm{\widehat\Sigma_c} \equiv \bm{\widehat \epsilon}_c \bm{\widehat \epsilon}_c^T.
\end{equation}
Note that the Liang-Zeger estimator is a generalization of the Huber-White estimator. Its justification is similar as well: while each $\bm{\widehat\Sigma_c}$ is a poor estimator, the resulting estimate of the (averaged) quantity $\bm X^T \bm{\widehat \Sigma}\bm X$ is not bad as long as the number of clusters is large. Liang-Zeger standard errors are sometimes referred to as ``clustered standard errors.''

\subsubsection{Bootstrap} \label{sec:bootstrap}

A completely different approach to constructing robust standard errors is the \textit{bootstrap}. The core idea of the bootstrap is to use the data to construct an approximation to the data-generating distribution, and then to approximate the sampling distribution of any test statistic by simulating from this approximate data-generating distribution. This approach, pioneered by Brad Efron in 1979, replaces mathematical derivations with computation. The bootstrap is extremely flexible, and can be adapted to apply in a variety of settings.

\paragraph{Parametric bootstrap.}

The parametric bootstrap proceeds by fitting a parametric model, and then by resampling from this model. In the linear regression case, we use the original data to fit $(\bm{\widehat \beta}, \widehat \sigma^2)$. Then, we sample new response vectors 
\begin{equation}
y^b_i = \bm x_{i*}^T\bm{\widehat \beta} + \epsilon_i^b, \quad \epsilon_i^b \overset{\text{i.i.d.}}\sim N(0, \widehat \sigma^2) \quad \text{for } b = 1, \dots, B.
\end{equation}
We then fit a least squares coefficient vector $\bm{\widehat \beta}^b$ to $(\bm X, \bm y^b)$ for each $b$, and then get variance estimates by treating $\{\bm{\widehat \beta}^b\}_{b = 1}^B$ as though it were the sampling distribution of $\bm{\widehat \beta}$. For example, we could use the sample standard deviation of $\widehat \beta_j^b$ as the standard error for $\beta_j$. 

This the most model-based of the bootstrap variants. It assumes a completely well-specified model, and gives equivalent results to traditional parametric inference. It is typically not applied in regression settings, and presented here mainly for pedagogical purposes.

\paragraph{Residual bootstrap.}

We can weaken the assumptions of the parametric bootstrap by assuming only that $y_i = \bm x_{i*}^T \bm \beta + \epsilon_i$, where $\epsilon_i \overset{\text{i.i.d.}}\sim F$ for some distribution $F$. Then, the data-generating distribution is specified by $(\bm \beta, F)$, which we approximate by substituting $\bm{\widehat \beta}$ for $\bm \beta$ and the empirical distribution of the residuals $\widehat \epsilon_i$ (call it $\widehat F$) for $F$. We can then sample new response vectors based on this approximate data-generating distribution:
\begin{equation}
y_i = \bm x_{i*}^T \bm{\widehat \beta} + \epsilon_i^b, \quad \epsilon_i^b \overset{\text{i.i.d.}}\sim \widehat F \quad \text{for } b = 1, \dots, B.
\end{equation}
Note that i.i.d. sampling $\epsilon_i^b$ from $\widehat F$ amounts to sampling $(\epsilon_1^b, \dots, \epsilon_n^b)$ with replacement from $(\widehat \epsilon_1, \dots, \widehat \epsilon_n)$. Then, as with the parametric bootstrap, we fit a least squares coefficient vector $\bm{\widehat \beta}^b$ to $(\bm X, \bm y^b)$ for each $b$ and obtain standard errors by treating $\{\bm{\widehat \beta}^b\}_{b = 1}^B$ as though it were the sampling distribution of $\bm{\widehat \beta}$.

The residual bootstrap corrects for non-normality, but not heteroskedasticity or correlated errors, since it assumes that the noise terms are i.i.d. from some distribution.

\paragraph{Pairs bootstrap.}

Weakening the assumptions further, let's assume only that $(\bm x_{i*}, y_i) \overset{\text{i.i.d.}} \sim F$ for some joint distribution $F$. We then resample our observations by sampling with replacement from the original observations. 

Note that, unlike the parametric or residual bootstrap, the pairs bootstrap treats the predictors $\bm X$ as random rather than fixed. The benefit of the pairs bootstrap is that it does not assume homoskedasticity, since the error variance is allowed to depend on $\bm x_{i*}$. Therefore, the pairs bootstrap addresses both non-normality and heteroskedasticity, though it does not address correlated errors (though variants of the pairs bootstrap do; see below). Note that the pairs bootstrap does not even assume that $\mathbb E[y_i] = \bm x_{i*}^T \bm \beta$ for some $\bm \beta$. However, in the presence of model bias, it is unclear for what parameters we are even doing inference. While the pairs bootstrap assumes less than the residual bootstrap, it may be somewhat less efficient in the case when the assumptions of the latter are met.

The pairs bootstrap has several variants that help it overcome correlated errors, in addition to heteroskedasticity. The \textit{cluster bootstrap} is applicable in the case when errors have a clustered/grouped structure. In this case, we sample entire clusters of observations, with replacement, from the original set of clusters. The \textit{moving blocks bootstrap} is applicable in the case of spatially or temporally structured errors. In this variant of the pairs bootstrap, we resample spatially or temporally adjacent blocks of observations together to preserve their joint correlation structure.



\subsection{Robust hypothesis testing} \label{sec:robust-tests}

\subsubsection{Permutation tests} \label{sec:permutation-tests}

\subsubsection{Rank-based tests} \label{sec:rank-based-tests}

\subsubsection{Bootstrap-based tests} \label{sec:bootstrap-tests}

\section{R demo} \label{sec:R-demo-misspecification}

\end{document}
