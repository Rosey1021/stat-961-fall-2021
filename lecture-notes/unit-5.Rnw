% (C) Brett Klamer - MIT - http://opensource.org/licenses/MIT
% Please contact me if you find any errors or make improvements
% Contact details at brettklamer.com

\documentclass[11pt,letterpaper,english,oneside]{article} % article class is a standard class
%==============================================================================
%Load Packages
%==============================================================================
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % easy page margins
\usepackage[utf8]{inputenc} % editor uses utf-8 encoding
\usepackage[T1]{fontenc} % T1 font pdf output
\usepackage{lmodern} % Latin modern roman font
\usepackage{bm, bbm} % bold and blackboard bold math symbols
\usepackage{amsmath, amsfonts, amssymb, amsthm} % math packages
\usepackage[final]{microtype} % better microtypography
\usepackage{graphicx} % for easier grahics handling
\usepackage[hidelinks, colorlinks=true, linkcolor = blue, urlcolor = blue]{hyperref} % to create hyperlinks
\usepackage{float} % tells floats to stay [H]ere!
\usepackage{mdframed} % it's better than framed. knitr uses framed so settings won't conflict
\usepackage{enumitem} % nice lists
\usepackage{fancyhdr} % nice headers
\usepackage{caption}  % to control figure and table captions
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{definition}[proposition]{Definition}



\captionsetup{width=0.9\textwidth, justification = raggedright}

%==============================================================================
% Enter name and homework title here
%==============================================================================
\title{Unit 5: Generalized linear models: Special cases}
\author{Eugene Katsevich}
% \date{August 31, 2021}

%==============================================================================
% Put title and author in PDF properties
%==============================================================================
\makeatletter % change interpretation of @
\hypersetup{pdftitle={\@title},pdfauthor={\@author}}


%==============================================================================
% Header settings
%==============================================================================
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @

%==============================================================================
% List spacing
%==============================================================================
\setlist[itemize]{parsep=0em} % fix itemize spacing
\setlist[enumerate]{parsep=0em} % fix enumerate spacing

%==============================================================================
% set knitr options
%==============================================================================
% latex (change space before and after knitr kframe; based on framed package)
\setlength{\OuterFrameSep}{0.3em}
% R
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)

# inline hook to process the output of \Sexpr{} statements to just 2 digits
inline_hook <- function(x) {
  if(is.numeric(x)) x <- round(x, 2)
  paste(as.character(x), collapse=", ")
}
knit_hooks$set(inline = inline_hook)

# cache all chunks
opts_chunk$set(cache = TRUE)

@

\begin{document}

\maketitle

Unit 4 developed a general theory for GLMs. In Unit 5, we specialize this theory to several important cases, including logistic regression and Poisson regression.

\section{Logistic regression} \label{sec:logistic-regression}

\subsection{Model definition and interpretation}

\paragraph{Model definition.} Recall from Unit 4 that the logistic regression model is
\begin{equation}
m_i y_i \overset{\text{ind}}\sim \text{Bin}(m_i, \pi_i); \quad \text{logit}(\pi_i) = \log\frac{\pi_i}{1-\pi_i} = \bm x^T_{i*}\bm \beta.
\end{equation}
Here we use the canonical logit link function, although other link functions are possible. The interpretation of the parameter $\beta_j$ is that a unit increase in $x_j$---other predictors held constant---is associated with an (additive) increase of $\beta_j$ on the log-odds scale or a multiplicative increase of $e^{\beta_j}$ on the odds scale. Note that logistic regression data come in two formats: \textit{ungrouped} and \textit{grouped}. For ungrouped data, we have $m_1 = \dots = m_n = 1$, so $y_i \in \{0,1\}$ are Bernoulli random variables. For grouped data, we can have several independent Bernoulli observations per predictor $\bm x_{i*}$, which give rise to binomial proportions $y_i \in [0,1]$. This happens most often when all the predictors are discrete. You can always convert grouped data into ungrouped data, but not necessarily vice versa. We'll discuss below that the grouped and ungrouped formulations of logistic regression have the same MLE and standard errors but different deviances.

\paragraph{Generative model equivalent.} Consider the following generative model for $(\bm x, y) \in \mathbb R^{p-1} \times \{0,1\}$:
\begin{equation}
y \sim \text{Ber}(\pi); \quad \bm x|y \sim \begin{cases}N(\bm \mu_0, \bm V) \quad \text{if } y = 0 \\ N(\bm \mu_1, \bm V) \quad \text{if } y = 1\end{cases}.
\end{equation}
Then, we can derive that $y|\bm x$ follows a logistic regression model (called a \textit{discriminative} model because it conditions on $\bm x$). Indeed,
\begin{equation}
\begin{split}
\text{logit}(p(y = 1|\bm x)) &= \log\frac{p(y = 1)p(\bm x|y = 1)}{p(y = 0)p(\bm x|y = 0)} \\
&= \log\frac{\pi \exp\left(-\frac12(\bm x - \bm \mu_1)^T\bm V^{-1}(\bm x - \bm \mu_1)\right)}{(1-\pi) \exp\left(-\frac12(\bm x - \bm \mu_0)^T\bm V^{-1}(\bm x - \bm \mu_0)\right)} \\
&= \beta_0 + \bm x^T \bm V^{-1}(\bm \mu_1 - \bm \mu_0) \\
&\equiv \beta_0 + \bm x^T \bm \beta_{\text{-}0}.
\end{split}
\end{equation}
This is another natural route to motivating the logistic regression model.

\paragraph{Special case: $2 \times 2$ contingency table.}

Suppose that $x \in \{0,1\}$, and consider the logistic regression model $\text{logit}(\pi_i) = \beta_0 + \beta_1 x_i$. For example, suppose that $x \in \{0,1\}$ encodes treatment (1) and control (0) in a clinical trial, and $y_i \in \{0,1\}$ encodes success (1) and failure (0). We make $n$ observations of $(x_i, y_i)$ in this ungrouped setup. The parameter $e^{\beta_1}$ can be interpreted as the \textit{odds ratio}:
\begin{equation}
e^{\beta_1} = \frac{\mathbb P[y = 1|x=1]/\mathbb P[y = 0|x=1]}{\mathbb P[y = 1|x=0]/\mathbb P[y = 0|x=0]}.
\end{equation}
This parameter is the multiple by which the odds of success increase when going from control to treatment. We can summarize such data via the $2 \times 2$ \textit{contingency table} (Table~\ref{tab:2-by-2-table}). A grouped version of this data would be $\{(x_1, y_1) = (0, 7/24), (x_2, y_2) = (1, 9/21)\}$. The null hypothesis $H_0: \beta_1 = 0 \Longleftrightarrow H_0: e^{\beta_1} = 1$ states that the success probability in both rows of the table is the same.
\begin{table}[h!]
\centering
\begin{tabular}{c|cc|c}
 & Success & Failure & Total \\
 \hline
 Treatment & 9 & 12 & 21 \\
 Control & 7 & 17 & 24 \\
 \hline
 Total & 16 & 29 & 45
\end{tabular}
\caption{An example of a $2 \times 2$ contingency table.}
\label{tab:2-by-2-table}
\end{table}

\paragraph{Logistic regression with case-control studies.}

In a prospective study (e.g. a clinical trial), we assign treatment or control (i.e., $x$) to individuals, and then observe a binary outcome (i.e., $y$). Sometimes, the outcome $y$ takes a long time to measure or has highly imbalanced distribution in the population (e.g. the development of lung cancer). In this case, an appealing study design is the \textit{retrospective study}, where individuals are sampled based on their \textit{response values} (e.g. presence of lung cancer) rather than their treatment/exposure status (e.g. smoking). It turns out that a logistic regression model is appropriate for such retrospective study designs as well. Indeed, suppose that $y|\bm x$ follows a logistic regression model. Let's try to figure out the distribution of $y|\bm x$ in the retrospectively gathered sample. Letting $z \in \{0,1\}$ denote the indicator that an observation is sampled, define $\rho_1 \equiv \mathbb P[z = 1|y = 1]$ and $\rho_0 \equiv \mathbb P[z = 1|y = 0]$, and assume that $\mathbb P[z = 1, y, \bm x] = \mathbb P[z = 1 | y]$. The latter assumption states that the predictors $\bm x$ were not used in the retrospective sampling process. Then,
\begin{equation*}
\text{logit}(\mathbb P[y = 1|z = 1, \bm x]) = \log \frac{\rho_1 \mathbb P[y = 1|\bm x]}{\rho_0 \mathbb P[y = 0|\bm x]} = \log \frac{\rho_1}{\rho_0} + \text{logit}(\mathbb P[y = 1|\bm x]) = \left(\log \frac{\rho_1}{\rho_0} + \beta_0\right) + \bm x^T \bm \beta_{\text{-}0}.
\end{equation*}
Thus, conditioning on retrospective sampling changes only the intercept term, but preserves the coefficients of $\bm x$. Therefore, we can carry out inference for $\bm \beta_{\text{-}0}$ in the same way regardless of whether the study design is prospective or retrospective.

\subsection{Estimation and inference}

\paragraph{Score and Fisher information.}

We recall from Unit 4 that the score is
\begin{equation}
\frac{\partial}{\partial \bm \beta}\log \mathcal L(\bm \beta) = \bm X^T\bm D \bm V^{-1}(\bm y - \bm \mu) = \bm X^T \text{diag}\left(\frac{\partial \mu_i/\partial \eta_i}{\text{Var}[y_i]}\right)(\bm y - \bm \mu).
\end{equation}
Note that 
\begin{equation}
\frac{\partial \mu_i/\partial \eta_i}{\text{Var}[y_i]} = \frac{\partial \mu_i/\partial \theta_i}{\text{Var}[y_i]} = \frac{\ddot \psi(\theta_i)}{\text{Var}[y_i]} = m_i.
\end{equation}
Therefore, the score equations are
\begin{equation}
0 = \bm X^T \text{diag}\left(m_i\right)(\bm y - \bm{\widehat\mu}) \quad \Longleftrightarrow \quad \sum_{i = 1}^n m_i x_{ij}(y_i-\widehat \pi_i) = 0, \quad j = 0, \dots, p-1.
\end{equation}
We can solve these equations using IRLS. The Fisher information is
\begin{equation}
\bm I(\bm \beta) = \bm X^T \bm W \bm X, \quad W_{ii} = \frac{(\partial \mu_i/\partial \eta_i)^2}{\text{Var}[y_i]} = \frac{\ddot \psi(\theta_i)^2}{\text{Var}[y_i]} = m_i^2 \text{Var}[y_i] = m_i \pi_i(1-\pi_i). 
\end{equation}

\paragraph{Wald inference.}
Using the results in the previous paragraph, we can carry out Wald inference based on the normal approximation
\begin{equation}
\bm{\widehat \beta} \overset \cdot \sim N\left(\bm \beta, \left(\bm X^T\text{diag}(m_i \widehat \pi_i(1-\widehat \pi_i))\bm X\right)^{-1}\right).
\end{equation}
This approximation holds for $\sum_{i = 1}^n m_i \rightarrow \infty$. Unfortunately, Wald inference in finite samples does not always perform very well. The Wald test above is known to be conservative due to the \textit{Hauck-Donner effect}. As an example, consider testing $H_0: \beta_0 = 0.5$ in the intercept-only model 
\begin{equation}
ny \sim \text{Bin}(n, \pi); \quad \text{logit}(\pi) = \beta_0.  
\end{equation}
The Wald test statistic is $z \equiv \widehat \beta/\text{SE} = \text{logit}(y)\sqrt{ny(1-y)}$. This test statistic actually tends to \textit{decrease} as $y \rightarrow 1$, since the standard error grows faster than the estimate itself. For example, take $n = 25$. Then, $z = 3.3$ for $n = 23/25$ but $z = 3.1$ for $n = 24/25$. So the test statistic becomes less significant as we go further away from the null!

\paragraph{Perfect separability.}

If we have a situation where a hyperplane in covariate space separates observations with $y_i = 0$ from those with $y_i = 1$, we have \textit{perfect separability}. It turns out that some of the maximum likelihood estimates are infinite in this case. The Wald test completely fails in this case, since it uses the parameter estimates as test statistics. 

\paragraph{Likelihood ratio inference.}

Let's first compute the deviance of a logistic regression model. We have
\begin{equation}
L(\bm y; \bm \pi) = \sum_{i = 1}^n m_i y_i \log \pi_i + m_i(1-y_i) \log(1-\pi_i),
\end{equation}
so
\begin{equation}
D(\bm y; \bm{\widehat\pi}) = 2(L(\bm y; \bm y) - L(\bm y; \bm{\widehat \pi})) = 2\sum_{i = 1}^n \left(m_i y_i \log \frac{y_i}{\widehat \pi_i} + m_i(1-y_i) \log\frac{1-y_i}{1-\widehat \pi_i}\right).
\label{eq:logistic-deviance}
\end{equation}
Letting $\bm{\widehat \pi}_0$ and $\bm{\widehat \pi}_1$ be the MLEs from two nested models, we can then express the likelihood ratio statistic as 
\begin{equation}
T^{\text{LRT}} = 2(L(\bm y; \bm{\widehat \pi}_1) - L(\bm y; \bm{\widehat \pi}_0)) = 2\sum_{i = 1}^n \left(m_i y_i \log \frac{\widehat \pi_{i1}}{\widehat \pi_{i0}} + m_i(1-y_i) \log\frac{1-\widehat \pi_{i1}}{1-\widehat \pi_{i0}}\right).
\end{equation}
We can then construct a likelihood ratio test in the usual way. Likelihood ratio inference can give nontrivial conclusions in cases when Wald inference cannot, e.g. in the case of perfect separability. Indeed, suppose that 
\begin{equation}
m_i y_i \sim \text{Bin}(m_i, \pi_i), \quad \text{logit}(\pi_i) = \beta_0 + \beta_1 x_i, \quad i = 1,2.
\end{equation}
We would like to test $H_0: \beta_1 = 0$. Suppose that we observe $(x_1, y_1) = (0, 0)$, $(x_2, y_2) = (1, 1)$, giving us complete separability. Can we still get a meaningful test of $H_0$? We can write out the likelihood ratio test statistic, which is 
\begin{equation*}
D(\bm y; \bm{\widehat\pi}) = 2\left(m_1 \log\frac{1}{1-\frac{m_2}{m_1 + m_2}} + m_2 \log \frac{1}{\frac{m_2}{m_1 + m_2}}\right) = 2\left(m_1 \log\frac{m_1 + m_2}{m_1} + m_2 \log \frac{m_1 + m_2}{m_2}\right).
\end{equation*}
This is a number that we can compare to the $\chi^2_1$ distribution to get a $p$-value, as usual.

\paragraph{Goodness of fit tests.}

We can test goodness of fit in the grouped logistic regression model by comparing the deviance statistic~\eqref{eq:logistic-deviance} to the asymptotic null distribution $\chi^2_{n-p}$. We can alternatively use the score test, which gives us Pearson's $X^2$ statistic:
\begin{equation}
X^2 = \sum_{i = 1}^n \frac{(y_i - \widehat \pi_i)^2}{\widehat \pi_i(1-\widehat \pi_i)/m_i}.
\end{equation}

\paragraph{Fisher's exact test.}

As an alternative to asymptotic tests for logistic regression, in the case of $2 \times 2$ tables there is an \textit{exact} test of $H_0: \beta_1 = 0$. Suppose we have 
\begin{equation}
s_1 = m_1y_1 \sim \text{Bin}(m_1, \pi_1) \quad and \quad s_2 = m_2y_2 \sim \text{Bin}(m_2, \pi_2).
\end{equation}
The trick is to conduct inference \textit{conditional on} $s_1 + s_2$. Note that under $H_0: \pi_1 = \pi_2$, we have
\begin{equation}
\begin{split}
\mathbb P[s_1 = t | s_1+s_2 = v] &= \mathbb P[s_1 = t | s_1 + s_2 = v] \\
&= \frac{\mathbb P[s_1 = t, s_2 = v-t]}{\mathbb P[s_1 + s_2 = v]} \\
&= \frac{{m_1 \choose t}\pi^{t}(1-\pi)^{m_1 - t}{m_2 \choose v-t}\pi^{v-t}(1-\pi)^{m_2 - (v-t)}}{{m_1 + m_2 \choose v}\pi^v (1-\pi)^{m_1 + m_2 - v}} \\
&= \frac{{m_1 \choose t}{m_2 \choose v-t}}{{m_1 + m_2 \choose v}}.
\end{split}
\end{equation}
Therefore, a finite-sample $p$-value to test $H_0: \pi_1 = \pi_2$ versus $H_1: \pi_1 > \pi_2$ is $\mathbb P[s_1 \geq t | s_1 + s_2]$, which can be computed exactly based on the formula above.

\section{Poisson regression} \label{sec:poisson-regression}

The Poisson regression model (with offsets) is
\begin{equation}
y_i \overset{\text{ind}}\sim \text{Poi}(\mu_i); \quad \log \mu_i = o_i + \bm x_{i*}^T \bm \beta.
\label{eq:poisson-with-offsets}
\end{equation}
Because the log of the mean is linear in the predictors, Poisson regression models are often called \textit{loglinear models}. We have seen in Unit 4 how to carry out inference for this model based on the Wald, likelihood ratio, and score tests. Recall, for example, that the deviance of this model is
\begin{equation}
D(\bm y; \bm{\widehat \mu}) = \sum_{i = 1}^n y_i \log\frac{y_i}{\widehat \mu_i}.
\end{equation}

\subsection{Modeling rates}

One cool feature of the Poisson model is that rates can be easily modeled with the help of offsets. Let's say that the count $y_i$ is collected over the course of a time interval of length $t_i$, or a spatial region with area $t_i$, or a population of size $t_i$, etc. Then, it is meaningful to model
\begin{equation}
y_i \overset{\text{ind}} \sim \text{Poi}(\pi_i t_i), \quad \log \pi_i = \bm x^T_{i*}\bm \beta,
\end{equation}
where $\pi_i$ represents the rate of events per day / per square mile / per capita, etc. In other words,
\begin{equation}
y_i \overset{\text{ind}} \sim \text{Poi}(\mu_i), \quad \log \mu_i = \log t_i + \bm x^T_{i*}\bm \beta,
\end{equation}
which is exactly equation~\eqref{eq:poisson-with-offsets} with offsets $o_i = \log t_i$. For example, in single cell RNA-sequencing, $y_i$ is the number of reads aligning to a gene in cell $i$ and $t_i$ is the total number of reads measured in the cell, a quantity called the \textit{sequencing depth}. We might use a Poisson regression model to carry out a \textit{differential expression analysis} between two cell types.

\subsection{Relationship between Poisson and multinomial distributions}

Suppose that $y_i \overset{\text{ind}}\sim \text{Poi}(\mu_i)$ for $i = 1, \dots, n$. Then,
\begin{equation}
\begin{split}
\mathbb P\left[y_1 = m_1, \dots, y_n = m_n \left| \sum_{i}y_i = m\right.\right] &= \frac{\mathbb P[y_1 = m_1, \dots, y_n = m_n]}{\mathbb P[\sum_{i}y_i = m]} \\
&= \frac{\prod_{i = 1}^n e^{-\mu_i}\frac{\mu_i^{y_i}}{y_i!}}{e^{-\sum_i \mu_i}\frac{(\sum_i \mu_i)^m}{m!}} \\
&= {m \choose m_1, \dots, m_n}\prod_{i = 1}^n \pi_i^{y_i}; \quad \pi_i \equiv \frac{\mu_i}{\sum_{i' = 1}^n \mu_{i'}}.
\end{split}
\end{equation}
We recognize the last expression as the probability mass function of the multinomial distribution with parameters $(\pi_1, \dots, \pi_n)$ summing to one. In words, the joint distribution of a set of independent Poisson distributions conditional on their sum is a multinomial distribution.

\subsection{Poisson model for $2 \times 2$ contingency tables}

Let's say that we have two binary random variables $x_1, x_2 \in \{0,1\}$ with joint distribution $\mathbb P(x_1 = j, x_2 = k) = \pi_{jk}$ for $j,k \in \{0,1\}$. We collect a total of $n$ samples from this joint distribution and summarize the counts in a $2 \times 2$ table, where $y_{jk}$ is the number of times we observed $(x_1, x_2) = (j,k)$, so that  
\begin{equation}
(y_{00}, y_{01}, y_{10}, y_{11})|n \sim \text{Mult}(n, (\pi_{00}, \pi_{01}, \pi_{10}, \pi_{11})). 
\end{equation}
Our primary question is whether these two random variables are independent, i.e. 
\begin{equation}
\pi_{jk} = \pi_{j+}\pi_{+k}, \quad \text{where} \quad \pi_{j+} \equiv \mathbb P[x_1 = j] = \pi_{j1} + \pi_{j2}; \quad \pi_{+k} \equiv \mathbb P[x_2 = k] = \pi_{1k} + \pi_{2k}.
\label{eq:null-product-formulation}
\end{equation}
We can express this equivalently as
\begin{equation}
\pi_{00}(\pi_{00} + \pi_{01} + \pi_{10} + \pi_{11}) = \pi_{00} = \pi_{0+}\pi_{+0} = (\pi_{00} + \pi_{01})(\pi_{00} + \pi_{10}) \quad \Longleftrightarrow \quad \pi_{00}\pi_{11} = \pi_{01}\pi_{10}.
\end{equation}
In other words, we can express the independence hypothesis concisely as
\begin{equation}
H_0: \frac{\pi_{11}\pi_{00}}{\pi_{10}\pi_{01}} = 1.
\label{eq:independence}
\end{equation}
Let's arbitrarily assume that, additionally, $n \sim \text{Poi}(\mu_{++})$. Then, 
\begin{equation}
(y_{00}, y_{01}, y_{10}, y_{11}) \sim \text{Poi}(\mu_{++}\pi_{00}) \times \text{Poi}(\mu_{++} \pi_{01}) \times \text{Poi}(\mu_{++}\pi_{10}) \times \text{Poi}(\mu_{++}\pi_{11}). 
\end{equation}
Let $i \in 1,2,3,4$ index the four pairs $(x_1, x_2) \in \{(0,0), (0,1), (1,0), (1,1)\}$, so that
\begin{equation}
y_i \overset{\text{ind}}\sim \text{Poi}(\mu_i); \quad \log \mu_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \beta_{12}x_{i1} x_{i2}, \quad i = 1, \dots, 4,
\label{eq:2-by-2-Poisson-reg}
\end{equation}
where
\begin{equation}
\beta_0 = \log \mu_{++} + \log \pi_{00}; \quad \beta_1 = \log \frac{\pi_{10}}{\pi_{00}}; \quad \beta_2 = \log \frac{\pi_{01}}{\pi_{00}}; \quad \beta_{12} = \log\frac{\pi_{11}\pi_{00}}{\pi_{10}\pi_{01}}.
\end{equation}
Note that the independence hypothesis~\eqref{eq:independence} reduces to the hypothesis $H_0: \beta_{12} = 0$:
\begin{equation}
H_0: \frac{\pi_{11}\pi_{00}}{\pi_{10}\pi_{01}} = 1 \quad \Longleftrightarrow \quad H_0: \beta_{12} = 0.
\end{equation}
So the presence of an interaction in the Poisson regression is equivalent to a lack of independence between $x_1$ and $x_2$. We can test the latter hypothesis using our standard tools for Poisson regression. For example, we can use the Pearson $X^2$ goodness of fit test. To apply this test, we must find the fitted means under the null hypothesis. The normal equations state that the observed cell counts equal those that would have been expected under the null hypothesis. Using the formulation~\eqref{eq:null-product-formulation}, we obtain 
\begin{equation}
y_{jk} = \mathbb E[y_{jk}] = \widehat \mu_{++} \widehat \pi_{j+}\widehat \pi_{+k},
\end{equation}
so that
\begin{equation}
\widehat \mu = y_{++}; \quad \widehat \mu_{++} \widehat \pi_{j+} = y_{j+}; \quad \widehat \mu_{++} \widehat \pi_{+k} = y_{+k},
\end{equation}
from which it follows that
\begin{equation}
\widehat \mu_{jk} = \widehat \mu_{++} \widehat \pi_{j+}\widehat \pi_{+k} = y_{++}\frac{y_{j+}}{y_{++}}\frac{y_{+k}}{y_{++}} = \frac{y_{j+}y_{+k}}{y_{++}}. 
\end{equation}
Hence, we have
\begin{equation}
X^2 = \sum_{j,k = 0}^1 \frac{(y_{jk} - \widehat \mu_{jk})^2}{\widehat \mu_{jk}}.
\end{equation}
Alternatively, we can use the likelihood ratio test, which gives
\begin{equation}
G^2 = \sum_{j,k = 0}^1 y_{jk}\log\frac{y_{jk}}{\widehat \mu_{jk}}.
\end{equation}

\subsection{Inference is the same regardless of conditioning on margins}

Now, our data might actually have been collected such that $n \sim \text{Poi}(\mu)$, or maybe $n$ was fixed in advance. Is the Poisson inference proposed above actually valid in the latter case? In fact, it is! To see this, we claim that the likelihood ratio statistic is the same for the Poisson and multinomial models. Indeed, let's write the Poisson likelihood as follows:
\begin{equation}
p_{\bm \mu}(\bm y) = p_{\mu_{++}}(y_{++} = n)p_{\bm \pi}(\bm y|y_{++} = n).
\end{equation}
Note that the fitted parameter $\widehat \mu_{++}$ is the same under the null and alternative hypotheses: $\widehat \mu^0_{++} = \widehat \mu^{1}_{++}$, so we have
\begin{equation}
\frac{p_{\bm{\widehat\mu}^1}(\bm y)}{p_{\bm{\widehat\mu}^0}(\bm y)} = \frac{p_{\widehat\mu^1_{++}}(y_{++} = n)p_{\bm{\widehat\pi}^1}(\bm y|y_{++} = n)}{p_{\widehat\mu^0_{++}}(y_{++} = n)p_{\bm{\widehat\pi}^0}(\bm y|y_{++} = n)} = \frac{p_{\bm{\widehat\pi}^1}(\bm y|y_{++} = n)}{p_{\bm{\widehat\pi}^0}(\bm y|y_{++} = n)}.
\end{equation}
The latter expression is the likelihood ratio statistic for the multinomial model. The same argument shows that conditioning on the row or column totals (as opposed to the overall total) also yields the same exact inference. Therefore, regardless of the sampling mechanism, we can always conduct an independence test in a $2 \times 2$ table via a Poisson regression.

\subsection{Equivalence among Poisson and logistic regressions}

We've talked above two ways to view a $2 \times 2$ contingency table. In the logistic regression view, we thought about one variable as a predictor and the other as a response, seeking to test whether the predictor has an impact on the response. In the Poisson regression view, we thought about the two variables symmetrically, seeking to test independence. It turns out that these two perspectives are equivalent. Note that under the Poisson model, we have
\begin{equation}
\text{logit }\mathbb P[x_2 = 1|x_1 = 0] = \log \frac{\pi_{01}}{\pi_{00}} = \beta_2
\end{equation}
and
\begin{equation}
\text{logit }\mathbb P[x_2 = 1|x_1 = 1] = \log \frac{\pi_{11}}{\pi_{10}} = \log \frac{\pi_{01}}{\pi_{00}} + \log\frac{\pi_{11}\pi_{00}}{\pi_{10}\pi_{01}} = \beta_2 + \beta_{12}. 
\end{equation}
In other words,
\begin{equation}
\text{logit }\mathbb P[x_2 = 1|x_1] = \beta_2 + \beta_{12}x_1.
\label{eq:2-by-2-logistic-reg}
\end{equation}
Therefore, the $\beta_{12}$ parameter for the Poisson regression~\eqref{eq:2-by-2-Poisson-reg} is the same as it is for the logistic regression~\eqref{eq:2-by-2-logistic-reg}.

\subsection{Poisson models for $J \times K$ contingency tables}

Suppose now that $x_1 \in \{1, \dots, J\}$ and $x_2 \in \{1, \dots, K\}$. Then, we denote $\mathbb P[x_1 = j, x_2 = k] = \pi_{jk}$. We still are interested in testing for independence between $j$ and $k$, which amounts to a goodness-of-fit test for the Poisson model
\begin{equation}
y_{jk} \overset{\text{ind}}\sim\text{Poi}(\mu_{jk}); \quad \log \mu_{jk} = \beta_0 + \beta^1_j + \beta^2_k.
\end{equation}
The Pearson statistic for this test is
\begin{equation}
\sum_{j = 1}^J \sum_{k = 1}^K \frac{(y_{ij} - \widehat \mu_{ij})^2}{\widehat \mu_{ij}}; \quad \widehat \mu_{ij} = \widehat y_{++}\frac{y_{i+}}{y_{++}}\frac{y_{+j}}{y_{++}}.
\end{equation}
Like with the $2 \times 2$ case, the test is the same regardless if we condition on the row sums, column sums, total count, or if we do not condition at all. The degrees of freedom in the full model is $JK$, while the degrees of freedom in the partial model is $J+K-1$, so the degrees of freedom for the goodness-of-fit test is $JK - J - K + 1 = (J-1)(K-1)$. Pearson erroneously concluded that the test had $JK-1$ degrees of freedom, which when Fisher corrected created a lot of animosity between these two statisticians.

\subsection{Poisson models for $J \times K \times L$ contingency tables}

These ideas can be extended to multi-way tables, for example three-way tables. If we have $x_1 \in \{1, \dots, J\}, x_2 \in \{1, \dots, K\}, x_3 \in \{1, \dots, L\}$, then we might be interested in testing several kinds of null hypotheses:
\begin{itemize}
\item Mutual independence: $H_0: x_1 \perp \!\!\! \perp x_2 \perp \!\!\! \perp x_3$. 
\item Joint independence: $H_0: x_1 \perp \!\!\! \perp (x_2, x_3)$.
\item Conditional independence: $H_0: x_1 \perp \!\!\! \perp x_2 \mid x_3$.
\end{itemize}
These three null hypotheses can be shown to be equivalent to the Poisson regression model
\begin{equation}
y_{jkl} \overset{\text{ind}}\sim \text{Poi}(\mu_{jkl}),
\end{equation}
where
\begin{equation}
\log \mu_{ijk} = \beta_0 + \beta^1_{j} + \beta^2_k + \beta^3_l \quad \text{(mutual independence)};
\end{equation}
\begin{equation}
\log \mu_{ijk} = \beta_0 + \beta^1_{j} + \beta^2_k + \beta^3_l + \beta^{2,3}_{kl} \quad \text{(joint independence)};
\end{equation}
\begin{equation}
\log \mu_{ijk} = \beta_0 + \beta^1_{j} + \beta^2_k + \beta^3_l + \beta^{1,2}_{jk} + \beta^{1,3}_{jl} \quad \text{(mutual independence)}.
\end{equation}

\section{Negative binomial regression} \label{sec:nb-regression}

\paragraph{Overdispersion.} A pervasive issue with Poisson regression is \textit{overdispersion}: that the variances of observations are greater than the corresponding means. A common cause of overdispersion is omitted variable bias. Suppose that $y \sim \text{Poi}(\mu)$, where $\log \mu = \beta_0 + \beta_1 x_1 + \beta_2 x_2$. However, we omitted variable $x_2$ and are considering the GLM based on $\log \mu = \beta_0 + \beta_1 x_1$. If $\beta_2 \neq 0$ and $x_2$ is correlated with $x_1$, then we have a confounding issue. Let's consider the more benign situation that $x_2$ is independent of $x_1$. Then, we have
\begin{equation}
\mathbb E[y|x_1] = \mathbb E[\mathbb E[y|x_1, x_2]|x_1] = \mathbb E[e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2}|x_1] = e^{\beta_0 + \beta_1 x_1}\mathbb E[e^{\beta_2 x_2}] = e^{\beta'_0 + \beta_1 x_1}.
\end{equation}
So in the model for the mean of $y$, the impact of omitted variable $x_2$ seems only to have impacted the intercept. Let's consider the variance of $y$:
\begin{equation}
\text{Var}[y|x_1] = \mathbb E[\text{Var}[y|x_1, x_2]|x_1] + \text{Var}[\mathbb E[y|x_1, x_2]|x_1] = e^{\beta'_0 + \beta_1 x_1} + e^{2(\beta'_0 + \beta_1 x_1)}\text{Var}[e^{\beta_2 x_2}] > e^{\beta'_0 + \beta_1 x_1} = \mathbb E[y|x_1].
\end{equation}
So indeed, the variance is larger than what we would have expected under the Poisson model.

\paragraph{Hierarchical Poisson regression.} Let's say that $y|\bm x \sim \text{Poi}(\lambda)$, where $\lambda|\bm x$ is random due to the fluctuations of the omitted variables. A common distribution used to model nonnegative random variables is the \textit{gamma} distribution $\Gamma(\mu, k)$, parameterized by a mean $\mu > 0$ and a \textit{shape} $k > 0$. This distribution has probability density function
\begin{equation}
f(\lambda; k, \mu) = \frac{(k/\mu)^k}{\Gamma(k)}e^{-k\lambda/\mu}\lambda^{k-1},
\end{equation}
with mean and variance given by
\begin{equation}
\mathbb E[\lambda] = \mu; \quad \text{Var}[\lambda] = \mu^2/k.
\end{equation}
Therefore, it makes sense to augment the Poisson regression model as follows:
\begin{equation}
\lambda|\bm x \sim \Gamma(\mu, k), \quad \log \mu = \bm x^T \bm \beta, \quad y | \lambda \sim \text{Poi}(\lambda).
\label{eq:nb-hierarchical}
\end{equation}

\paragraph{Negative binomial distribution.} 

A simpler way to write the hierarchical model~\eqref{eq:nb-hierarchical} would be to marginalize out $\lambda$. Doing so leaves us with a count distribution called the \textit{negative binomial distribution}:
\begin{equation}
\lambda \sim \Gamma(\mu, k),\  y | \lambda \sim \text{Poi}(\lambda) \quad \Longrightarrow \quad y \sim \text{NegBin}(\mu, k).
\end{equation}
The negative binomial probability mass function is
\begin{equation}
p(y; \mu, k) = \int_0^\infty \frac{(k/\mu)^k}{\Gamma(k)}e^{-k\lambda/\mu}\lambda^{k-1}e^{-\lambda}\frac{\lambda^y}{y!}d\lambda = \frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)}\left(\frac{\mu}{\mu + k}\right)^{y}\left(\frac{k}{\mu + k}\right)^{k}.
\end{equation}
This random variable has mean and variance given by
\begin{equation}
\mathbb E[y] = \mathbb E[\lambda] = \mu \quad \text{and} \quad \text{Var}[y] = \mathbb E[\lambda] + \text{Var}[\lambda] = \mu + \frac{\mu^2}{k}.
\end{equation}

\paragraph{Negative binomial as exponential dispersion model.} 

If we treat $k$ as known, then the negative binomial distribution is in the exponential family:
\begin{equation}
p(y; \mu, k) = \exp\left(y \log \frac{\mu}{\mu + k} - k \log \frac{\mu + k}{k}\right)\frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)}.
\end{equation}
We can read off that
\begin{equation}
\theta = \log \frac{\mu}{\mu + k}, \quad \psi(\theta) = k\log \frac{\mu + k}{k} = -k\log(1-e^{\theta}).
\label{eq:neg-bin-exp-fam}
\end{equation}
This is a regular exponential family model, and not an exponential dispersion model. Given the extra parameter $k$ controlling the variance, we may have been expecting to see an EDM. We can arrive at the EDM form by putting $1/k$ in the denominator:
\begin{equation}
p(y; \mu, k) = \exp\left(\frac{\frac{y}{k} \log \frac{\mu}{\mu + k} - \log \frac{\mu + k}{k}}{1/k}\right)\frac{\Gamma(y+k)}{\Gamma(k)\Gamma(y+1)}.
\end{equation}
Note that the ``normalized'' variable $y/k$ has the EDM distribution rather than the count variable $y$; this parallels our modeling of the binomial \textit{proportion} (rather than the binomial count) as an EDM. We then see that $y/k$ has the dispersion parameter $\phi = 1/k$. An alternate parameterization of the negative binomial model is via $\gamma = \phi = 1/k$. Here, $\gamma$ is called the negative binomial \textit{dispersion}.

\paragraph{Negative binomial regression.}

Let's revisit the hierarchical model~\eqref{eq:nb-hierarchical}, writing it more succinctly in terms of the negative binomial distribution:
\begin{equation}
y_i \overset{\text{ind}}\sim \text{NegBin}(\mu_i, \gamma), \quad \log \mu_i = \bm x^T \bm \beta.
\end{equation}
Notice that we typically assume that all observations share the same dispersion parameter $\gamma$. Reading off from equation~\eqref{eq:neg-bin-exp-fam}, we see that the canonical link function for the negative binomial distribution is $\mu \mapsto \log \frac{\mu}{\mu + k}$. However, typically for negative binomial regression we use the log link $g(\mu) = \log \mu$ instead. This is our first example of a non-canonical link! 

\paragraph{Estimation in negative binomial regression.}

Negative binomial regression is an EDM when $\gamma$ is known, but typically the dispersion parameter is unknown. Note that there is a dependency in $\psi$ on $k$ (i.e. on $\gamma$), which complicates things. It means that the estimate $\bm{\widehat \beta}$ depends on the parameter $\gamma$ (this does not happen, for example, in the normal linear model case).\footnote{Having said that, the dependency between $\bm{\widehat \beta}$ and $\widehat \gamma$ is weak, as the two are asymptotically independent parameters.} Therefore, estimation in negative binomial regression is typically an iterative procedure, where at each step $\bm \beta$ is estimated for the current value of $\gamma$ and then $\gamma$ is estimated based on the updated value of $\bm \beta$. Let's discuss each of these tasks in turn. Given a value of $\gamma$, we have the normal equations
\begin{equation}
0 = \bm X^T \text{diag}\left(\frac{\partial \mu_i/\partial \eta_i}{\text{Var}[y_i]}\right)(\bm y - \bm \mu) = \bm X^T \text{diag}\left(\frac{\mu_i}{\mu_i + \gamma \mu_i^2}\right)(\bm y - \bm \mu) = \bm X^T \text{diag}\left(\frac{1}{1 + \gamma \mu_i}\right)(\bm y - \bm \mu).
\end{equation}
This reduces to the Poisson normal equations when $\gamma = 0$. Solving these equations for a fixed value of $\gamma$ can be done via IRLS, as usual. Estimating $\gamma$ for a fixed value of $\bm \beta$ can be done in several ways, including setting to zero the derivative of the likelihood with respect to $\gamma$. This results in a nonlinear equation (not given here) that must be solved iteratively.

\paragraph{Wald inference.}

Note that
\begin{equation}
\bm W_{ii} = \frac{(\partial \mu_i/\partial \eta_i)^2}{\text{Var}[y_i]} = \frac{\mu_i^2}{\mu_i + \gamma \mu_i^2} = \frac{\mu_i}{1 + \gamma \mu_i}.
\end{equation}
Hence, Wald inference is based on
\begin{equation}
\widehat{\text{Var}}[\bm{\widehat \beta}] = (\bm X^T \bm{\widehat W} \bm X)^{-1}, \quad \text{where} \quad \bm{\widehat W} = \text{diag}\left(\frac{\widehat \mu_i}{1 + \widehat\gamma \widehat\mu_i}\right).
\end{equation}

\paragraph{Likelihood ratio test inference.}

The negative binomial deviance is
\begin{equation}
D(\bm y; \bm{\widehat \mu}) = 2\sum_{i = 1}^n \left(y_i \log \frac{y_i}{\widehat \mu_i} - \left(y_i + \frac{1}{\widehat \gamma}\right)\log \frac{1 + \widehat \gamma y_i}{1 + \widehat \gamma \widehat \mu_i}\right). 
\end{equation}
We can use this for comparing nested models and for goodness of fit testing, as usual.

\paragraph{Testing for overdispersion.}

It is reasonable to want to test for overdispersion, i.e. to test the null hypothesis $H_0: \gamma = 0$. This is somewhat of a tricky task, because $\gamma = 0$ is at the edge of the parameter space. There are formal tests of this hypothesis, but they are beyond the scope of this course. Another approach is to simply fit a negative binomial model and get a confidence interval for $\gamma$. It is probably not particularly reliable for small values of $\gamma$, but if it is far away from zero then likely we have some overdispersion on our hands. Finally, if goodness of fit tests in the Poisson model are significant, this may be an indication of overdispersion. It may also be an indication of omitted variable bias (e.g. you forgot to include an interaction), so it's somewhat tricky.

\paragraph{Overdispersion in logistic regression.}

Note that overdispersion is potentially an issue not only in Poisson regression models, but in logistic regression models as well. Dealing with overdispersion in the latter case is more tricky, because the analog of the negative binomial model (the beta-binomial model) is not an exponential family. An alternate route to dealing with overdispersion is quasi-likelihood modeling, but this topic is beyond the scope of the course.

\section{R demo} \label{sec:r-demo}

<<message = FALSE>>=
library(tidyverse)
@
\noindent Here we are again, face to face with the crime data, with one last chance to get the analysis right. Let's load and preprocess it, as before.
<<message = FALSE>>=
# read crime data
crime_data = read_tsv("../data/Statewide_crime.dat")

# read and transform population data
population_data = read_csv("../data/state-populations.csv")
population_data = population_data %>%
  filter(State != "Puerto Rico") %>% 
  select(State, Pop) %>%
  rename(state_name = State, state_pop = Pop)

# collate state abbreviations
state_abbreviations = tibble(state_name = state.name, 
                             state_abbrev = state.abb) %>%
  add_row(state_name = "District of Columbia", state_abbrev = "DC")

# add CrimeRate to crime_data
crime_data = crime_data %>%
  mutate(STATE = ifelse(STATE == "IO", "IA", STATE)) %>%
  rename(state_abbrev = STATE) %>%
  filter(state_abbrev != "DC") %>%       # remove outlier
  left_join(state_abbreviations, by = "state_abbrev") %>%
  left_join(population_data, by = "state_name") %>%
  select(state_abbrev, Violent, Metro, HighSchool, Poverty, state_pop)

crime_data
@
\noindent Let's recall the logistic regression we ran on these data in Unit 4:
<<>>=
bin_fit = glm(Violent/state_pop ~ Metro + HighSchool + Poverty,
              weights = state_pop,
              family = "binomial",
              data = crime_data)
@
\noindent Recall that everything was significant:
<<>>=
summary(bin_fit)
@
\noindent But there were already signs of trouble in this regression summary. The summary tells us that the residual deviance is 11742 on 46  degrees of freedom. This is a measure of the goodness of fit of the model, as the residual deviance should have a chi-square distribution with 46 degrees of freedom if the model fits well. But this distribution has a mean of 46, so having a value of 11742 seems way too large. We can confirm this suspicion with a formal deviance-based goodness of fit test:
<<>>=
pchisq(bin_fit$deviance, 
       df = bin_fit$df.residual, 
       lower.tail = FALSE)
@
\noindent Wow, we get a $p$-value of zero! Let's try doing a score-based (i.e. Pearson) goodness of fit test:
<<>>=
pchisq(sum(resid(bin_fit, "pearson")^2), 
       df = bin_fit$df.residual, 
       lower.tail = FALSE)
@
\noindent Here the code \verb|sum(resid(bin_fit, "pearson")^2)| extracts the sum of the squares of the Pearson residuals, which we did not discuss, but which gives us Pearson's $X^2$ statistic. So in this case, we again get a $p$-value of zero! So this model definitely does not fit well. We have therefore omitted some important variables and/or we have serious overdispersion on our hands. 

\noindent We haven't discussed in any detail how to deal with overdispersion in logistic regression models, so let's try a Poisson model instead. The natural way to model rates using Poisson distributions is via offsets:
<<>>=
pois_fit = glm(Violent ~ Metro + HighSchool + Poverty + offset(log(state_pop)),
               family = "poisson",
               data = crime_data)
summary(pois_fit)
@
\noindent Again, everything is significant, and again, the regression summary shows that we have a huge residual deviance. This was to be expected, given that $\text{Bin}(m, \pi) \approx \text{Poi}(m\pi)$ for large $m$ and small $\pi$. So, the natural thing to try is a negative binomial regression! Negative binomial regression is not implemented in the regular \verb|glm| package, but \verb|glm.nb()| from the \verb|MASS| package is a dedicated function for this task. Let's see what we get:
<<>>=
nb_fit = MASS::glm.nb(Violent ~ Metro + HighSchool + Poverty + offset(log(state_pop)),
                data = crime_data)
summary(nb_fit)
@
\noindent Aha! Things are not looking so significant anymore! And the residual deviance is not as huge! The estimated value of $\gamma$ (confusingly called $\theta$ in the summary) is significantly different from zero, indicating overdispersion. Now it appears that this model fits. Finally! Let's do a deviance-based goodness of fit test to make sure:
<<>>=
pchisq(nb_fit$deviance, 
       df = nb_fit$df.residual, 
       lower.tail = FALSE)
@
\noindent Ok, great. Now that we have a well-fitting model, we can do inference within this model that we can trust. For example, we can get Wald confidence intervals:
<<>>=
confint.default(nb_fit)
@
\noindent Or we can get LRT-based (i.e. profile) confidence intervals:
<<>>=
confint(nb_fit)
@
\noindent Or we can get confidence intervals for the predicted means:
<<>>=
predict(nb_fit,
        newdata = crime_data %>% column_to_rownames(var = "state_abbrev"),
        type = "response",
        se.fit = TRUE)
@
\noindent We can carry out some hypothesis tests as well, e.g. to test $H_0: \beta_{\text{Metro}} = 0$:
<<>>=
nb_fit_partial = MASS::glm.nb(Violent ~ HighSchool + Poverty + offset(log(state_pop)),
                     data = crime_data)
anova_fit = anova(nb_fit_partial, nb_fit)
anova_fit
@

\end{document}
