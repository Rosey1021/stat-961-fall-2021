% (C) Brett Klamer - MIT - http://opensource.org/licenses/MIT
% Please contact me if you find any errors or make improvements
% Contact details at brettklamer.com

\documentclass[11pt,letterpaper,english,oneside]{article} % article class is a standard class
%==============================================================================
%Load Packages
%==============================================================================
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry} % easy page margins
\usepackage[utf8]{inputenc} % editor uses utf-8 encoding
\usepackage[T1]{fontenc} % T1 font pdf output
\usepackage{lmodern} % Latin modern roman font
\usepackage{bm, bbm} % bold and blackboard bold math symbols
\usepackage{amsmath, amsfonts, amssymb, amsthm} % math packages
\usepackage[final]{microtype} % better microtypography
\usepackage{graphicx} % for easier grahics handling
\usepackage[hidelinks, colorlinks=true, linkcolor = blue, urlcolor = blue]{hyperref} % to create hyperlinks
\usepackage{float} % tells floats to stay [H]ere!
\usepackage{mdframed} % it's better than framed. knitr uses framed so settings won't conflict
\usepackage{enumitem} % nice lists
\usepackage{fancyhdr} % nice headers
\usepackage{caption}  % to control figure and table captions
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}[proposition]{Lemma}
\newtheorem{definition}[proposition]{Definition}



\captionsetup{width=0.9\textwidth, justification = raggedright}

%==============================================================================
% Enter name and homework title here
%==============================================================================
\title{Unit 4: Generalized linear models: General theory}
\author{Eugene Katsevich}
% \date{August 31, 2021}

%==============================================================================
% Put title and author in PDF properties
%==============================================================================
\makeatletter % change interpretation of @
\hypersetup{pdftitle={\@title},pdfauthor={\@author}}


%==============================================================================
% Header settings
%==============================================================================
\pagestyle{fancy} % turns on fancy header styles
\fancyhf{} % clear all header and footer fields
\makeatletter
\lhead{\@author} % left header
\chead{\@title} % center header
\makeatother
\rhead{Page \thepage} % right header
\setlength{\headheight}{13.6pt} % fixes minor warning
\makeatother % change back interpretation of @

%==============================================================================
% List spacing
%==============================================================================
\setlist[itemize]{parsep=0em} % fix itemize spacing
\setlist[enumerate]{parsep=0em} % fix enumerate spacing

%==============================================================================
% set knitr options
%==============================================================================
% latex (change space before and after knitr kframe; based on framed package)
\setlength{\OuterFrameSep}{0.3em}
% R
<<setup, include=FALSE, cache=FALSE>>=
library(knitr)

# inline hook to process the output of \Sexpr{} statements to just 2 digits
inline_hook <- function(x) {
  if(is.numeric(x)) x <- round(x, 2)
  paste(as.character(x), collapse=", ")
}
knit_hooks$set(inline = inline_hook)

# cache all chunks
opts_chunk$set(cache = TRUE)

@

\begin{document}

\maketitle

Units 1-3 focused on the most common class of models used in applications: linear models. Despite their versatility, linear models do not apply in all situations. In particular, they are not designed to deal with binary or count responses. In Unit 4, we introduced \textit{generalized linear models} (GLMs), a generalization of linear models that encompasses a wide variety of incredibly useful models including logistic regression and Poisson regression. 

We'll start Unit 4 by introducing exponential family models (Section~\ref{sec:exp-fam}), a generalization of the Gaussian distribution that serves as the backbone of GLMs. Then we formally define a GLM, demonstrating logistic regression and Poisson regression as special cases (Section~\ref{sec:glm-def}). Next we discuss maximum likelihood inference in GLMs (Section~\ref{sec:glm-max-lik}). Finally, we discuss how to carry out statistical inference in GLMs (Section~\ref{sec:glm-inf}).

\section{Exponential family distributions} \label{sec:exp-fam}

\paragraph{Definition and examples.} Let's start with the Gaussian distribution, taking variance $\sigma^2 = 1$ for simplicity. If $y \sim N(\mu, 1)$, then it has density
\begin{equation}
f(y) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}(y-\mu)^2\right) = \exp\left(\mu y - \frac{1}{2}\mu^2\right) \cdot \frac{1}{\sqrt{2\pi}}\exp\left(-\frac12 y^2\right).
\end{equation}
Here is a way of generalizing this density:
\begin{equation}
f_\eta(y) = \exp(\eta y - \psi(\eta))h(y).
\end{equation}
Here $\eta$ is called the \textit{natural parameter}, $\psi$ is called the \textit{log-partition function}, and $h$ is called the \textit{base measure}. The distribution with density $f_\eta$ is called a \textit{one-parameter natural exponential family}. Therefore, $y \sim N(\mu, 1)$ is in the exponential family with
\begin{equation}
\eta = \mu, \quad \psi(\eta) = -\frac 12 \eta^2, \quad h(y) = \frac{1}{\sqrt{2\pi}}\exp\left(-\frac12 y^2\right).
\end{equation}
Several other well-known distributions are in the exponential family as well. For example, consider $y \sim \text{Ber}(\pi)$. Then, we have
\begin{equation}
f(y) = \pi^{y}(1-\pi)^{1-y} = \exp\left(y \log \frac{\pi}{1-\pi} + \log(1-\pi) \right).
\end{equation}
Therefore, we have $\eta = \log \frac{\pi}{1-\pi}$, so that $\log(1-\pi) = -\log(1+e^\eta)$. It follows that
\begin{equation}
\eta = \log \frac{\pi}{1-\pi}, \quad \psi(\eta) = \log(1+e^\eta), \quad h(y) = 1.
\end{equation}
As another example, consider the Poisson distribution $y \sim \text{Poi}(\mu)$. We have
\begin{equation}
f(y) = e^{-\mu}\frac{\mu^y}{y!} = \exp(y \log \mu - \mu)\frac{1}{y!}.
\end{equation}
Therefore, we have $\eta = \log \mu$, so that $\mu = e^\eta$. It follows that
\begin{equation}
\eta = \log \mu, \quad \psi(\eta) = e^\eta, \quad h(y) = \frac{1}{y!}.
\end{equation}

\paragraph{Moments of exponential family distributions.}

It turns out that the derivatives of the log-partition function $\psi$ give the moments of $y$. Indeed, let's start with the relationship
\begin{equation}
\int f_\eta(y)dy = \int \exp(\eta y - \psi(\eta))h(y) dy = 1. 
\end{equation}
Differentiating in $\eta$ and interchanging the derivative and the integral, we obtain
\begin{equation}
0 = \frac{d}{d\eta} \int f_\eta(y)dy = \int (y - \dot \psi(\eta))f_\eta(y) dy,
\end{equation}
from which it follows that
\begin{equation}
\dot \psi(\eta) = \int \dot \psi(\eta)f_{\eta}(y)dy = \int y f_\eta(y)dy = \mathbb E_\eta[y] \equiv \mu_\eta.
\label{eq:psi-dot}
\end{equation}
Thus, the first derivative of the log partition function is the mean of $y$. Differentiating again, we get
\begin{equation}
\ddot \psi(\eta) = \int y(y - \dot \psi(\eta))f_\eta(y) dy = \int y(y - \mu_\eta)f_\eta(y) dy = \int (y - \mu_\eta)^2f_\eta(y) dy = \text{Var}_\eta[y].
\end{equation}
Thus, the second derivative of the log-partition function is the variance of $y$.

\paragraph{Relationship between mean and natural parameter.}

The log-partition function $\psi$ induces a connection~\eqref{eq:psi-dot} between the natural parameter $\eta$ and the mean $\mu$. Because 
\begin{equation}
\frac{d\mu}{d\eta} = \frac{d}{d\eta}\dot \psi(\eta) = \ddot \psi(\eta) = \text{Var}_\eta[y] > 0,
\end{equation}
it follows that $\mu$ is a strictly increasing function of $\eta$, so in particular the mapping between $\mu$ and $\eta$ is bijective. Therefore, we can think of equivalently parameterizing the distribution via $\mu$ or $\eta$. In the context of GLMs (see Section~\ref{sec:glm-def}), the mean-variance relationship is quantified in terms of the \textit{canonical link function} $g$, which maps the mean to the natural parameter:
\begin{equation}
\eta = \dot \psi^{-1}(\mu) \equiv g(\mu).
\end{equation}

\paragraph{Relationship between mean and variance.}

Note that the mean of an exponential family distribution determines its variance (since it determines the natural parameter $\eta$). For example, a Poisson random variable with mean $\mu$ has variance $\mu$ and a Bernoulli random variable with mean $\mu$ has variance $\mu(1-\mu)$. The mean-variance relationship turns out to characterize the exponential family distribution, i.e. an exponential family distribution with mean equal to its variance is the Poisson distribution. 

\section{Generalized linear models and examples} \label{sec:glm-def}

In this class, the focus is on building models that tie a vector of predictors $(\bm x_{i*})$ to a response $y_i$. For linear regression, the mean of $y$ was modeled as a linear combination of the predictors $\bm x_{i*}^T \bm \beta$: $\mu = \bm x_{i*}^T \bm \beta$. Typically, the ``right'' thing to do is to model the response linearly on the scale of the natural parameter $\eta$ rather than on the scale of the mean parameter $\mu$. It just happens for linear models (where the underlying distribution is Gaussian) that these two parameters coincide. 

\paragraph{Definition.} We define $\{(y_i, \bm x_{i*})\}_{i = 1}^n$ as following a generalized linear model based on the exponential family $f_\eta$ if 
\begin{equation}
y_i \overset{\text{ind}}\sim f_{\eta_i}, \quad \eta_i = \bm x_{i*}^T \bm \beta.
\label{eq:glm-def}
\end{equation}
GLMs are often written in terms of their link functions $g$, which relate the mean of $y$ to the linear predictor $\bm x_{i*}^T \bm \beta$. When modeling the natural parameter as a linear function in the predictors, as in the definition~\eqref{eq:glm-def}, we get a GLM with \textit{canonical link function} $g = \dot\psi^{-1}$:
\begin{equation}
g(\mathbb E[y_i]) = \dot \psi^{-1}(\mathbb E[y_i]) = \bm x_{i*}^T \bm \beta.
\end{equation}

\paragraph{Examples.} For example, \textit{logistic regression} is the GLM based on the Bernoulli distribution:
\begin{equation}
y_i \overset{\text{ind}}\sim \text{Ber}(\pi_i); \quad \eta_i = \log\frac{\pi_i}{1-\pi_i} = \bm x_{i*}^T \bm \beta.
\end{equation}
Thus the canonical link function for logistic regression is the \textit{logistic link function} $g(\mu) = \log \frac{\mu}{1-\mu}$. As another example, \textit{Poisson regression} is the GLM based on the Poisson distribution:
\begin{equation}
y_i \overset{\text{ind}}\sim \text{Poi}(\mu_i); \quad \eta_i = \log \mu_i = \bm x_{i*}^T \bm \beta.
\end{equation}
Thus the canonical link function for Poisson regression is the \textit{log link function} $g(\mu) = \log \mu$.

\section{Maximum likelihood estimation in GLMs} \label{sec:glm-max-lik}

\paragraph{GLM normal equations.}

Recall that the least squares estimate $\bm{\widehat \beta}$ is also the maximum likelihood estimate. For general GLMs, we also estimate $\bm \beta$ via maximum likelihood. To derive this estimates, let's write down the GLM likelihood and then take a derivative. The GLM likelihood is
\begin{equation}
\mathcal L(\bm \beta) = \prod_{i = 1}^n f_{\eta_i}(y_i) = \prod_{i = 1}^n \exp(\eta_i y_i - \psi(\eta_i)) h(y_i).
\end{equation}
Taking a logarithm, we have
\begin{equation}
\log \mathcal L(\bm \beta) = \sum_{i = 1}^n (\eta_i y_i - \psi(\eta_i)) + \sum_{i = 1}^n \log h(y_i) = \sum_{i = 1}^n (\bm x_{i*}^T \bm \beta y_i - \psi(\bm x_{i*}^T \bm \beta)) + \sum_{i = 1}^n \log h(y_i).
\label{eq:glm-log-likelihood}
\end{equation}
Taking a gradient in $\bm \beta$, we get
\begin{equation}
\nabla_{\bm \beta}\log \mathcal L(\bm \beta) = \sum_{i = 1}^n (\bm x_{i*} y_i - \bm x_{i*}\dot \psi(\bm x^T_{i*}\bm \beta)) = \bm X^T (\bm y - \bm \mu(\bm \beta)).
\label{eq:log-likelihood-derivative}
\end{equation}
Setting this expression to zero, we get the normal equations:
\begin{equation}
\bm X^T(\bm y - \bm \mu(\bm{\widehat \beta})) = 0.
\label{eq:glm-normal-equations}
\end{equation}
Recall that, for least squares, we got the same equation, with $\bm \mu(\bm{\widehat \beta}) = \bm X \bm{\widehat\beta}$. We can interpret the normal equations as stating that $\bm \mu(\bm{\widehat \beta})$ is a projection of $\bm y$ onto the model ``space''
\begin{equation}
C_\mu(\bm X) \equiv \{\bm \mu = \dot\psi(\bm \eta) = \dot \psi(\bm X \bm \beta): \bm \beta \in \mathbb R^p\}.
\end{equation}
parallel to the columns of $\bm X$. Note that the subscript $\mu$ on $C_\mu(\bm X)$ indicates that we are considering the ``space'' (actually, \textit{set}) of possible $\bm \mu$ as opposed to the space $C_\eta(\bm X)$ of possible $\bm \eta$, which we denoted in Unit 1 simply as $C(\bm X)$. For linear models, it is the case that $C_\mu(\bm X) = C_\eta(\bm X)$, but in general, these two are different. Note that $C_\mu(\bm X)$ in general is a manifold as opposed to a linear subspace of $\mathbb R^n$, while $C_\eta(\bm X)$ is always a linear subspace.

\paragraph{Log-concavity of GLM likelihood.} Unlike linear regression, in general GLMs the function $\bm \mu(\bm{\beta})$ is nonlinear. Therefore, there is in general no closed-form solution to the GLM normal equations~\eqref{eq:glm-normal-equations}. We must instead iteratively compute the maximum likelihood estimate $\bm{\widehat\beta}$. Before talking about the computation of the MLE $\bm{\widehat\beta}$, we state the important fact that $\log \mathcal L(\bm \beta)$ is a concave function of $\bm \beta$, which implies that this function is ``easy to optimize'', i.e. has no local maxima.

\begin{proposition} \label{prop:log-concavity}
The function $\log \mathcal L(\bm \beta)$ defined in~\eqref{eq:glm-log-likelihood} is concave in $\bm \beta$.
\end{proposition}
\begin{proof}
We claim it suffices to show that $\psi$ is a convex function. Indeed, then $\log \mathcal L(\bm \beta)$ would be the sum of a linear function of $\bm \beta$ and the composition of a concave function with a linear function. To verify that $\psi$ is convex, it suffices to recall that $\ddot \psi(\eta) = \text{Var}_\eta[y] \geq 0$.
\end{proof}
\noindent Proposition~\eqref{prop:log-concavity} gives us confidence that an iterative algorithm will converge to the global maximum of the likelihood. We present such an iterative algorithm next.

\paragraph{Newton-Raphson.}

We can solve the equation~\eqref{eq:glm-normal-equations} using the Newton Raphson algorithm, which involves the gradient and Hessian of the function we'd like to maximize. We already computed the gradient in equation~\eqref{eq:log-likelihood-derivative}. To compute the Hessian, we take another gradient in $\bm \beta$. We have
\begin{equation}
\begin{split}
\nabla^2_{\bm \beta} \log \mathcal L(\bm \beta) &= \nabla_{\bm \beta}(\bm X^T(\bm y - \dot \psi(\bm X \bm \beta))) = -\nabla_{\bm \beta} \bm X^T \dot \psi(\bm X \bm \beta) \\
&\quad= - \bm X^T \text{diag}(\ddot \psi(\bm X\bm \beta))\bm X \equiv -\bm X^T \bm W(\bm \beta)\bm X.
\end{split}
\label{eq:hessian}
\end{equation}
Here, $\dot \psi$ and $\ddot \psi$ applied to vectors are interpreted element-wise and $\bm W(\bm \beta) \in \mathbb R^{n \times n}$ is the diagonal matrix such that
\begin{equation}
W_{ii}(\bm \beta) = \text{Var}_{\bm \beta}[y_i].
\label{eq:W-def}
\end{equation}
The Newton-Raphson iteration is therefore
\begin{equation}
\bm{\widehat \beta}^{(t+1)} = \bm{\widehat \beta}^{(t)} - (\nabla^2_{\bm \beta} \log \mathcal L(\bm{\widehat \beta}^{(t)}))^{-1} \nabla_{\bm \beta} \log \mathcal L(\bm{\widehat \beta}^{(t)}) = \bm{\widehat \beta}^{(t)} + (\bm X^T \bm W(\bm{\widehat \beta}^{(t)})\bm X)^{-1}\bm X^T(\bm y - \bm \mu(\bm{\widehat \beta}^{(t)})).
\label{eq:NR-iteration}
\end{equation}

\paragraph{Iteratively reweighted least squares (IRLS).}

A nice interpretation of the Newton-Raphson algorithm is as a sequence of weighted least squares fits, known as the iteratively reweighted least squares (IRLS) algorithm. Suppose that we have a current estimate $\bm{\widehat \beta}^{(t)}$, and suppose we are looking for a vector $\bm \beta$ near $\bm{\widehat \beta}^{(t)}$ that fits the model even better. We have
\begin{equation*}
\mathbb E_{\bm \beta}[\bm y] = \dot \psi(\bm X \bm \beta) \approx \dot \psi(\bm X \bm{\widehat \beta}^{(t)}) + \text{diag}(\ddot \psi(\bm X \bm{\widehat \beta}^{(t)}))(\bm X \bm \beta - \bm X \bm{\widehat \beta}^{(t)}) = \bm \mu(\bm{\widehat \beta}^{(t)}) + \bm W(\bm{\widehat \beta}^{(t)})(\bm X \bm \beta - \bm X \bm{\widehat \beta}^{(t)}).
\end{equation*}
and
\begin{equation*}
\text{Var}_{\bm \beta}[\bm y] \approx \bm W(\bm{\widehat \beta}^{(t)}).
\end{equation*}
Thus, up to the first two moments, near $\bm \beta = \bm{\widehat \beta}^{(t)}$ the distribution of $\bm y$ is approximately
\begin{equation}
\bm y = \bm \mu(\bm{\widehat \beta}^{(t)}) + \bm W(\bm{\widehat \beta}^{(t)})(\bm X \bm \beta - \bm X \bm{\widehat \beta}^{(t)}) + \bm \epsilon, \quad \bm \epsilon \sim N(\bm 0, \bm W(\bm{\widehat \beta}^{(t)})),
\end{equation}
or, equivalently,
\begin{equation}
\bm z^{(t)} \equiv \bm W(\bm{\widehat \beta}^{(t)})^{-1}(\bm y - \bm \mu(\bm{\widehat \beta}^{(t)})) + \bm X \bm{\widehat \beta}^{(t)} = \bm X \bm \beta + \bm \epsilon', \quad \bm \epsilon' \sim N(\bm 0, \bm W(\bm{\widehat \beta}^{(t)})^{-1}).
\end{equation}
The regression of the \textit{adjusted response variable} $\bm z^{(t)}$ on $\bm X$ leaves us with a weighted linear regression, whose maximum likelihood estimate is
\begin{equation}
\bm{\widehat \beta}^{(t+1)} = (\bm X^T \bm W(\bm{\widehat \beta}^{(t)}) \bm X)^{-1} \bm X^T \bm W(\bm{\widehat \beta}^{(t)}) \bm z^{(t)},
\label{eq:IRLS-iteration}
\end{equation}
which we define as our next iterate. It's easy to verify that the IRLS iteration~\eqref{eq:IRLS-iteration} is equivalent to the Newton-Raphson iteration~\eqref{eq:NR-iteration}.

\section{Inference in GLMs} \label{sec:glm-inf}

\paragraph{Asymptotic normality and GLM standard errors.} Inference in GLMs is based on asymptotic likelihood theory. Using the Hessian computation~\eqref{eq:hessian}, we can compute the Fisher information matrix
\begin{equation}
\bm I(\bm \beta) = -\mathbb E_{\bm \beta}[\nabla^2_{\bm \beta}\log \mathcal L(\bm \beta)] = \bm X^T \bm W(\bm \beta) \bm X,
\end{equation}
recalling the definition of $\bm W$ in equation~\eqref{eq:W-def}. Therefore, likelihood theory tells us that, as the sample size $n$ grows, we have
\begin{equation}
\bm{\widehat \beta} \overset \cdot \sim N(\bm \beta, (\bm X^T \bm W(\bm \beta) \bm X)^{-1}).
\end{equation}
Using the plug-in variance estimate, we typically construct GLM standard errors based on
\begin{equation}
\widehat{\text{Var}}[\bm{\widehat \beta}] \equiv (\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}.
\end{equation}

\paragraph{Confidence intervals.}

A confidence interval for each coordinate $\beta_j$ can be obtained via
\begin{equation}
\text{CI}(\widehat \beta_j) \equiv \widehat \beta_j \pm 2 \cdot \text{SE}(\widehat \beta_j), \quad \text{where} \quad \text{SE}(\beta_j) \equiv \sqrt{(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}_{jj}}.
\label{eq:conf-int-beta}
\end{equation}
A confidence interval for $\eta_i = \bm x_{i*}^T \bm \beta$ can be obtained via
\begin{equation}
\text{CI}(\widehat \eta_i) \equiv \bm x_{i*}^T \bm{\widehat\beta} \pm 2 \cdot \text{SE}(\eta_i), \quad \text{where} \quad \text{SE}(\widehat \eta_i) \equiv \sqrt{\bm x_{i*}^T(\bm X^T \bm W(\bm{\widehat \beta}) \bm X)^{-1}\bm x_{i*}}.
\end{equation}
A confidence interval for $\mu_i \equiv \mathbb E_{\bm \beta}[y_i] = \dot \psi(\eta_i)$ can be obtained by applying the strictly increasing function $\dot \psi$ to the endpoints of the confidence interval for $\eta_i$. Note that the resulting confidence interval may be asymmetric.

\paragraph{Testing a single coordinate.}

We can invert the confidence interval~\eqref{eq:conf-int-beta} to get a test of the hypothesis $H_0: \beta_j = \beta_j^0$ for any $\beta_j^0 \in \mathbb R$:
\begin{equation}
\phi(\bm X, \bm y) = \mathbbm 1(|z(\bm X, \bm y)| > z_{1-\alpha/2}), \quad \text{where} \quad z(\bm X, \bm y) \equiv \frac{\widehat \beta_j}{\text{SE}(\widehat \beta_j)}.
\end{equation}
This is the analog of the $t$-test for a linear regression.

\paragraph{Testing multiple coordinates.}

Likelihood ratio test, deviances, goodness of fit testing. (TBD)

\section{Bells and whistles} \label{sec:bells-and-whistles}

\paragraph{Offsets.}

\paragraph{Exponential dispersion families.}

\paragraph{Non-canonical links.}


\end{document}
